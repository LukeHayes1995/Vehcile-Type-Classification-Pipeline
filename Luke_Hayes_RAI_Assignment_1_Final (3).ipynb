{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Luke_Hayes_RAI_Assignment_1_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v2A3-BPShz-",
        "outputId": "98da7486-4dd9-434c-feb0-395ca33add63"
      },
      "source": [
        "#USE GOOGLE DRIVE FOR FOLES TO \n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQlO2HBiTyKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5a233b-4911-44ac-d9c6-f1ac5cb1ace9"
      },
      "source": [
        "#SPECIFIC VERSIONS OF TENSORFLOW AND KERAS ARE REQUIRED TO RUN WITH DARKNET AND MY MODEL\n",
        "#TRIED TO USE THE KERAS MODEL IN THE INSTRUCTIONS BUT IT WAS BUILT ON THE PREVIOUS TENSORFLOW\n",
        "#WITH MY MODEL BEING BUILT ON TENSORFLOW 2.0 THERE WAS HUGE ISSUES WITH TRYING TO USE THE MODEL\n",
        "\n",
        "!pip install tensorflow==2.4.0\n",
        "!pip install keras==2.1.5 \n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (54.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: keras==2.1.5 in /usr/local/lib/python3.7/dist-packages (2.1.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5rB-twATzto",
        "outputId": "7842d93f-cc17-4a9d-9cd3-52cafc75d3c5"
      },
      "source": [
        "#HERE WE CREATE THE TRAINING AND VALIDATION DATA FROM THE FOLDERS OF IMAGES THAT I HAVE FOR MY PROJECT\n",
        "#I HAVE A TOTAL OF 800 OF EACH TYPE OF IMAGE\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/gdrive/MyDrive/dataset/dataset2/\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",  # categorical, binary\n",
        "    class_names=['sedan', 'suv'],\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(416, 416),  # reshape if not in this size\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/gdrive/MyDrive/dataset/dataset2/\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",  # categorical, binary\n",
        "    class_names=['sedan', 'suv'],\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(416, 416),  # reshape if not in this size\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 files belonging to 2 classes.\n",
            "Using 1280 files for training.\n",
            "Found 1600 files belonging to 2 classes.\n",
            "Using 320 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkH2QRVfgtYK"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "#HERE IS WHERE E DO SOME DATA AUGMENTATION\n",
        "#WE DO A RANDOM HORIZONTAL FLIPS AND ROTATION ON SOME OF THE DATA \n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPjXjT0Jf3Pu",
        "outputId": "cef70e2e-190a-4868-cd7e-11d43e19c3e4"
      },
      "source": [
        "# MOBILENET MODE TRAINED ON IMAGENT DATASET\n",
        "base_model = keras.applications.MobileNet(\n",
        "    weights=\"imagenet\",  \n",
        "    input_shape=(416, 416, 3),\n",
        "    include_top=False,\n",
        ")  \n",
        "\n",
        "print(train_ds)\n",
        "\n",
        "# KEEP THE BASE MODEL FOR LATER DONT TRAIN STRAIGHT AWAY\n",
        "base_model.trainable = False\n",
        "\n",
        "# HERE WE CREATE THE NEW MODEL ON TOP OF THE BASE MODEL \n",
        "inputs = keras.Input(shape=(416, 416, 3))\n",
        "# HERE WE APPLY THE DATA AUGMENTATION AT RANDOM\n",
        "x = data_augmentation(inputs) \n",
        "\n",
        "\n",
        "#HERE WE DO SOME NORMALIZATION OF THE INPUT IMAGE DATA TO (-1,1)\n",
        "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
        "mean = np.array([127.5] * 3)\n",
        "var = mean ** 2\n",
        "x = norm_layer(x)\n",
        "norm_layer.set_weights([mean, var])\n",
        "\n",
        "# WE WILL KEEP THE BASE MODEL FROM TRAINING SO WE CAN DO SOME FINE TUNING LATER\n",
        "x = base_model(x, training=False)\n",
        "# USE AVERAGE POOLING LAYER TO \n",
        "# REDUCE SIZE OF REPRESENTATION AND SPEED UP COMPUTATION\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "#IMPLEMENT RANDOM DROPOUT OF SOME OF THE NN NODES \n",
        "#MAKES THE NN NOT RELY ON ANY ONE FEATURE AS IT CAN GO AWAY BASICALLY \n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "# ADD A SIMPLE DENSE LAYER WITH TWO OUTPUTS \n",
        "outputs = keras.layers.Dense(2)(x)\n",
        "# NOW WE HAVE THE MODEL\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "<BatchDataset shapes: ((None, 416, 416, 3), (None, 2)), types: (tf.float32, tf.float32)>\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 416, 416, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 416, 416, 3)       0         \n",
            "_________________________________________________________________\n",
            "normalization_2 (Normalizati (None, 416, 416, 3)       7         \n",
            "_________________________________________________________________\n",
            "mobilenet_1.00_224 (Function (None, 13, 13, 1024)      3228864   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 3,230,921\n",
            "Trainable params: 2,050\n",
            "Non-trainable params: 3,228,871\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaWgrDFuk2Sb",
        "outputId": "70b64074-721d-41e0-f3fc-c734ca722fa6"
      },
      "source": [
        "# HERE WE USE ADAM INSTEAD OF SGD\n",
        "# BinaryCrossentropy AS OUR LOSS FUNCTION\n",
        "# AND BINARY ACCURACY AS OUT ACCURACY METRIC\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(train_ds)\n",
        "\n",
        "##WE DO SOME TRAINING \n",
        "epochs = 20\n",
        "model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 416, 416, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 416, 416, 3)       0         \n",
            "_________________________________________________________________\n",
            "normalization_2 (Normalizati (None, 416, 416, 3)       7         \n",
            "_________________________________________________________________\n",
            "mobilenet_1.00_224 (Function (None, 13, 13, 1024)      3228864   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 3,230,921\n",
            "Trainable params: 2,050\n",
            "Non-trainable params: 3,228,871\n",
            "_________________________________________________________________\n",
            "<BatchDataset shapes: ((None, 416, 416, 3), (None, 2)), types: (tf.float32, tf.float32)>\n",
            "Epoch 1/20\n",
            "40/40 [==============================] - 15s 291ms/step - loss: 0.7232 - binary_accuracy: 0.5190 - val_loss: 0.6016 - val_binary_accuracy: 0.5734\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 13s 283ms/step - loss: 0.6012 - binary_accuracy: 0.6265 - val_loss: 0.5239 - val_binary_accuracy: 0.6906\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 13s 283ms/step - loss: 0.5395 - binary_accuracy: 0.6997 - val_loss: 0.4724 - val_binary_accuracy: 0.7641\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 12s 281ms/step - loss: 0.4910 - binary_accuracy: 0.7454 - val_loss: 0.4317 - val_binary_accuracy: 0.8078\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 13s 285ms/step - loss: 0.4587 - binary_accuracy: 0.7793 - val_loss: 0.4034 - val_binary_accuracy: 0.8266\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 13s 286ms/step - loss: 0.4395 - binary_accuracy: 0.7901 - val_loss: 0.3937 - val_binary_accuracy: 0.8156\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 12s 275ms/step - loss: 0.4166 - binary_accuracy: 0.8077 - val_loss: 0.3687 - val_binary_accuracy: 0.8469\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 12s 272ms/step - loss: 0.4006 - binary_accuracy: 0.8197 - val_loss: 0.3631 - val_binary_accuracy: 0.8313\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 12s 277ms/step - loss: 0.3915 - binary_accuracy: 0.8146 - val_loss: 0.3486 - val_binary_accuracy: 0.8484\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 12s 276ms/step - loss: 0.3811 - binary_accuracy: 0.8282 - val_loss: 0.3470 - val_binary_accuracy: 0.8375\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 12s 272ms/step - loss: 0.3593 - binary_accuracy: 0.8318 - val_loss: 0.3389 - val_binary_accuracy: 0.8438\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 12s 277ms/step - loss: 0.3645 - binary_accuracy: 0.8263 - val_loss: 0.3183 - val_binary_accuracy: 0.8625\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 12s 273ms/step - loss: 0.3546 - binary_accuracy: 0.8464 - val_loss: 0.3324 - val_binary_accuracy: 0.8516\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 12s 275ms/step - loss: 0.3394 - binary_accuracy: 0.8533 - val_loss: 0.3235 - val_binary_accuracy: 0.8500\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 12s 273ms/step - loss: 0.3294 - binary_accuracy: 0.8586 - val_loss: 0.3217 - val_binary_accuracy: 0.8547\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 12s 274ms/step - loss: 0.3377 - binary_accuracy: 0.8473 - val_loss: 0.3144 - val_binary_accuracy: 0.8562\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 12s 275ms/step - loss: 0.3258 - binary_accuracy: 0.8502 - val_loss: 0.3073 - val_binary_accuracy: 0.8609\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 12s 272ms/step - loss: 0.3188 - binary_accuracy: 0.8554 - val_loss: 0.2886 - val_binary_accuracy: 0.8844\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 12s 272ms/step - loss: 0.3308 - binary_accuracy: 0.8596 - val_loss: 0.2889 - val_binary_accuracy: 0.8766\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 12s 274ms/step - loss: 0.3106 - binary_accuracy: 0.8596 - val_loss: 0.3055 - val_binary_accuracy: 0.8594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5d400e55d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV7rtVi6OJMq",
        "outputId": "4da0e35c-b7b1-4237-ad43-890ec686f401"
      },
      "source": [
        "#NOW WE TRAIN THE BASE MODEL AS A MEANS OF DOING SOME FINE TUNING TRAINING\n",
        "#SO ESENTIALLY WE TRAIN USING OUR MODEL ON MY TRAINING DATA\n",
        "#THEN WE FINE TUNE THIS USING THE BASE MODEL (MOBILENET)\n",
        "#ALSO USE A VERY LOW LEARNING RATE \n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5), \n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 416, 416, 3)]     0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 416, 416, 3)       0         \n",
            "_________________________________________________________________\n",
            "normalization_2 (Normalizati (None, 416, 416, 3)       7         \n",
            "_________________________________________________________________\n",
            "mobilenet_1.00_224 (Function (None, 13, 13, 1024)      3228864   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 3,230,921\n",
            "Trainable params: 3,209,026\n",
            "Non-trainable params: 21,895\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 34s 747ms/step - loss: 0.3481 - binary_accuracy: 0.8354 - val_loss: 0.2418 - val_binary_accuracy: 0.9078\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 30s 736ms/step - loss: 0.2716 - binary_accuracy: 0.8817 - val_loss: 0.2401 - val_binary_accuracy: 0.9031\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 31s 740ms/step - loss: 0.2275 - binary_accuracy: 0.9099 - val_loss: 0.2040 - val_binary_accuracy: 0.9156\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 31s 738ms/step - loss: 0.2289 - binary_accuracy: 0.9011 - val_loss: 0.1948 - val_binary_accuracy: 0.9250\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 31s 735ms/step - loss: 0.1926 - binary_accuracy: 0.9211 - val_loss: 0.1781 - val_binary_accuracy: 0.9281\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 31s 745ms/step - loss: 0.1954 - binary_accuracy: 0.9227 - val_loss: 0.1942 - val_binary_accuracy: 0.9219\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 31s 738ms/step - loss: 0.1737 - binary_accuracy: 0.9339 - val_loss: 0.1544 - val_binary_accuracy: 0.9453\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 31s 741ms/step - loss: 0.1544 - binary_accuracy: 0.9429 - val_loss: 0.1798 - val_binary_accuracy: 0.9312\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 31s 739ms/step - loss: 0.1579 - binary_accuracy: 0.9445 - val_loss: 0.1617 - val_binary_accuracy: 0.9453\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 31s 741ms/step - loss: 0.1626 - binary_accuracy: 0.9348 - val_loss: 0.1590 - val_binary_accuracy: 0.9422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5b101cc8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSqkcHYQW6hM",
        "outputId": "8c7d5f4e-87de-4c49-fae7-f6845fa93fc5"
      },
      "source": [
        "#SAVE THE MODEL SO I DON'T HAVE TO KEEP RUNNING IT \n",
        "model.save('saved_model/my_model_new_softmax1')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model_new_softmax1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYtcSS7pOnxs"
      },
      "source": [
        "#LOAD THE OLD SAVED MODEL \n",
        "#THIS IS WHEN MY COLAB TIMES OUT \n",
        "new_model = tf.keras.models.load_model('/content/gdrive/MyDrive/my_model_new_softmax')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUb6ch_SU22u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2259fc-d825-48d4-b37b-a9b4c5b5b638"
      },
      "source": [
        "#CLONE THE DARKNET LIBRARY\n",
        "#LOAD IN THE WEIGHTS FOR TINY YOLO \n",
        "\n",
        "!git clone https://github.com/AlexeyAB/darknet.git\n",
        "!wget https://pjreddie.com/media/files/yolov3-tiny.weights\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 14748, done.\u001b[K\n",
            "remote: Total 14748 (delta 0), reused 0 (delta 0), pack-reused 14748\u001b[K\n",
            "Receiving objects: 100% (14748/14748), 13.29 MiB | 22.34 MiB/s, done.\n",
            "Resolving deltas: 100% (10031/10031), done.\n",
            "--2021-03-30 11:09:14--  https://pjreddie.com/media/files/yolov3-tiny.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35434956 (34M) [application/octet-stream]\n",
            "Saving to: ‘yolov3-tiny.weights’\n",
            "\n",
            "yolov3-tiny.weights 100%[===================>]  33.79M  15.2MB/s    in 2.2s    \n",
            "\n",
            "2021-03-30 11:09:16 (15.2 MB/s) - ‘yolov3-tiny.weights’ saved [35434956/35434956]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R06CRXkaPlF9",
        "outputId": "4e4a782f-3607-4c28-da14-9d105065c870"
      },
      "source": [
        "#I HAD TO MAKE NUMEROUR CHANGES TO THE DARKNET PROJECT TO \n",
        "#1 ALLOW IT TO BE IMPORTED AS A LIBRARY\n",
        "#2 GET THE LIBRRY TO WORK EITH MY IMAGES \n",
        "#ALSO I HAD TO CHANGE TINY-YOLOT.CFG FILE TO CREATE CORRECT BOUDING BOXES AS INITIALLY\n",
        "#THEY ONLY COVERED A TINY PART OF MY VEHICLES\n",
        "\n",
        "#THEREFOR HERE I NEED TO UPLOAD\n",
        "#1 A NEW DARKNET.PY FILE\n",
        "#2 A NEW MAKEFILE\n",
        "#3 A NEW TINY-YOLO.CFG FILE ALL BEFORE THE NEXT MAKE STEP\n",
        "\n",
        "%cd /content/darknet\n",
        "#NEED TO CHANGE FILES BEFORE EXECUTING NEXT CELL"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0LqkXhJZ16-",
        "outputId": "74680603-8211-4efc-f4ee-397bb5dd6167"
      },
      "source": [
        "#NEED TO MAKE CHANGES DON'T MAKE\n",
        "#MAKE THE NEW DARKNET PROJECT \n",
        "######################################################################################################\n",
        "##PLEASE LOOK AT INSTRUCTIONS FILE IN ZIP FILE BEFORE MAKING\n",
        "######################################################################################################\n",
        "!make"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir -p ./obj/\n",
            "mkdir -p backup\n",
            "chmod +x *.sh\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/image_opencv.cpp -o obj/image_opencv.o\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_detections_cv_v3(void**, detection*, int, float, char**, image**, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:926:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Krgb\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                 float \u001b[01;35m\u001b[Krgb\u001b[m\u001b[K[3];\n",
            "                       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_train_loss(char*, void**, int, float, float, int, int, float, int, char*, float, int, int, double)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1127:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "             \u001b[01;35m\u001b[Kif\u001b[m\u001b[K (iteration_old == 0)\n",
            "             \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1130:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "          \u001b[01;36m\u001b[Kif\u001b[m\u001b[K (iteration_old != 0){\n",
            "          \u001b[01;36m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid cv_draw_object(image, float*, int, int, int*, float*, int*, int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1424:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbuff\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         char \u001b[01;35m\u001b[Kbuff\u001b[m\u001b[K[100];\n",
            "              \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1400:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kit_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kit_tb_res\u001b[m\u001b[K = cv::createTrackbar(it_trackbar_name, window_name, &it_trackbar_value, 1000);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1404:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Klr_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Klr_tb_res\u001b[m\u001b[K = cv::createTrackbar(lr_trackbar_name, window_name, &lr_trackbar_value, 20);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1408:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcl_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kcl_tb_res\u001b[m\u001b[K = cv::createTrackbar(cl_trackbar_name, window_name, &cl_trackbar_value, classes-1);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1411:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbo_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kbo_tb_res\u001b[m\u001b[K = cv::createTrackbar(bo_trackbar_name, window_name, boxonly, 1);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/http_stream.cpp -o obj/http_stream.o\n",
            "In file included from \u001b[01m\u001b[K./src/http_stream.cpp:580:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K./src/httplib.h:129:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K\"INVALID_SOCKET\" redefined\n",
            " #define INVALID_SOCKET (-1)\n",
            " \n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:73:0:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kthis is the location of the previous definition\n",
            " #define INVALID_SOCKET -1\n",
            " \n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool JSON_sender::write(const char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:249:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int \u001b[01;35m\u001b[Kn\u001b[m\u001b[K = _write(client, outputbuf, outlen);\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool MJPG_sender::write(const cv::Mat&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:507:113:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%zu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "                 sprintf(head, \"--mjpegstream\\r\\nContent-Type: image/jpeg\\r\\nContent-Length: %zu\\r\\n\\r\\n\", outlen\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid set_track_id(detection*, int, float, float, float, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:863:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (int i = 0; \u001b[01;35m\u001b[Ki < v.size()\u001b[m\u001b[K; ++i) {\n",
            "                         \u001b[01;35m\u001b[K~~^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:871:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int old_id = 0; \u001b[01;35m\u001b[Kold_id < old_dets.size()\u001b[m\u001b[K; ++old_id) {\n",
            "                          \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:890:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int index = 0; \u001b[01;35m\u001b[Kindex < new_dets_num*old_dets.size()\u001b[m\u001b[K; ++index) {\n",
            "                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:926:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     if (\u001b[01;35m\u001b[Kold_dets_dq.size() > deque_size\u001b[m\u001b[K) old_dets_dq.pop_front();\n",
            "         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/gemm.c -o obj/gemm.o\n",
            "\u001b[01m\u001b[K./src/gemm.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kconvolution_2d\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gemm.c:2038:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_w\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     const int \u001b[01;35m\u001b[Kout_w\u001b[m\u001b[K = (w + 2 * pad - ksize) / stride + 1;    // output_width=input_width for stride=1 and pad=1\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gemm.c:2037:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_h\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     const int \u001b[01;35m\u001b[Kout_h\u001b[m\u001b[K = (h + 2 * pad - ksize) / stride + 1;    // output_height=input_height for stride=1 and pad=1\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/utils.c -o obj/utils.o\n",
            "\u001b[01m\u001b[K./src/utils.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcustom_hash\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/utils.c:1045:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around assignment used as truth value [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "     while (\u001b[01;35m\u001b[Kc\u001b[m\u001b[K = *str++)\n",
            "            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/dark_cuda.c -o obj/dark_cuda.o\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcudnn_check_error_extended\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:228:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcudaError_t {aka enum cudaError}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum <anonymous>\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         if (status \u001b[01;35m\u001b[K!=\u001b[m\u001b[K CUDNN_STATUS_SUCCESS)\n",
            "                    \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kpre_allocate_pinned_memory\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:364:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K MB, num_of_blocks = %Iu, block_size = %Iu MB \\n\",\n",
            "                                      \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                      \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "             \u001b[32m\u001b[Ksize / (1024*1024)\u001b[m\u001b[K, num_of_blocks, pinned_block_size / (1024 * 1024));\n",
            "             \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K          \n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:364:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = %Iu MB, num_of_blocks = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K, block_size = %Iu MB \\n\",\n",
            "                                                              \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                              \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:364:82:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = %Iu MB, num_of_blocks = %Iu, block_size = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K MB \\n\",\n",
            "                                                                                \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                                                \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:374:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "                 printf(\" Allocated \u001b[01;35m\u001b[K%d\u001b[m\u001b[K pinned block \\n\", pinned_block_size);\n",
            "                                    \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcuda_make_array_pinned_preallocated\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:395:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"\\n Pinned block_id = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, filled = %f %% \\n\", pinned_block_id, filled);\n",
            "                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:410:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"Try to allocate new pinned memory, size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K MB \\n\", \u001b[32m\u001b[Ksize / (1024 * 1024)\u001b[m\u001b[K);\n",
            "                                                               \u001b[01;35m\u001b[K~^\u001b[m\u001b[K         \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "                                                               \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:416:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"Try to allocate new pinned BLOCK, size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K MB \\n\", \u001b[32m\u001b[Ksize / (1024 * 1024)\u001b[m\u001b[K);\n",
            "                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K         \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "At top level:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:256:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KswitchBlasHandle\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            " static cublasHandle_t \u001b[01;35m\u001b[KswitchBlasHandle\u001b[m\u001b[K[16];\n",
            "                       \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:255:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KswitchBlasInit\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            " static int \u001b[01;35m\u001b[KswitchBlasInit\u001b[m\u001b[K[16] = { 0 };\n",
            "            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/convolutional_layer.c -o obj/convolutional_layer.o\n",
            "\u001b[01m\u001b[K./src/convolutional_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_convolutional_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/convolutional_layer.c:1341:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kt_intput_size\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                         size_t \u001b[01;35m\u001b[Kt_intput_size\u001b[m\u001b[K = binary_transpose_align_input(k, n, state.workspace, &l.t_bit_input, ldb_align, l.bit_align);\n",
            "                                \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/list.c -o obj/list.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/image.c -o obj/image.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/activations.c -o obj/activations.o\n",
            "\u001b[01m\u001b[K./src/activations.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kactivate\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KRELU6\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(a){\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KSWISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KHARD_MISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN_SOFTMAX\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN_SOFTMAX_MAXVAL\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgradient\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KSWISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(a){\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KHARD_MISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/im2col.c -o obj/im2col.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/col2im.c -o obj/col2im.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/blas.c -o obj/blas.o\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbackward_shortcut_multilayer_cpu\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:207:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int \u001b[01;35m\u001b[Kout_index\u001b[m\u001b[K = id;\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfind_sim\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:597:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, j = %d, z = %d \\n\", i, j, z);\n",
            "                                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:597:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = %d, j = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, z = %d \\n\", i, j, z);\n",
            "                                                                  \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:597:75:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = %d, j = %d, z = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, j, z);\n",
            "                                                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfind_P_constrastive\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:611:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, j = %d, z = %d \\n\", i, j, z);\n",
            "                                                                   \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:611:76:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = %d, j = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, z = %d \\n\", i, j, z);\n",
            "                                                                           \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                           \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:611:84:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = %d, j = %d, z = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, j, z);\n",
            "                                                                                   \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KP_constrastive_f\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:651:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, l = %d \\n\", i, l);\n",
            "                                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:651:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = %d, l = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, l);\n",
            "                                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KP_constrastive\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:785:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, l = %d \\n\", i, l);\n",
            "                                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:785:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = %d, l = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, l);\n",
            "                                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/crop_layer.c -o obj/crop_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/dropout_layer.c -o obj/dropout_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/maxpool_layer.c -o obj/maxpool_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/softmax_layer.c -o obj/softmax_layer.o\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_contrastive_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:203:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 9 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "     fprintf(stderr, \"contrastive %4d x%4d x%4d x emb_size %4d x batch: %4d  classes = %4d, step = \u001b[01;35m\u001b[K%4d\u001b[m\u001b[K \\n\", w, h, l.n, l.embedding_size, batch, l.classes, step);\n",
            "                                                                                                   \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                                                                   \u001b[32m\u001b[K%4ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_contrastive_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:244:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kmax_truth\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kmax_truth\u001b[m\u001b[K = 0;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:423:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\" Error: too large number of bboxes: contr_size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K > max_contr_size  = %d \\n\", contr_size, max_contr_size);\n",
            "                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/data.c -o obj/data.o\n",
            "\u001b[01m\u001b[K./src/data.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kload_data_detection\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/data.c:1297:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int k, \u001b[01;35m\u001b[Kx\u001b[m\u001b[K, y;\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/data.c:1090:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kr_scale\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     float r1 = 0, r2 = 0, r3 = 0, r4 = 0, \u001b[01;35m\u001b[Kr_scale\u001b[m\u001b[K = 0;\n",
            "                                           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/matrix.c -o obj/matrix.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/network.c -o obj/network.o\n",
            "\u001b[01m\u001b[K./src/network.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_network_waitkey\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network.c:434:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kema_period\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kema_period\u001b[m\u001b[K = (net.max_batches - ema_start_point - 1000) * (1.0 - net.ema_alpha);\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/network.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_network\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network.c:659:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Knet->input_pinned_cpu, size * sizeof(float), cudaHostRegisterMapped))\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/network.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/connected_layer.c -o obj/connected_layer.o\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_connected_layer_gpu\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:346:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kone\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kone\u001b[m\u001b[K = 1;    // alpha[0], beta[0]\n",
            "           \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:344:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Kc\u001b[m\u001b[K = l.output_gpu;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:343:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kb\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Kb\u001b[m\u001b[K = l.weights_gpu;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:342:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ka\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Ka\u001b[m\u001b[K = state.input;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:341:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kn\u001b[m\u001b[K = l.outputs;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:340:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kk\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kk\u001b[m\u001b[K = l.inputs;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:339:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Km\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Km\u001b[m\u001b[K = l.batch;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/cost_layer.c -o obj/cost_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/parser.c -o obj/parser.o\n",
            "\u001b[01m\u001b[K./src/parser.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kparse_network_cfg_custom\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/parser.c:1689:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Knet.input_pinned_cpu, size * sizeof(float), cudaHostRegisterMapped)) net.input_pinned_cpu_flag = 1;\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activation_layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/parser.c:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/parser.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kget_classes_multipliers\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/parser.c:431:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kargument 1 range [18446744071562067968, 18446744073709551615] exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Walloc-size-larger-than=\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kclasses_multipliers = (float *)calloc(classes_counters, sizeof(float))\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K./src/parser.c:3:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/stdlib.h:541:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin a call to allocation function ‘\u001b[01m\u001b[Kcalloc\u001b[m\u001b[K’ declared here\n",
            " extern void *\u001b[01;36m\u001b[Kcalloc\u001b[m\u001b[K (size_t __nmemb, size_t __size)\n",
            "              \u001b[01;36m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/option_list.c -o obj/option_list.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/darknet.c -o obj/darknet.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/detection_layer.c -o obj/detection_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/captcha.c -o obj/captcha.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/route_layer.c -o obj/route_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/writing.c -o obj/writing.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/box.c -o obj/box.o\n",
            "\u001b[01m\u001b[K./src/box.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbox_iou_kind\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/box.c:154:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMSE\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(iou_kind) {\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/box.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdiounms_sort\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/box.c:898:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbeta_prob\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kbeta_prob\u001b[m\u001b[K = pow(dets[j].prob[k], 2) / sum_prob;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/box.c:897:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kalpha_prob\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kalpha_prob\u001b[m\u001b[K = pow(dets[i].prob[k], 2) / sum_prob;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/nightmare.c -o obj/nightmare.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/normalization_layer.c -o obj/normalization_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/avgpool_layer.c -o obj/avgpool_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/coco.c -o obj/coco.o\n",
            "\u001b[01m\u001b[K./src/coco.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_coco_recall\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/coco.c:248:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbase\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     char *\u001b[01;35m\u001b[Kbase\u001b[m\u001b[K = \"results/comp4_det_test_\";\n",
            "           \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/dice.c -o obj/dice.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/yolo.c -o obj/yolo.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/detector.c -o obj/detector.o\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_detector\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:386:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K&&\u001b[m\u001b[K’ within ‘\u001b[01m\u001b[K||\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "             \u001b[01;35m\u001b[K(iteration >= (iter_save + 1000) || iteration % 1000 == 0) && net.max_batches < 10000\u001b[m\u001b[K)\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kprint_cocos\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:486:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat not a string literal and no format arguments [\u001b[01;35m\u001b[K-Wformat-security\u001b[m\u001b[K]\n",
            "                 fprintf(fp, \u001b[01;35m\u001b[Kbuff\u001b[m\u001b[K);\n",
            "                             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Keliminate_bdd\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:579:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kstatement with no effect [\u001b[01;35m\u001b[K-Wunused-value\u001b[m\u001b[K]\n",
            "                     \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K (k; buf[k + n] != '\\0'; k++)\n",
            "                     \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_detector\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:700:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kmkd2\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kmkd2\u001b[m\u001b[K = make_directory(buff2, 0777);\n",
            "             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:698:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kmkd\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kmkd\u001b[m\u001b[K = make_directory(buff, 0777);\n",
            "             \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_detector_map\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:1332:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kclass_recall\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kclass_recall\u001b[m\u001b[K = (float)tp_for_thresh_per_class[i] / ((float)tp_for_thresh_per_class[i] + (float)(truth_classes_count[i] - tp_for_thresh_per_class[i]));\n",
            "               \u001b[01;35m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:1331:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kclass_precision\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kclass_precision\u001b[m\u001b[K = (float)tp_for_thresh_per_class[i] / ((float)tp_for_thresh_per_class[i] + (float)fp_for_thresh_per_class[i]);\n",
            "               \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdraw_object\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:1867:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kinv_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "             float \u001b[01;35m\u001b[Kinv_loss\u001b[m\u001b[K = 1.0 / max_val_cmp(0.01, avg_loss);\n",
            "                   \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/layer.c -o obj/layer.o\n",
            "\u001b[01m\u001b[K./src/layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfree_layer_custom\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/layer.c:208:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K&&\u001b[m\u001b[K’ within ‘\u001b[01m\u001b[K||\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "     if (l.delta_gpu && (l.optimized_memory < 1 || \u001b[01;35m\u001b[Kl.keep_delta_gpu && l.optimized_memory < 3\u001b[m\u001b[K)) cuda_free(l.delta_gpu), l.delta_gpu = NULL;\n",
            "                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/compare.c -o obj/compare.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/classifier.c -o obj/classifier.o\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:146:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcount\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kcount\u001b[m\u001b[K = 0;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kpredict_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:855:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktime\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     clock_t \u001b[01;35m\u001b[Ktime\u001b[m\u001b[K;\n",
            "             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdemo_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:1287:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktval_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         struct timeval tval_before, tval_after, \u001b[01;35m\u001b[Ktval_result\u001b[m\u001b[K;\n",
            "                                                 \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:1287:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktval_after\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         struct timeval tval_before, \u001b[01;35m\u001b[Ktval_after\u001b[m\u001b[K, tval_result;\n",
            "                                     \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/local_layer.c -o obj/local_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/swag.c -o obj/swag.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/shortcut_layer.c -o obj/shortcut_layer.o\n",
            "\u001b[01m\u001b[K./src/shortcut_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_shortcut_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/shortcut_layer.c:55:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kscale\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kscale\u001b[m\u001b[K = sqrt(2. / l.nweights);\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/activation_layer.c -o obj/activation_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/rnn_layer.c -o obj/rnn_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/gru_layer.c -o obj/gru_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/rnn.c -o obj/rnn.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/rnn_vid.c -o obj/rnn_vid.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/crnn_layer.c -o obj/crnn_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/demo.c -o obj/demo.o\n",
            "\u001b[01m\u001b[K./src/demo.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdetect_in_thread\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/demo.c:98:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         layer \u001b[01;35m\u001b[Kl\u001b[m\u001b[K = net.layers[net.n - 1];\n",
            "               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/tag.c -o obj/tag.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/cifar.c -o obj/cifar.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/go.c -o obj/go.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/batchnorm_layer.c -o obj/batchnorm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/art.c -o obj/art.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/region_layer.c -o obj/region_layer.o\n",
            "\u001b[01m\u001b[K./src/region_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_region_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/region_layer.c:59:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kold_h\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kold_h\u001b[m\u001b[K = l->h;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/region_layer.c:58:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kold_w\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kold_w\u001b[m\u001b[K = l->w;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/reorg_layer.c -o obj/reorg_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/reorg_old_layer.c -o obj/reorg_old_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/super.c -o obj/super.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/voxel.c -o obj/voxel.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/tree.c -o obj/tree.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/yolo_layer.c -o obj/yolo_layer.o\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:68:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.output, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.output_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:75:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.delta, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.delta_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:106:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->output, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:115:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->delta, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kprocess_batch\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:426:25:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kbest_match_t\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                     int \u001b[01;35m\u001b[Kbest_match_t\u001b[m\u001b[K = 0;\n",
            "                         \u001b[01;35m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:707:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_anyobj\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_anyobj\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:706:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_obj\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_obj\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:705:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_cat\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_cat\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:704:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Krecall75\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Krecall75\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:703:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Krecall\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Krecall\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:702:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_ciou_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_ciou_loss\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:701:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_diou_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_diou_loss\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:698:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_ciou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_ciou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:697:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_diou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_diou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:696:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_giou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_giou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:668:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int b, \u001b[01;35m\u001b[Kn\u001b[m\u001b[K;\n",
            "            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/gaussian_yolo_layer.c -o obj/gaussian_yolo_layer.o\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_gaussian_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:71:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.output, batch*l.outputs * sizeof(float), cudaHostRegisterMapped)) l.output_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:78:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.delta, batch*l.outputs * sizeof(float), cudaHostRegisterMapped)) l.delta_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_gaussian_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:110:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->output, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:119:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->delta, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/upsample_layer.c -o obj/upsample_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/lstm_layer.c -o obj/lstm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/conv_lstm_layer.c -o obj/conv_lstm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/scale_channels_layer.c -o obj/scale_channels_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -c ./src/sam_layer.c -o obj/sam_layer.o\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/convolutional_kernels.cu -o obj/convolutional_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/activation_kernels.cu -o obj/activation_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/im2col_kernels.cu -o obj/im2col_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/col2im_kernels.cu -o obj/col2im_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/blas_kernels.cu -o obj/blas_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "\u001b[01m\u001b[K./src/blas_kernels.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid backward_shortcut_multilayer_gpu(int, int, int, int*, float**, float*, float*, float*, float*, int, float*, float**, WEIGHTS_NORMALIZATION_T)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas_kernels.cu:1130:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kstep\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kint \u001b[m\u001b[Kstep = 0;\n",
            "     \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/crop_layer_kernels.cu -o obj/crop_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/dropout_layer_kernels.cu -o obj/dropout_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/maxpool_layer_kernels.cu -o obj/maxpool_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/network_kernels.cu -o obj/network_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "\u001b[01m\u001b[K./src/network_kernels.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfloat train_network_datum_gpu(network, float*, float*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network_kernels.cu:364:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kl\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[K \u001b[m\u001b[K layer l = net.layers[net.n - 1];\n",
            "       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC\" -c ./src/avgpool_layer_kernels.cu -o obj/avgpool_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC obj/image_opencv.o obj/http_stream.o obj/gemm.o obj/utils.o obj/dark_cuda.o obj/convolutional_layer.o obj/list.o obj/image.o obj/activations.o obj/im2col.o obj/col2im.o obj/blas.o obj/crop_layer.o obj/dropout_layer.o obj/maxpool_layer.o obj/softmax_layer.o obj/data.o obj/matrix.o obj/network.o obj/connected_layer.o obj/cost_layer.o obj/parser.o obj/option_list.o obj/darknet.o obj/detection_layer.o obj/captcha.o obj/route_layer.o obj/writing.o obj/box.o obj/nightmare.o obj/normalization_layer.o obj/avgpool_layer.o obj/coco.o obj/dice.o obj/yolo.o obj/detector.o obj/layer.o obj/compare.o obj/classifier.o obj/local_layer.o obj/swag.o obj/shortcut_layer.o obj/activation_layer.o obj/rnn_layer.o obj/gru_layer.o obj/rnn.o obj/rnn_vid.o obj/crnn_layer.o obj/demo.o obj/tag.o obj/cifar.o obj/go.o obj/batchnorm_layer.o obj/art.o obj/region_layer.o obj/reorg_layer.o obj/reorg_old_layer.o obj/super.o obj/voxel.o obj/tree.o obj/yolo_layer.o obj/gaussian_yolo_layer.o obj/upsample_layer.o obj/lstm_layer.o obj/conv_lstm_layer.o obj/scale_channels_layer.o obj/sam_layer.o obj/convolutional_kernels.o obj/activation_kernels.o obj/im2col_kernels.o obj/col2im_kernels.o obj/blas_kernels.o obj/crop_layer_kernels.o obj/dropout_layer_kernels.o obj/maxpool_layer_kernels.o obj/network_kernels.o obj/avgpool_layer_kernels.o -o darknet -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++\n",
            "g++ -std=c++11 -shared -std=c++11 -fvisibility=hidden -DLIB_EXPORTS -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC ./obj/image_opencv.o ./obj/http_stream.o ./obj/gemm.o ./obj/utils.o ./obj/dark_cuda.o ./obj/convolutional_layer.o ./obj/list.o ./obj/image.o ./obj/activations.o ./obj/im2col.o ./obj/col2im.o ./obj/blas.o ./obj/crop_layer.o ./obj/dropout_layer.o ./obj/maxpool_layer.o ./obj/softmax_layer.o ./obj/data.o ./obj/matrix.o ./obj/network.o ./obj/connected_layer.o ./obj/cost_layer.o ./obj/parser.o ./obj/option_list.o ./obj/darknet.o ./obj/detection_layer.o ./obj/captcha.o ./obj/route_layer.o ./obj/writing.o ./obj/box.o ./obj/nightmare.o ./obj/normalization_layer.o ./obj/avgpool_layer.o ./obj/coco.o ./obj/dice.o ./obj/yolo.o ./obj/detector.o ./obj/layer.o ./obj/compare.o ./obj/classifier.o ./obj/local_layer.o ./obj/swag.o ./obj/shortcut_layer.o ./obj/activation_layer.o ./obj/rnn_layer.o ./obj/gru_layer.o ./obj/rnn.o ./obj/rnn_vid.o ./obj/crnn_layer.o ./obj/demo.o ./obj/tag.o ./obj/cifar.o ./obj/go.o ./obj/batchnorm_layer.o ./obj/art.o ./obj/region_layer.o ./obj/reorg_layer.o ./obj/reorg_old_layer.o ./obj/super.o ./obj/voxel.o ./obj/tree.o ./obj/yolo_layer.o ./obj/gaussian_yolo_layer.o ./obj/upsample_layer.o ./obj/lstm_layer.o ./obj/conv_lstm_layer.o ./obj/scale_channels_layer.o ./obj/sam_layer.o ./obj/convolutional_kernels.o ./obj/activation_kernels.o ./obj/im2col_kernels.o ./obj/col2im_kernels.o ./obj/blas_kernels.o ./obj/crop_layer_kernels.o ./obj/dropout_layer_kernels.o ./obj/maxpool_layer_kernels.o ./obj/network_kernels.o ./obj/avgpool_layer_kernels.o src/yolo_v2_class.cpp -o libdarknet.so -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++\n",
            "In file included from \u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:2:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid track_kalman_t::clear_old_states()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:878:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 if ((result_vec_pred[state_id].x > img_size.width) ||\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:879:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     (result_vec_pred[state_id].y > img_size.height))\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ktrack_kalman_t::tst_t track_kalman_t::get_state_id(bbox_t, std::vector<bool>&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:899:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::predict()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:989:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::correct(std::vector<bbox_t>)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:1024:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> Detector::tracking_id(std::vector<bbox_t>, bool, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:370:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         if (\u001b[01;35m\u001b[Kprev_bbox_vec_deque.size() > frames_story\u001b[m\u001b[K) prev_bbox_vec_deque.pop_back();\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:385:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     if (\u001b[01;35m\u001b[Kcur_dist < max_dist\u001b[m\u001b[K && (k.track_id == 0 || dist_vec[m] > cur_dist)) {\n",
            "                         \u001b[01;35m\u001b[K~~~~~~~~~^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:409:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         if (\u001b[01;35m\u001b[Kprev_bbox_vec_deque.size() > frames_story\u001b[m\u001b[K) prev_bbox_vec_deque.pop_back();\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -fPIC -o uselib src/yolo_console_dll.cpp -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++ -L ./ -l:libdarknet.so\n",
            "In file included from \u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:23:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid track_kalman_t::clear_old_states()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:878:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 if ((result_vec_pred[state_id].x > img_size.width) ||\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:879:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     (result_vec_pred[state_id].y > img_size.height))\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ktrack_kalman_t::tst_t track_kalman_t::get_state_id(bbox_t, std::vector<bool>&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:899:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::predict()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:989:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::correct(std::vector<bbox_t>)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:1024:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_boxes(cv::Mat, std::vector<bbox_t>, std::vector<std::__cxx11::basic_string<char> >, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:192:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "             int max_width = (\u001b[01;35m\u001b[Ktext_size.width > i.w + 2\u001b[m\u001b[K) ? text_size.width : (i.w + 2);\n",
            "                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:201:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 int const max_width_3d = (\u001b[01;35m\u001b[Ktext_size_3d.width > i.w + 2\u001b[m\u001b[K) ? text_size_3d.width : (i.w + 2);\n",
            "                                           \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:183:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcolors\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int const \u001b[01;35m\u001b[Kcolors\u001b[m\u001b[K[6][3] = { { 1,0,1 },{ 0,0,1 },{ 0,1,1 },{ 0,1,0 },{ 1,1,0 },{ 1,0,0 } };\n",
            "               \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7cp2NGNPtc3"
      },
      "source": [
        "#IMPORT THE PROJECTS DARKNET.PY FILE\n",
        "import darknet as dn\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q6M9xIpP6iB",
        "outputId": "59bf41fe-0675-43b8-e58e-4058431a445d"
      },
      "source": [
        "import importlib\n",
        "importlib.reload(dn)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'darknet' from '/content/darknet/darknet.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TKfNT36yhD"
      },
      "source": [
        "#HERE WE CREATE A SOFTMAX LAYER FOR PREDICTION\n",
        "probability_model = tf.keras.Sequential([new_model, tf.keras.layers.Softmax()])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rt49KYFQQmE",
        "outputId": "d59ad6e6-47be-4633-9814-aa6272c8f0b5"
      },
      "source": [
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import timeit\n",
        "import sys \n",
        "start = 0\n",
        "#THIS IS THE VERSION WITHOUT THE MUTITHREADING OPTIMIZATION \n",
        "\n",
        "#THIS IS THE METHOD THAT IS CALLED WHEN WE ARE LOOKING TO PARSE THE VIDEO\n",
        "\n",
        "throughput_times = []\n",
        "yolo_times = []\n",
        "full_frame_times = []\n",
        "classifier_times = []\n",
        "q1_times = []\n",
        "q2_times = []\n",
        "crop_times = []\n",
        "\n",
        "def getFrame(sec,cval, net, class_names, colors, lis, start, old_time):\n",
        "\n",
        "    #THIS IS THE VIDEO READER\n",
        "    #HERE WE USE THE SEC VALUE WHICH INCREMENTS THE AMOUNT WE NEED TO GET 900\n",
        "    #FRAMES FROM THE GIVEN VIDEO \n",
        "    framestart = timeit.default_timer()\n",
        "    q1start = timeit.default_timer()\n",
        "\n",
        "    vidcap.set(cv.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames,image = vidcap.read()\n",
        "    total_vehicles = 0\n",
        "    sedan = 0\n",
        "    suv = 0\n",
        "\n",
        "    #ENTER IF THERE IS A FRAME AND WE HAVEN'T READ MORE THAN 900 FRAMES \n",
        "    if hasFrames and cval <= 900:\n",
        "\n",
        "      #WRITE THE FRAME TO DISK \n",
        "      cv.imwrite(\"/content/gdrive/MyDrive/images/image\"+str(count)+\".jpg\", image)     \n",
        "\n",
        "      #GET THE PATH OF THE FRAME AS MY YOLO TAKES THE PATH AND LOOKS UP THE FILE\n",
        "      #THUS WE HAVE TO WRITE TO DISK AND RE READ IN YOLO\n",
        "      path = \"/content/gdrive/MyDrive/images/image\"+str(count)+\".jpg\"\n",
        "\n",
        "      q1stop = timeit.default_timer()\n",
        "      q1_times.append(q1stop - q1start)\n",
        "      q2start = timeit.default_timer()\n",
        "\n",
        "\n",
        "      #HERE Q2 STARTS SO \n",
        "\n",
        "      #GET THE IMAGE AND THE BOUNDING BOX LOCATIONS FROM YOLO\n",
        "      x, bb = YOLO(path, cval, net, class_names, colors)\n",
        "      image_list = []\n",
        "      val = 0\n",
        "      total_vehicles = 0\n",
        "      sedan = 0\n",
        "      suv = 0\n",
        "\n",
        "      #LOOP THROUGH THE BOUNDING BOX LOCATIONS \n",
        "      for b in bb:\n",
        "        if (b[0] == \"car\"):\n",
        "          #GET THE LOCATIONS AND CROP OUT THE PROPOSED REGIONS FOR THE CAR\n",
        "          left, top, right, bottom = dn.bbox2points(b[2])\n",
        "          image_list.append(cropRegions(image, left,right,top,bottom, val))\n",
        "          val = val + 1\n",
        "      i = 0\n",
        "\n",
        "      #LOOP THROUGH THE CROPPED IMAGES \n",
        "      for image in image_list:\n",
        "        total_vehicles = total_vehicles + 1\n",
        "        #SEND EACH IMAGE TO BE CLASSIFIED\n",
        "        sedanres, suvres = classify(image, i)\n",
        "        #INCREMEMNT THE RESULTS BASED ON WHAT IS CLASSIFIED\n",
        "        sedan = sedan + sedanres\n",
        "        suv = suv + suvres\n",
        "        i = i + 1\n",
        "\n",
        "      #CREATE A TUPLE WITH THE RESULTS FOR THE FRAME \n",
        "      x = (sedan, suv, total_vehicles)\n",
        "      #APPEND THE RESULTS TO A LIST \n",
        "      lis.append(x)\n",
        "\n",
        "      time = timeit.default_timer()\n",
        "\n",
        "\n",
        "      #IF WE HAVE DONE 900 FRAMES THEN WE STOP THE TIMER AND RETURN THE THROUGHPUT\n",
        "      if(cval != 1):\n",
        "        throughput_times.append((time - old_time))\n",
        "      \n",
        "\n",
        "      old_time = time\n",
        "\n",
        "      if (cval == 900):\n",
        "        stop = timeit.default_timer()\n",
        "        print('Throughput (FPS) without optimization is: ', (900 / (stop - start))) \n",
        "\n",
        "      q2stop = timeit.default_timer()\n",
        "      q2_times.append(q2stop - q2start)\n",
        "\n",
        "    framestop = timeit.default_timer()\n",
        "    full_frame_times.append(framestop - framestart)\n",
        "\n",
        "    return hasFrames, lis, start, old_time\n",
        "\n",
        "#YOLO FUNCTIONALITY\n",
        "def YOLO(path, cval, net, class_names, colors):\n",
        "  yolostart = timeit.default_timer()\n",
        "\n",
        "  #WEIGHTS AND DATA ARE NEEDED \n",
        "  weights = \"/content/yolov3-tiny.weights\"\n",
        "  data = \"/content/darknet/cfg/coco.data\"\n",
        "\n",
        "  #SET THE THRESHOLDS \n",
        "  nms_thresh = 0.2\n",
        "  thresh = 0.2\n",
        "  hier_thresh = 0.5\n",
        "\n",
        "  #RUN THE DARKNET TINY YOLO DETECTION ON THE IMAGE \n",
        "  det = dn.detect_image(net, class_names, path.encode('utf-8'), thresh=thresh, hier_thresh=hier_thresh, nms=nms_thresh)\n",
        "\n",
        "  #RETUNR THE IMAGE AND THE THE BB LIST OF ALL THE POSSIBLE VEHICLES\n",
        "  newimage, bblist = dn.draw_boxes(det, path, colors)\n",
        "\n",
        "  yolostop = timeit.default_timer()\n",
        "\n",
        "  yolo_times.append(yolostop - yolostart)\n",
        "\n",
        "  return newimage, det\n",
        "\n",
        "#CROP THE REGION OF INTEREST FROM THE IMAGE \n",
        "def cropRegions(image, left,right,top,bottom, val):\n",
        "  cropstart = timeit.default_timer()\n",
        "  #SOMETIMES YOLO CAN GIVE NEGATIVE LOCATIONS WHICH WE SET TO 0 \n",
        "  if left < 0:\n",
        "    left = 0\n",
        "  if right < 0:\n",
        "    right = 0\n",
        "  if top < 0:\n",
        "    top = 0\n",
        "  if bottom < 0:\n",
        "    bottom = 0\n",
        "  \n",
        "  #CROP\n",
        "  image = image[top:bottom, left:right]\n",
        "  crop_stop = timeit.default_timer()\n",
        "  crop_times.append(crop_stop - cropstart)\n",
        "\n",
        "\n",
        "  return image \n",
        "\n",
        "\n",
        "def classify(img, i):\n",
        "  classifierstart = timeit.default_timer()\n",
        "\n",
        "  #WE NEED TO RESIZE THE IMAGE \n",
        "  dim = (416, 416)\n",
        "  resized = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n",
        "\n",
        "  #WE NEED TO GIVE IT ANOTHER DIMENSION TO MATCH THE CLASSIFIER\n",
        "  img = np.expand_dims(resized, axis=0)\n",
        "  #USE OUR MODEL TO REUTN A PROBABILITY OF IT BEING A SUV AND A PROBABILITY OF IT BEING A SEDAN \n",
        "  predictions_single = probability_model.predict(img)\n",
        "  #WE TAKE THE BIGGEST PROB AS OUR PREDICTION\n",
        "  class_res = np.argmax(predictions_single)\n",
        "  sedan = 0\n",
        "  suv = 0\n",
        "\n",
        "  #SET OUT PREDICTION VALUES THAT WE RETURN FOR OUR RESULTS\n",
        "  if class_res == 0:\n",
        "    sedan = 1\n",
        "\n",
        "  elif class_res == 1:\n",
        "    suv = 1\n",
        "\n",
        "  classifierstop = timeit.default_timer()\n",
        "\n",
        "  classifier_times.append(classifierstop - classifierstart)\n",
        "\n",
        "  return sedan, suv\n",
        "\n",
        "#VIDEO LOCATION\n",
        "vidcap = cv.VideoCapture('/content/assignment-clip.mp4')\n",
        "cval = 1\n",
        "\n",
        "count = 1\n",
        "bb = []\n",
        "sec = 0\n",
        "\n",
        "#THIS FRAME RATE GIVES US 900 IMAGES IN 30 SECONDS OF VIDEO \n",
        "frameRate = 1/30 \n",
        "count=1\n",
        "model = \"/content/darknet/cfg/yolov3-tiny.cfg\"\n",
        "weights = \"/content/yolov3-tiny.weights\"\n",
        "data = \"/content/darknet/cfg/coco.data\"\n",
        "\n",
        "#WE NEED THIS NETWORK \n",
        "net, class_names, colors = dn.load_network(model, data, weights, 0)\n",
        "\n",
        "#READ IN THE GROUND TRUTH DATA SO WE CAN COMPARE \n",
        "df = pd.read_excel(\"/content/gdrive/MyDrive/Groundtruth.xlsx\", sheet_name = 'Sheet2')\n",
        "#PUT THE RESULTS INTO THEIR OWN LISTS\n",
        "frames = pd.DataFrame(df['Frame#'])\n",
        "sedans = pd.DataFrame(df['Sedan'])\n",
        "suvs = pd.DataFrame(df['SUV'])\n",
        "total = pd.DataFrame(df['Total'])\n",
        "\n",
        "#CONVERT TO NUMPY\n",
        "total = total.to_numpy()\n",
        "sedans = sedans.to_numpy()\n",
        "suvs = suvs.to_numpy()\n",
        "frames = frames.to_numpy()\n",
        "\n",
        "result = []\n",
        "old_time = 0\n",
        "#START THE TIMER AND SET OFF THE PROCESS OF READING IN THE\n",
        "start = timeit.default_timer()\n",
        "success, res, start, old_time = getFrame(sec,cval, net, class_names, colors, result, start, old_time)\n",
        "\n",
        "#LOOP UNTIL THERE ARE NO MORE FRAMES \n",
        "while success:\n",
        "    cval = cval + 1\n",
        "    count = count + 1\n",
        "    sec = sec + frameRate\n",
        "    success, res, start, old_time = getFrame(sec,cval, net, class_names, colors, result, start, old_time)\n",
        "\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Throughput (FPS) without optimization is:  4.6511023557157785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BC81l_Xlypm",
        "outputId": "2de73aa7-4cd3-4f85-b361-582c4b50ce4e"
      },
      "source": [
        "print(q1_times)\n",
        "print(q2_times)\n",
        "print(throughput_times)\n",
        "print(yolo_times)\n",
        "print(classifier_times)\n",
        "\n",
        "#WE COULD GET THE AVEAGE OF ALL THESE AND PUT THEM INTO A TABLE \n",
        "\n",
        "print(\"The average Q1 time is \", (sum(q1_times)/len(q1_times)))\n",
        "print(\"The average Q2 time is \", (sum(q2_times)/len(q2_times)))\n",
        "print(\"The average Throughput time is \", (sum(throughput_times)/len(throughput_times)))\n",
        "print(\"The average YOLO time is \", (sum(yolo_times)/len(yolo_times)))\n",
        "print(\"The average Classifier time is \", (sum(classifier_times)/len(classifier_times)))\n",
        "print(\"The average cropping time is \", (sum(crop_times)/len(crop_times)))\n",
        "#THEN LETS WRITE Q1 Q2 AND THROUGHPUT TO A .CSV FILE AND PLOT\n",
        "\n",
        "\n",
        "with open('/content/results.csv','w') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['Q1 Times','Q2 Times', 'Throughput'])\n",
        "    for i in range(len(throughput_times)):\n",
        "        resnew = ((q1_times[i]) ,q2_times[i], throughput_times[i])\n",
        "        csv_out.writerow(resnew)\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01830045599490404, 0.01324162400123896, 0.012442182000086177, 0.013803066998661961, 0.014165005995891988, 0.013740015994699206, 0.014395976002560928, 0.01450832100090338, 0.015251813994836994, 0.016528168998775072, 0.01677397499588551, 0.016942333997576497, 0.016584436998527963, 0.04492505599773722, 0.019623940002929885, 0.020814987998164725, 0.020351218998257536, 0.020619558003090788, 0.022784559994761366, 0.021835086001374293, 0.0222445899999002, 0.023003521004284266, 0.023899684005300514, 0.02347621799708577, 0.025067940005101264, 0.02441620599711314, 0.026417772998684086, 0.02791661099763587, 0.027892811995116062, 0.04834463899896946, 0.02712104100646684, 0.028873385002953, 0.027284171999781393, 0.029901196998253, 0.030596724995120894, 0.03313338400039356, 0.03136430199811002, 0.036710789005155675, 0.03210114700050326, 0.032068118001916446, 0.03402253300009761, 0.0346495500052697, 0.03347392599971499, 0.06546436499775155, 0.036256146006053314, 0.036266991002776194, 0.04605087199888658, 0.037465558998519555, 0.04311634099576622, 0.0379898460014374, 0.03658844900201075, 0.04313609399832785, 0.03823725099937292, 0.04182069400121691, 0.03962282800057437, 0.040924105000158306, 0.043961366995063145, 0.050626353004190605, 0.04326610700081801, 0.04188024000177393, 0.04616814299515681, 0.049786194002081174, 0.04610174499975983, 0.044398312995326705, 0.04530566000175895, 0.04616697200253839, 0.0508878070031642, 0.04619171100057429, 0.05193211499863537, 0.05136942099488806, 0.051789700002700556, 0.050938021995534655, 0.04802790300163906, 0.05315859500115039, 0.055058088997611776, 0.0219135000006645, 0.02191498399770353, 0.023433456997736357, 0.022310975000436883, 0.022356184999807738, 0.023611764001543634, 0.02252871599921491, 0.02407285700610373, 0.02373623600578867, 0.02630309599771863, 0.02942516399343731, 0.027065795002272353, 0.027326057002937887, 0.027321239998855162, 0.028542163003294263, 0.028683929995168, 0.03037720599968452, 0.029707631998462602, 0.029098261999024544, 0.02957666799920844, 0.02896434900321765, 0.032420262999949045, 0.030068097003095318, 0.03820325100241462, 0.03798824200202944, 0.03456702500261599, 0.0320130869949935, 0.03553425599966431, 0.033771340997191146, 0.033195650998095516, 0.036152708998997696, 0.034933128001284786, 0.033936884996364824, 0.03532146899669897, 0.035977044004539493, 0.034532934994786046, 0.035018007998587564, 0.05744602600316284, 0.04025820399692748, 0.03600673300388735, 0.040103478997480124, 0.04218629599927226, 0.03854073600086849, 0.04268562499783002, 0.04216673399787396, 0.04200348399899667, 0.042011503006506246, 0.04179635400214465, 0.04507323900179472, 0.04390401899581775, 0.04716947899578372, 0.04417248599929735, 0.04959963000146672, 0.049174412997672334, 0.047656073002144694, 0.04925789499975508, 0.04505332400003681, 0.04955950099974871, 0.046334758000739384, 0.04785713699675398, 0.021645064000040293, 0.02238552000198979, 0.04247764500178164, 0.023560510999232065, 0.023575777995574754, 0.02618111799529288, 0.03019157800008543, 0.025454579998040572, 0.026676380999560934, 0.02735091599606676, 0.03207888699398609, 0.027108283997222316, 0.03006138899945654, 0.030767637996177655, 0.028869806999864522, 0.05667418300436111, 0.03161259499756852, 0.032467592995089944, 0.03106126099737594, 0.03937243100517662, 0.03370869200443849, 0.031041212001582608, 0.03173664800124243, 0.03559607000352116, 0.03251278700190596, 0.03511866099870531, 0.038084029001765884, 0.03521935799653875, 0.05922598799952539, 0.03720648700254969, 0.0380351720014005, 0.03554034400440287, 0.03987374999996973, 0.037551932997303084, 0.03810348600381985, 0.046843887997965794, 0.03955780300020706, 0.040025209993473254, 0.03969135700026527, 0.04247251599736046, 0.04350084000179777, 0.04117004699946847, 0.06586556199908955, 0.04494309699657606, 0.04610143100580899, 0.04354429100203561, 0.04617090999818174, 0.04713080199871911, 0.05321335399639793, 0.048773404996609315, 0.04865819899714552, 0.05091110200010007, 0.055319746999884956, 0.04740657400543569, 0.063291571001173, 0.07034737199865049, 0.04798782699799631, 0.05062745400209678, 0.0475085569996736, 0.05712647600012133, 0.021465416997671127, 0.022634073000517674, 0.02242569500231184, 0.022757304999686312, 0.025196927999786567, 0.02449257200350985, 0.02712494300067192, 0.02458015500451438, 0.025686450993816834, 0.029117825004504994, 0.025439842000196222, 0.027056359002017416, 0.026990435995685402, 0.027073503995779902, 0.028465221002988983, 0.02862147000269033, 0.031456365999474656, 0.03070182700321311, 0.03106009300245205, 0.034883328997239005, 0.031931475998135284, 0.03312848099449184, 0.05481461199815385, 0.030716113004018553, 0.03263960299955215, 0.04119659400021192, 0.0533199319979758, 0.034722092997981235, 0.039139820997661445, 0.03684576499654213, 0.036868110997602344, 0.03623323099600384, 0.035699652005860116, 0.036449964994972106, 0.03685938799753785, 0.06952873399859527, 0.035846844999468885, 0.03640775300300447, 0.0396907510003075, 0.041400949005037546, 0.040673506999155506, 0.04157947599742329, 0.04143587800353998, 0.04173311900376575, 0.04629098199802684, 0.04726625700277509, 0.045901306002633646, 0.07030717599991476, 0.04854940099903615, 0.046937101004004944, 0.04985536599997431, 0.048269252001773566, 0.05055885500041768, 0.0555199430018547, 0.048697484999138396, 0.05189498900290346, 0.04881657700025244, 0.04761833200609544, 0.05272381999384379, 0.05245630400168011, 0.023536859996966086, 0.02554684800270479, 0.023672189003264066, 0.022637942995061167, 0.023833846003981307, 0.023117218996048905, 0.024746413000684697, 0.025962007995985914, 0.02542048799659824, 0.025173852998705115, 0.025947401001758408, 0.033773028997529764, 0.027098220998595934, 0.028029265995428432, 0.026284988001862075, 0.029467693995684385, 0.028591661000973545, 0.03130362400406739, 0.03142917800141731, 0.03132662200368941, 0.03199504299846012, 0.029817603004630655, 0.037735830002930015, 0.03234050199534977, 0.032050173002062365, 0.03153968299739063, 0.03091924400359858, 0.0344881790006184, 0.033418822997191455, 0.06290442500176141, 0.03489430300396634, 0.03572267399431439, 0.03644114200142212, 0.036421873002836946, 0.037454168996191584, 0.03821416199934902, 0.0397362490039086, 0.040604724003060255, 0.0381699520003167, 0.039021655000397004, 0.03731381399848033, 0.041330078995088115, 0.036888050999550615, 0.04562940399773652, 0.04264961300214054, 0.040887027003918774, 0.04423538199625909, 0.04318875400349498, 0.04307474799861666, 0.042951957999321166, 0.045858372999646235, 0.04497443600121187, 0.04979124799865531, 0.0437266169974464, 0.045896928997535724, 0.0462929570057895, 0.07493956099642674, 0.04950740299682366, 0.05103376499755541, 0.04996132299856981, 0.024899869997170754, 0.02222475499729626, 0.022724422997271176, 0.022902554002939723, 0.022957937995670363, 0.02272573699883651, 0.024241996005002875, 0.024732261001190636, 0.025990511996496934, 0.024803943000733852, 0.05115590700006578, 0.028771085999323986, 0.028978724003536627, 0.031011309998575598, 0.030725059994438197, 0.030934196001908276, 0.0328189840001869, 0.029559565999079496, 0.030125807999866083, 0.03331583000544924, 0.03261970899620792, 0.03233631100010825, 0.03197759499744279, 0.06615622399840504, 0.03443492200312903, 0.034211641002912074, 0.033683876994473394, 0.03619703600270441, 0.03501314599998295, 0.03327895500115119, 0.03770248299406376, 0.03481349600042449, 0.0384360580064822, 0.035909405996790156, 0.0382224029963254, 0.038233861996559426, 0.03690298500441713, 0.04421453100076178, 0.040645002998644486, 0.03966193400265183, 0.04241552000166848, 0.04149290100031067, 0.046364554997126106, 0.04135073900397401, 0.042313682002713904, 0.043385090997617226, 0.04563553400657838, 0.04336045600211946, 0.04913074299838627, 0.046299727000587154, 0.04843301600340055, 0.049252996002906, 0.0695312300013029, 0.04928796699823579, 0.048652930003299844, 0.04836838100163732, 0.04885733399714809, 0.05000405500322813, 0.04922603200247977, 0.05445306700130459, 0.022608553001191467, 0.021844467002665624, 0.024169418997189496, 0.02385398700425867, 0.024014474998693913, 0.02554970500204945, 0.0242649329957203, 0.052375892002601177, 0.025964184998883866, 0.025825659999100026, 0.02605099100037478, 0.02575584800069919, 0.03342028000042774, 0.027988145004201215, 0.028040874996804632, 0.02926664899860043, 0.028954989997146185, 0.036043759995664004, 0.04247758099518251, 0.031835321002290584, 0.0490347209997708, 0.032565557994530536, 0.030096467002294958, 0.03378532300121151, 0.03318906399363186, 0.032365456005209126, 0.03496208199794637, 0.03371786099887686, 0.035663258997374214, 0.03408851999847684, 0.0373327289998997, 0.041458847001194954, 0.03734618199814577, 0.03382738200161839, 0.03862868899886962, 0.037210618997050915, 0.06467400900146458, 0.037918640999123454, 0.038653324998449534, 0.04576887599978363, 0.03877347399975406, 0.03609600100026, 0.04021174200170208, 0.04130208300193772, 0.05249796199495904, 0.04632064700126648, 0.043490955999004655, 0.042648070004361216, 0.043555152995395474, 0.046483284000714775, 0.04669750299945008, 0.045632474997546524, 0.06587943399790674, 0.04546019199915463, 0.04991217199858511, 0.04573024799901759, 0.04840510599751724, 0.053100246004760265, 0.05102233900106512, 0.0483266180017381, 0.02011327099899063, 0.02039056999637978, 0.022021725002559833, 0.02260251400002744, 0.027368210998247378, 0.02307170299900463, 0.022347903999616392, 0.023862679001467768, 0.024240174003352877, 0.02806117099680705, 0.025742429003003053, 0.025263031006033998, 0.025363016997289378, 0.026565485997707583, 0.027107552996312734, 0.02622089599753963, 0.027542494994122535, 0.030224016998545267, 0.028193543999805115, 0.03088733799813781, 0.03171572700375691, 0.0354715510038659, 0.031755941003211774, 0.0324669809997431, 0.03234284600330284, 0.03866522399766836, 0.0385079829939059, 0.03454747600335395, 0.035513850998540875, 0.0389975290017901, 0.03735072199924616, 0.03760136300115846, 0.045666730002267286, 0.0395898010028759, 0.039345559998764656, 0.03980408200004604, 0.03836036199936643, 0.039028974999382626, 0.04050913099490572, 0.0465653770006611, 0.04165185899910284, 0.04441059599776054, 0.04234619600174483, 0.04479423299926566, 0.04228271299507469, 0.04435609599750023, 0.04332983199856244, 0.04500832499616081, 0.047367941995617, 0.04721603000507457, 0.04649828199762851, 0.04845734700211324, 0.04992161599511746, 0.057101664002402686, 0.053510381003434304, 0.05444956900464604, 0.050512997004261706, 0.049044086001231335, 0.056432800003676675, 0.0554505159961991, 0.02193123100005323, 0.02496348899876466, 0.023262953996891156, 0.0243674790035584, 0.02621268200164195, 0.05143913700158009, 0.02947950499947183, 0.030901291000191122, 0.029297360000782646, 0.0327327859995421, 0.028539063001517206, 0.031740644000819884, 0.02843591299460968, 0.03047309299290646, 0.03394280499924207, 0.033555863999936264, 0.033508690998132806, 0.061110845002986025, 0.03589304300112417, 0.035055644999374636, 0.03613972500170348, 0.03731920599966543, 0.03683463899506023, 0.0388785960021778, 0.03739471099834191, 0.04107438299979549, 0.036720313000842, 0.03933890399639495, 0.043312014000548515, 0.045375893998425454, 0.06650801700016018, 0.04630448499665363, 0.04255110299709486, 0.04372852100641467, 0.050080213994078804, 0.046930090997193474, 0.045551842005806975, 0.04666601200005971, 0.04356477599503705, 0.05281945499882568, 0.050361420995614026, 0.05594125200150302, 0.04803795399493538, 0.05338453800504794, 0.07374632099526934, 0.053035819000797346, 0.05279775999952108, 0.054919319998589344, 0.056457489998138044, 0.05526947500038659, 0.05475635399488965, 0.05706821100466186, 0.05629909699928248, 0.05807198799448088, 0.057684104998770636, 0.056684012997720856, 0.05751071000122465, 0.05896239699359285, 0.0603829579995363, 0.06425222100369865, 0.02344792999792844, 0.025606969000364188, 0.024556188996939454, 0.025775107998924796, 0.025740129000041634, 0.02676680399599718, 0.03144439999596216, 0.02697849799733376, 0.028375734997098334, 0.030825387002550997, 0.032783404996735044, 0.052068318997044116, 0.03064675100176828, 0.03205303099821322, 0.03392628599976888, 0.03489855099905981, 0.03512737900018692, 0.035466378001729026, 0.03565537700342247, 0.03832202199555468, 0.03731313699972816, 0.040610228999867104, 0.03831013199669542, 0.042045953996421304, 0.04271795300155645, 0.06707740599813405, 0.042149705994233955, 0.044220846000825986, 0.04478485199797433, 0.048135598000953905, 0.0464840019994881, 0.04614515400317032, 0.050504102000559215, 0.05003862999728881, 0.05386627699772362, 0.0526216009966447, 0.05385163399478188, 0.056213662996015046, 0.05189352900197264, 0.05243891900317976, 0.05266641399794025, 0.055667888998868875, 0.056696290004765615, 0.05723246499837842, 0.06387597999855643, 0.05766045999916969, 0.062352299006306566, 0.06238938799651805, 0.06434614500176394, 0.0903495140009909, 0.06590658299683128, 0.06121258100029081, 0.06248835699807387, 0.06191203000344103, 0.0635110529983649, 0.06627720200049225, 0.07085174099483993, 0.0710207590018399, 0.06885821199830389, 0.06842801599850645, 0.022819354002422187, 0.0542380460028653, 0.027658142003929242, 0.028455769002903253, 0.026966203993652016, 0.02857479199883528, 0.02950314299960155, 0.03035091600031592, 0.03216694200091297, 0.030057782998483162, 0.032952572000795044, 0.03211921099864412, 0.05367590099922381, 0.036533297003188636, 0.03416678800567752, 0.03723457799787866, 0.03765712399763288, 0.03768846499588108, 0.03619340200384613, 0.035061132002738304, 0.03673664800589904, 0.04051662299752934, 0.03899545599415433, 0.04012488899752498, 0.04232774700358277, 0.04159076100040693, 0.040472266999131534, 0.040183684999647085, 0.043024964004871435, 0.04440822199831018, 0.047483595000812784, 0.04517962999670999, 0.04142432299704524, 0.046253296997747384, 0.0471822899999097, 0.0438500180025585, 0.04489136699703522, 0.07681753399810987, 0.0475773569996818, 0.04633916499733459, 0.048023690993431956, 0.04839718199946219, 0.04993568899953971, 0.04780805600603344, 0.05158276599831879, 0.05057691800175235, 0.05300116999569582, 0.05024434199731331, 0.05737680300080683, 0.07892255899787415, 0.051270356998429634, 0.05174815699865576, 0.052667067997390404, 0.05846495500009041, 0.05528786599461455, 0.058006405997730326, 0.060786235000705346, 0.056886147998739034, 0.05642764600634109, 0.06578546600212576, 0.023405521002132446, 0.023799581998900976, 0.024980982001579832, 0.027004595001926646, 0.028405541997926775, 0.026701079004851636, 0.02790025399735896, 0.029564591000962537, 0.029296487002284266, 0.030455464002443478, 0.03121186199859949, 0.03256842299742857, 0.033658127998933196, 0.05961113699595444, 0.035984396999992896, 0.034909439003968146, 0.03701205000106711, 0.034954449998622295, 0.04015323700150475, 0.03847890800534515, 0.035579408999183215, 0.039563569000165444, 0.03821829499793239, 0.03975220100255683, 0.03813099599938141, 0.044269120997341815, 0.06670396000117762, 0.041454204998444766, 0.04210277100355597, 0.04979962800280191, 0.03989618500054348, 0.04429106099996716, 0.040848746000847314, 0.053260007000062615, 0.04129025499423733, 0.044016567000653595, 0.04784728000231553, 0.04792710600304417, 0.04357877299480606, 0.04475077600363875, 0.04863917400507489, 0.048128278998774476, 0.04822180799965281, 0.0478149789996678, 0.049767927994253114, 0.04944142900058068, 0.05278415799693903, 0.053496023996558506, 0.05454004200146301, 0.05383989000256406, 0.054322611998941284, 0.052297469999757595, 0.05814778199419379, 0.05492770599812502, 0.053173050000623334, 0.05601698000100441, 0.05688147500040941, 0.055621444997086655, 0.057927723006287124, 0.05943040199781535, 0.022186481997778174, 0.023675905998970848, 0.02370750199770555, 0.02196517000265885, 0.02429621000192128, 0.02543919200252276, 0.024704154006030876, 0.031482125996262766, 0.05618848099402385, 0.02772297599585727, 0.02790685299987672, 0.027712711002095602, 0.02731899199716281, 0.03138510400458472, 0.03172306299529737, 0.03284236900071846, 0.03517767300218111, 0.03233210899634287, 0.032535005993850064, 0.03197259500302607, 0.03360016700025881, 0.033874392000143416, 0.06177005600329721, 0.03775274199870182, 0.03596277099859435, 0.0363432010053657, 0.03790569000557298, 0.03697864599962486, 0.03938474899769062, 0.040518820998840965, 0.038491780003823806, 0.037472097006684635, 0.045622948004165664, 0.03843860600318294, 0.07027710300462786, 0.04011927600367926, 0.04186264999589184, 0.041994058003183454, 0.0494140110022272, 0.04075978699984262, 0.04385806200298248, 0.04830008700082544, 0.048451256996486336, 0.04612733700196259, 0.04498402599710971, 0.046919318003347144, 0.0489092569987406, 0.06876115100021707, 0.045990472004632466, 0.0481263229958131, 0.05100535900419345, 0.0502915339966421, 0.04988668799342122, 0.0547195820035995, 0.056295521004358307, 0.050292849002289586, 0.05074996600160375, 0.0538346450048266, 0.04995656399842119, 0.051235316997917835, 0.04408854500070447, 0.023197230999358, 0.02355338499910431, 0.025296435997006483, 0.024526250002963934, 0.022903432000020985, 0.024740147004195023, 0.02470428399828961, 0.027316883999446873, 0.027492624998558313, 0.026230267001665197, 0.026878007003688253, 0.03125788699981058, 0.02863450400036527, 0.050691539006948005, 0.030968279999797232, 0.02693689199804794, 0.0310979200003203, 0.030884313993738033, 0.03125447899947176, 0.03212894900207175, 0.03373622999788495, 0.03634384099859744, 0.03453972400166094, 0.039384895004332066, 0.0343789629987441, 0.03489504900062457, 0.06140693600173108, 0.03654755900060991, 0.03634399099973962, 0.036724485995364375, 0.03754794199630851, 0.037852433997613844, 0.03791989399906015, 0.04069956600142177, 0.04196602499723667, 0.044483281002612785, 0.03908952599886106, 0.04900792099942919, 0.04464970100525534, 0.041565827996237203, 0.0419510520005133, 0.04777399000158766, 0.04164668700104812, 0.04611817299883114, 0.046254377004515845, 0.05084428400004981, 0.04344593999849167, 0.044693951997032855, 0.05303644599916879, 0.06219922199670691, 0.05088160100422101, 0.05107471300289035, 0.05464563499845099, 0.07535512399772415, 0.05011867899884237, 0.04979562800144777, 0.052102095003647264, 0.04796457199699944, 0.05300390600314131, 0.02234845200291602, 0.02946396599872969, 0.02740668399928836, 0.025432838003325742, 0.02314167500298936, 0.02424626800348051, 0.023891967000963632, 0.025199950003298, 0.05017403500096407, 0.02610298900253838, 0.026999656001862604, 0.028347314000711776, 0.026888636995863635, 0.028578122997714672, 0.028828260998125188, 0.032236061997537035, 0.0296600470028352, 0.030594586998631712, 0.035034264998103026, 0.02969943299831357, 0.0336424929992063, 0.03416429700155277, 0.05728466300206492, 0.03406017499946756, 0.035205769003368914, 0.03729357200063532, 0.03946401499706553, 0.0369296819990268, 0.03811377399688354, 0.040927816000476014, 0.03560443199967267, 0.036871097996481694, 0.041752024000743404, 0.04274817799887387, 0.035310781000589486, 0.03882409200014081, 0.041256092001276556, 0.042090635004569776, 0.044436244999815244, 0.037518648998229764, 0.039650654995057266, 0.04230256400478538, 0.041479364001133945, 0.04494215299928328, 0.05196645199612249]\n",
            "[0.1616827820034814, 0.16236796299926937, 0.15827693000028376, 0.16053654299321352, 0.1655731439968804, 0.15755658299895003, 0.15491299399582203, 0.15812200700020185, 0.15430474900495028, 0.15025187799619744, 0.1647311400010949, 0.15675040199857904, 0.1550806589948479, 0.17071177600155352, 0.20592637300433125, 0.20255430900579086, 0.1556091720049153, 0.19964101300138282, 0.2032202269983827, 0.19655885999964084, 0.20106923099956475, 0.20301258899417007, 0.20194274899404263, 0.20416732400190085, 0.2017935270050657, 0.19846270699781599, 0.194184376996418, 0.19944016900262795, 0.20011101600539405, 0.20095837600092636, 0.1955769840060384, 0.21037499299563933, 0.20608503100083908, 0.20612566499767127, 0.20241307800461072, 0.19646078500227304, 0.21112550700490829, 0.19960268199793063, 0.20999569500418147, 0.20295976100169355, 0.20980319500085898, 0.2042798539987416, 0.205551295002806, 0.201857131003635, 0.19468419100303436, 0.21466401599900564, 0.2019751140032895, 0.20712623000144958, 0.19935930800420465, 0.20718136899813544, 0.20594481599982828, 0.21190529799787328, 0.19842144399444805, 0.20057760900090216, 0.20747750400187215, 0.1798250529973302, 0.1631443689984735, 0.15951476099871797, 0.15864413999952376, 0.1644418349969783, 0.16002310500334715, 0.15575500600243686, 0.11157683299825294, 0.12005606599996099, 0.11213827899337048, 0.113003906000813, 0.11535348500183318, 0.1623623889972805, 0.06957641700137174, 0.13342283700330881, 0.15660175700031687, 0.06869802200526465, 0.12372505800158251, 0.11804896799731068, 0.11130200100160437, 0.11886580300051719, 0.11689986199780833, 0.06783721799729392, 0.06904311700054677, 0.0694894469997962, 0.07139108599949395, 0.1175889539954369, 0.1105591040031868, 0.1284822610032279, 0.13182021400280064, 0.11871949399937876, 0.11159612499614013, 0.11875094599963631, 0.11318619200028479, 0.11443254500045441, 0.11272393200488295, 0.11296500299795298, 0.12515649700071663, 0.112978896999266, 0.11225863399886293, 0.11157052600174211, 0.12159788099961588, 0.11417144499864662, 0.13439679000293836, 0.11182313299650559, 0.11226983799861046, 0.1269097900003544, 0.11728815599781228, 0.11133284799871035, 0.12223186399933184, 0.12438893799844664, 0.11929726100061089, 0.11555520300316857, 0.11603551000007428, 0.11635777999617858, 0.12010073199780891, 0.1145071499995538, 0.11947641400183784, 0.10736652999912621, 0.120525678001286, 0.1159703899975284, 0.15450971100653987, 0.15676700299809454, 0.16668153699720278, 0.1571279700001469, 0.16049597899836954, 0.16730657900188817, 0.1644249849996413, 0.1678994120011339, 0.16977860800398048, 0.15898440099408617, 0.15535964699665783, 0.16940038700704463, 0.1534464759970433, 0.2055104940009187, 0.1968556920037372, 0.20522779600287322, 0.20959820899588522, 0.1979641840007389, 0.20538391500303987, 0.20707949699863093, 0.20680861599976197, 0.20369640900025843, 0.2023422690035659, 0.1982815719966311, 0.1944644379982492, 0.21296764600265305, 0.20215196999924956, 0.20964271999400808, 0.1996277169964742, 0.2039354650041787, 0.1979291600000579, 0.14915967699926114, 0.16183228800218785, 0.2052078269989579, 0.20849674400233198, 0.20415065300039714, 0.11395057500340044, 0.16443607000110205, 0.20639802399819018, 0.20605033300671494, 0.19907003499974962, 0.19834043500304688, 0.2195233210004517, 0.1565677590042469, 0.17151838399877306, 0.15490817499812692, 0.1564589019981213, 0.1693684799975017, 0.15567855799599783, 0.1558542769998894, 0.1670794739984558, 0.15995853899948997, 0.1966547880001599, 0.21414729199750582, 0.20110699699580437, 0.19830642500164686, 0.16818779200548306, 0.15818355300143594, 0.1592710140030249, 0.15308072699554032, 0.1574601629981771, 0.17138100799638778, 0.1651118800000404, 0.1585595370052033, 0.16375183100171853, 0.16332019199762726, 0.15414392099773977, 0.1646021069973358, 0.1601299729954917, 0.15711821400327608, 0.1615581040023244, 0.16185539699654328, 0.15625855499820318, 0.1597824210039107, 0.15906232799898135, 0.15442386799986707, 0.16165193100459874, 0.16268305200355826, 0.1570922180035268, 0.16632666700024856, 0.1597605460046907, 0.15508354300254723, 0.1644786250035395, 0.16228798300289782, 0.1634440199995879, 0.15957541000534547, 0.15904765800223686, 0.1748162629955914, 0.16673866200289922, 0.16121125999779906, 0.15596886099956464, 0.17477665899787098, 0.1562732460006373, 0.1534926920066937, 0.16811525199591415, 0.16767643099592533, 0.1581131679995451, 0.1610567629977595, 0.16136845700384583, 0.15286909700080287, 0.17294062800647225, 0.1628693380043842, 0.16172353999718325, 0.16988705399853643, 0.15592387299693655, 0.1577298019983573, 0.1670387570047751, 0.2004469519961276, 0.21344334699824685, 0.203566916003183, 0.1959038339991821, 0.20530751300248085, 0.2000063820014475, 0.20018176400481025, 0.1981306189991301, 0.20897672799765132, 0.2117922739998903, 0.202261605001695, 0.20302799199998844, 0.19604183399496833, 0.2063275309992605, 0.20304092900187243, 0.21347040299588116, 0.2019175509994966, 0.20640827599709155, 0.20549997800117126, 0.21393244399951072, 0.20978545700199902, 0.20155916400108254, 0.16573417299514404, 0.15657786899828352, 0.1550618250039406, 0.1669848550009192, 0.15457210600288818, 0.15896935899945674, 0.16088130999560235, 0.16043124200223247, 0.1514687020026031, 0.1712755569969886, 0.17334666100214235, 0.15535354299936444, 0.16573090600286378, 0.1559926030022325, 0.15608619800332235, 0.16337133299384732, 0.1581647420025547, 0.1549082570054452, 0.1661258629974327, 0.16067820700118318, 0.1592307989994879, 0.16715254899463616, 0.16416481100168312, 0.1506328690011287, 0.6853407449962106, 0.15549644999555312, 0.1627501300026779, 0.16028041900426615, 0.1603523839949048, 0.1604422460004571, 0.16737570000259439, 0.1645857289986452, 0.16126472700125305, 0.1688009349963977, 0.15870152400020743, 0.15521513600106118, 0.1678106140025193, 0.15839584600325907, 0.16018327300116653, 0.16813065000314964, 0.16023425499588484, 0.16180185999837704, 0.1715265820021159, 0.1591551819947199, 0.15850814200530294, 0.16609276799863437, 0.16285938899818575, 0.16451941700506723, 0.2143820470009814, 0.16057929200178478, 0.15909490400372306, 0.1812335200011148, 0.18300185699627036, 0.16315069700067397, 0.16458024000166915, 0.15912809800647665, 0.15852481099864235, 0.15805589099909412, 0.15274126199801685, 0.15858118200412719, 0.20281667599920183, 0.20560114399995655, 0.21787562400277238, 0.20137266800156794, 0.20540058800543193, 0.2016234650000115, 0.21250160400086315, 0.2086925080002402, 0.19994587799737928, 0.1671304759947816, 0.20018458399863448, 0.1629257120002876, 0.15871094500471372, 0.15415014600148425, 0.16829359200346516, 0.1575110929989023, 0.16140270100004273, 0.16227406800317112, 0.15833767000003718, 0.15685351899446687, 0.16193822599598207, 0.15878530100599164, 0.15818590800336096, 0.21404141599487048, 0.20547062200057553, 0.15697437100607203, 0.2149480170046445, 0.19888104199344525, 0.11896833500213688, 0.12240220300009241, 0.1606600479935878, 0.1529388380004093, 0.1614620790060144, 0.1570842229994014, 0.1584829150015139, 0.1614814380009193, 0.2035838430019794, 0.21187700200243853, 0.1958261190011399, 0.20876515999407275, 0.20284685700607952, 0.20555548600532347, 0.21117514399520587, 0.20372341300389962, 0.20714076999865938, 0.15589453899883665, 0.2325234609961626, 0.20482815899595153, 0.15567712299525738, 0.2055972739981371, 0.20641563899698667, 0.16353042500122683, 0.11467232299764873, 0.11485292600264074, 0.15739962499355897, 0.12364293499558698, 0.1202349189989036, 0.11338144000183092, 0.11244798099505715, 0.11100142099894583, 0.12018793700553942, 0.11659131800115574, 0.11571040000126231, 0.11309193799388595, 0.12422625700128265, 0.11522976100241067, 0.11621035600546747, 0.11647456499485997, 0.11457451600290369, 0.12427341000147862, 0.11727137200068682, 0.1177074610022828, 0.11329030899651116, 0.11971887999970932, 0.15443013000185601, 0.15409811100107618, 0.15935403399635106, 0.15558407000207808, 0.1567038200009847, 0.1656143990039709, 0.15811972899973625, 0.20127313600096386, 0.2065703839980415, 0.19910816299670842, 0.15995691400166834, 0.15694649900251534, 0.1536157340015052, 0.16279374599980656, 0.15953186000115238, 0.15645927299920004, 0.16214732699882006, 0.15741006899770582, 0.15447380000114208, 0.15898481699696276, 0.15492432400060352, 0.15510257400455885, 0.16656146899913438, 0.15698005499871215, 0.15683064900076715, 0.16485988200292923, 0.15693862400075886, 0.11572331200295594, 0.15351246199861635, 0.12615364500379656, 0.11721854700590484, 0.1174818880026578, 0.11694771199836396, 0.12361131999932695, 0.11650690299575217, 0.12084526499529602, 0.1166778129991144, 0.10805861099652248, 0.1241017309948802, 0.11265470799844479, 0.11347121900325874, 0.11543316399911419, 0.11754250800004229, 0.11324881899781758, 0.1086010709987022, 0.11664462099724915, 0.11124693299643695, 0.1252506809978513, 0.11797202000161633, 0.11391862400341779, 0.1139334309991682, 0.12197164200188126, 0.11427964999893447, 0.11570214899984421, 0.15536759999667993, 0.15794855899730464, 0.11309884500224143, 0.11240088599879527, 0.114822761999676, 0.16306358799920417, 0.11440930800017668, 0.11268323099648114, 0.10966724099853309, 0.1583238480016007, 0.17783727499772795, 0.10977653599547921, 0.11229569299757713, 0.11771527000382775, 0.10925500600569649, 0.11048626599949785, 0.10711885500495555, 0.11860164200334111, 0.1143594929963001, 0.11280008000176167, 0.11251480699866079, 0.11099274199659703, 0.12204070400184719, 0.11353351799334632, 0.11893615499866428, 0.11258447200088995, 0.14421792299981462, 0.11494709199905628, 0.11136651500419248, 0.11914225700456882, 0.11405390800064197, 0.1228782219986897, 0.11508349300129339, 0.12046714199823327, 0.11368258099537343, 0.12390848599898163, 0.10995639800239587, 0.11443294800119475, 0.11814023700571852, 0.11310835100448458, 0.18673355499777244, 0.16206883900304092, 0.15769719800300663, 0.16237127700151177, 0.16014052299578907, 0.20425490000343416, 0.19915930700517492, 0.19324602500273613, 0.21102426599827595, 0.19826785400073277, 0.20146446299622767, 0.2030743139985134, 0.20871912199800136, 0.199235841006157, 0.17763311999442521, 0.20839216499734903, 0.15677813500224147, 0.20932855299906805, 0.1636618839984294, 0.16632539500278654, 0.1664495290024206, 0.16173340100067435, 0.1576046519985539, 0.1681995699982508, 0.2077617309987545, 0.21048478900047485, 0.16170645599777345, 0.19635328000003938, 0.20449217400164343, 0.20152204800251639, 0.21419280100235483, 0.2036043299958692, 0.1568344040060765, 0.20639108299656073, 0.24111660600465257, 0.20945027400011895, 0.19981299999926705, 0.20916873299574945, 0.16406800000549993, 0.169995372001722, 0.12121284900058527, 0.11834479100070894, 0.07233096799609484, 0.07332809099898441, 0.07055073299852666, 0.07104197599983308, 0.06885657500242814, 0.07805411799927242, 0.0724657200044021, 0.07172924099722877, 0.07072124800470192, 0.07203286200092407, 0.07385817600152222, 0.07139272899803473, 0.06998371399822645, 0.06744679000257747, 0.07688607899763156, 0.07099002399627352, 0.0712752410036046, 0.07175652399746468, 0.07062108199897921, 0.11331108000013046, 0.11148761100048432, 0.12843595899903448, 0.11475008199340664, 0.11348995799926342, 0.11761087100603618, 0.12126972300029593, 0.11532578200421995, 0.11347632900287863, 0.11805734699737513, 0.11110104500403395, 0.12642009800038068, 0.1144379939942155, 0.11319848399580223, 0.11642199999914737, 0.12625141500029713, 0.1136376209979062, 0.11549339200428221, 0.1357650290010497, 0.11580103100277483, 0.12211808899883181, 0.11448144399764715, 0.11568272799922852, 0.11273096899822121, 0.12412670200137654, 0.11724148199573392, 0.11379010499513242, 0.11266138799692271, 0.11262257600174053, 0.12169008500495693, 0.07054799600155093, 0.07020961899979739, 0.12230352899496211, 0.11453234799409984, 0.12300305200187722, 0.11443994000001112, 0.06889581799623556, 0.11424007499590516, 0.11407265500020003, 0.1214334210017114, 0.024393078005232383, 0.07180614399840124, 0.025627000999520533, 0.0693286449968582, 0.06939450999925612, 0.024208546004956588, 0.028947913000592962, 0.07036515200161375, 0.11503225600608857, 0.11654920200089691, 0.1232037770023453, 0.11443582899664761, 0.10996794100356055, 0.11379232599574607, 0.1252365819964325, 0.15568115900532575, 0.1996838260020013, 0.20727908900153125, 0.22691213600046467, 0.1651205399975879, 0.24846079699636903, 0.21126960599940503, 0.2424436559958849, 0.24707222099823412, 0.2390379389980808, 0.24840496400429402, 0.2848838360005175, 0.29693900299753295, 0.20906560200091917, 0.24122514900227543, 0.25502669300476555, 0.24431699800334172, 0.24565912399702938, 0.23845305199938593, 0.25286917600169545, 0.24531030600337544, 0.2547924509999575, 0.2536311570001999, 0.2355704669971601, 0.29536127699975623, 0.28559448799933307, 0.2513288120026118, 0.28769682699930854, 0.2485476430010749, 0.2986755469974014, 0.2411942109974916, 0.2529846350007574, 0.24851644899899838, 0.2525155100011034, 0.24552023399883183, 0.24756739500298863, 0.242905590996088, 0.24876026499987347, 0.2134496630023932, 0.20124526799918385, 0.20504057799553266, 0.19495765100145945, 0.2012103729939554, 0.2173456119999173, 0.1954569259978598, 0.21303935699688736, 0.1994836320009199, 0.21117516799859004, 0.19794326099508908, 0.20577668299665675, 0.2339760649992968, 0.21405818899802398, 0.20173464300023625, 0.19641873399814358, 0.20557382800325286, 0.19970890800323104, 0.2164929389982717, 0.20358126099745277, 0.2051544060013839, 0.19961023899668362, 0.2058225620057783, 0.24452885700156912, 0.2034890769937192, 0.20284853100019973, 0.19614579899644013, 0.204405577002035, 0.19664597699738806, 0.21378445799928159, 0.19832232200133149, 0.20270920100301737, 0.19888924500264693, 0.20518973700382048, 0.19662659899768187, 0.19960403900040546, 0.21093381999526173, 0.20713489699846832, 0.20697627199842827, 0.20032544800051255, 0.20657727599609643, 0.2484428370007663, 0.2486639990020194, 0.2529092020049575, 0.2524170300021069, 0.2534981639983016, 0.24676118599745678, 0.25372940200031735, 0.262661061999097, 0.24945296200166922, 0.24532045600062702, 0.2516438370003016, 0.2520523619969026, 0.24773089600057574, 0.2445423919998575, 0.25554909300262807, 0.25602173899824265, 0.2405346110026585, 0.2517619869977352, 0.2486537020013202, 0.24652255199907813, 0.2487035049998667, 0.253588038001908, 0.24243372100318084, 0.25214746499841567, 0.25137619199813344, 0.24195947899715975, 0.2565952799996012, 0.2565850860046339, 0.2616371510011959, 0.24485794000065653, 0.25103990099887596, 0.24317678599618375, 0.2505844370025443, 0.25023842599330237, 0.24647051799547626, 0.24324684600287583, 0.24509154800034594, 0.2498267949995352, 0.24508225099998526, 0.24867705500219017, 0.24846260199410608, 0.25165238700719783, 0.20110502400348196, 0.7120342010020977, 0.20837753399973735, 0.1979187999968417, 0.2669163489990751, 0.2011281279992545, 0.20655345499835676, 0.20239878600114025, 0.19843272199796047, 0.20645035400229972, 0.19922513899655314, 0.19757982499868376, 0.20898937500169268, 0.19803215399588225, 0.20530646500264993, 0.19863112400344107, 0.2098239850020036, 0.15739161599776708, 0.1762471199981519, 0.1606544720052625, 0.19442078499560012, 0.2023677420002059, 0.20496590200491482, 0.19784863700624555, 0.16322233399841934, 0.15158832499582786, 0.16294226199534023, 0.20381919400097104, 0.20006647600530414, 0.20992720299545908, 0.1585979240044253, 0.15794101200299338, 0.16631717100244714, 0.15815184399980353, 0.15804994400241412, 0.21122308000485646, 0.20221398300054716, 0.21085766999749467, 0.20978869299869984, 0.21323610599938547, 0.19901576799748, 0.20722422200196888, 0.20175049699901138, 0.19921065099333646, 0.21024014299473492, 0.20243052099976921, 0.20305841099616373, 0.20619239700317848, 0.21500656900025206, 0.2078708339977311, 0.20641567099664826, 0.20214688399573788, 0.19700906600337476, 0.225554883996665, 0.24153849499998614, 0.25601360199652845, 0.2493660140025895, 0.24026402099843835, 0.2519749239945668, 0.20946134500263724, 0.20824341499974253, 0.15478319700196153, 0.2008131399998092, 0.2075812249968294, 0.1520862670004135, 0.21006308100186288, 0.1563001649992657, 0.15032769899698906, 0.16959946200222475, 0.16304804399987916, 0.1578626729969983, 0.1608559160013101, 0.1657628670000122, 0.20069032400351716, 0.16187871199508663, 0.20226897399697918, 0.15470679999998538, 0.2031384119982249, 0.20164930800092407, 0.19998805200157221, 0.20693049200053792, 0.20343090200185543, 0.20586799699958647, 0.20912585799669614, 0.24884805999317905, 0.25257841699931305, 0.20938894700520905, 0.197531940000772, 0.24793052499444457, 0.2496003939959337, 0.23843220499838935, 0.24578513400047086, 0.2402086860020063, 0.24671868199948221, 0.24176931999681983, 0.25264404399786144, 0.25105822299519787, 0.19838063800125383, 0.20077005500206724, 0.20795100800023647, 0.19939843699830817, 0.21052041799703147, 0.1999404520029202, 0.20382157200219808, 0.20271962900005747, 0.20534020899503957, 0.20792420100042364, 0.19828725799743552, 0.21196051399601856, 0.20137209899985464, 0.20929302099830238, 0.20467536700016353, 0.21250088800297817, 0.20108021500345785, 0.2058521099970676, 0.20870313700288534, 0.2007104989970685, 0.20883631700417027, 0.20832058099767892, 0.20129014299891423, 0.19871938399592182, 0.20752314400306204, 0.19692757899611024, 0.21026173700374784, 0.2018627329962328, 0.19790366899542278, 0.23091956199641572, 0.20320119700045325, 0.2031624430019292, 0.20394016199861653, 0.20881128600012744, 0.19839244699687697, 0.2000362819962902, 0.20147373100189725, 0.1968329570008791, 0.20762821000244003, 0.20186920699779876, 0.20813900500070304, 0.2007947320016683, 0.20939402399380924, 0.20554336700297426, 0.21008541600167518, 0.20372558000235585, 0.1977030880007078, 0.20777031499892473, 0.20433477800543187, 0.2085089749962208, 0.20514460100093856, 0.20564294799987692, 0.20008269800018752, 0.2089771640021354, 0.20593074600037653, 0.20689394700457342, 0.21164199399936479, 0.19748305100074504, 0.21616589699988253, 0.20092311900225468, 0.21250934999989113, 0.20732306900026742, 0.21194793399627088, 0.19907676400180208, 0.19402368000010028, 0.2071171179995872, 0.20559039100044174, 0.21227005300170276, 0.201972904993454, 0.208559904996946, 0.20110040200233925, 0.21067234499787446, 0.20127761399635347, 0.19341263100068318, 0.224126064000302, 0.2065661859960528, 0.20337737000227207, 0.20030404499993892, 0.20607182400271995, 0.1996823890003725, 0.20679336900502676, 0.20725750899873674, 0.20415852500445908, 0.22510311800579075, 0.20768397399660898, 0.2110784699980286, 0.20247254200512543, 0.20353629400051432, 0.19952295899565797, 0.2052905929958797, 0.19790456799819367, 0.19883581199974287, 0.2023317200000747]\n",
            "[0.17600234499695944, 0.17074900200532284, 0.17440157999953954, 0.179775119999249, 0.1713345929965726, 0.1693359480050276, 0.1726576250002836, 0.1695889089969569, 0.1668211380019784, 0.18152990399539704, 0.17372040600457694, 0.17169098599697463, 0.21566565900138812, 0.22559099800128024, 0.22340313499444164, 0.17599116600467823, 0.22028643899830058, 0.22602869300317252, 0.21842368799843825, 0.2233399089964223, 0.22607460300059756, 0.22587044700048864, 0.22767018400190864, 0.22689025699946797, 0.22290556300140452, 0.22066156700020656, 0.22738194299745373, 0.22804862500197487, 0.24933191500167595, 0.22272465899732197, 0.2392744210010278, 0.23339768299774732, 0.2360522300004959, 0.2330355329977465, 0.22962190399994142, 0.24251609700149857, 0.236350464001589, 0.2421240800031228, 0.23505404399475083, 0.24384987200028263, 0.23896858600346604, 0.23905106999882264, 0.2673487430001842, 0.2309660199971404, 0.2509579120014678, 0.24805646800086834, 0.24461886499921093, 0.24252035400422756, 0.24519624399545137, 0.24256297200190602, 0.25506631399912294, 0.23668225300207268, 0.24242626899649622, 0.24715043399919523, 0.22080055200058268, 0.20714563700312283, 0.2101645489965449, 0.20195203900220804, 0.20635494100133656, 0.2062181529981899, 0.20556839700293494, 0.1577058559996658, 0.16448023599514272, 0.15747060099965893, 0.15919634100282565, 0.1662720430031186, 0.208596091993968, 0.12153392800246365, 0.18481541700020898, 0.20841731900145533, 0.11966113300150027, 0.17178053799580084, 0.1712327909990563, 0.166409328005102, 0.14080423099949257, 0.1388408509956207, 0.0912933760046144, 0.09138561799773015, 0.09186859399778768, 0.09502543000417063, 0.14016163799533388, 0.13465866600017762, 0.15224337900144747, 0.15815153600124177, 0.14817061299982015, 0.13869581300241407, 0.14610556399566121, 0.1405333930015331, 0.1430008609968354, 0.14142936900316272, 0.14337546600290807, 0.1548887319950154, 0.14211286000499967, 0.14185928699589567, 0.14056014100060565, 0.15404654000303708, 0.14428434900037246, 0.17264129599789158, 0.14984975900006248, 0.14686316099687247, 0.15894740300427657, 0.15285081199544948, 0.14513016500131926, 0.15545395600202028, 0.16059825699630892, 0.1542557510038023, 0.14954221800144296, 0.15140778399654664, 0.1523709230023087, 0.15466017799917608, 0.1495512649999, 0.1769465350007522, 0.14764939800079446, 0.1565887440010556, 0.15609752299496904, 0.19672281300154282, 0.19533376300387317, 0.2093921399937244, 0.19932341500680195, 0.20252609799354104, 0.20935721500427462, 0.2062498070008587, 0.2129990019966499, 0.21370708900212776, 0.2061828299993067, 0.19955605499853846, 0.21905508100462612, 0.20265892799943686, 0.2531913290004013, 0.2461530419968767, 0.25030885000160197, 0.2591861010005232, 0.24433146599767497, 0.25327922600263264, 0.2287544809951214, 0.22922197000298183, 0.24620435299584642, 0.22593070400034776, 0.2218918390062754, 0.22067137199337594, 0.24319681500492152, 0.22763940000004368, 0.23636125199845992, 0.22701415800111135, 0.23604250700009288, 0.22506472800159827, 0.17924713699903805, 0.1926278859973536, 0.23411486200348008, 0.2651981099988916, 0.2358065579974209, 0.1464447359976475, 0.19552367000142112, 0.24579910000466043, 0.23978431800060207, 0.23013746399374213, 0.2301018270009081, 0.2551546690010582, 0.18910983599926112, 0.20666538800287526, 0.19301652799913427, 0.19170586700056447, 0.2286233769991668, 0.19291095600055996, 0.19392533400241518, 0.2026503329980187, 0.1998587909984053, 0.2342268590000458, 0.2522953099978622, 0.2479789040007745, 0.2378910640036338, 0.2082406599947717, 0.19790171700151404, 0.20177145300112898, 0.1966082209983142, 0.1986568510037614, 0.2372871679981472, 0.21008929399977205, 0.2046880920024705, 0.20733250000193948, 0.20953220299998065, 0.20132992399885552, 0.21784045299864374, 0.20892895799624966, 0.20580217200040352, 0.2125196760025574, 0.2172173319995636, 0.2036893179974868, 0.22309883600246394, 0.22943631499947514, 0.20243838299938943, 0.2123034500036738, 0.2102170149955782, 0.21424431700143032, 0.18781937300082063, 0.1824215960004949, 0.17753507899760734, 0.18726031899859663, 0.18751140300446423, 0.18796216599730542, 0.18672769900149433, 0.18365463500231272, 0.20053819799795747, 0.19588420200307155, 0.18667942499450874, 0.18305209000391187, 0.2017936819975148, 0.1834000009985175, 0.18198489900532877, 0.19676454000000376, 0.1991627269962919, 0.18884305500250775, 0.1921437899945886, 0.19627651700284332, 0.1848116410037619, 0.2061122829982196, 0.21771332299977075, 0.19247682899731444, 0.202554406998388, 0.19714979600394145, 0.21107914300228003, 0.20178644899715437, 0.23962267799652182, 0.250317158002872, 0.2404598889988847, 0.23217161399952602, 0.24103415600256994, 0.23651684299693443, 0.23706785100512207, 0.2677072890001, 0.24485344599816017, 0.24825502900057472, 0.24197921699669678, 0.24445439199917018, 0.23674056900199503, 0.2479337890035822, 0.244506115996046, 0.2552305470017018, 0.24823478599864757, 0.2537008310027886, 0.2514370270000654, 0.28427140900021186, 0.2583617439959198, 0.24852393200126244, 0.21561729200038826, 0.20489217300200835, 0.20564596600161167, 0.22254706599778729, 0.20331042500038166, 0.210891636001179, 0.20972593299666187, 0.20807667200278956, 0.2042530339967925, 0.22376215099939145, 0.19691305000014836, 0.18092774599790573, 0.1894302940054331, 0.17866679599683266, 0.17994701000134228, 0.18651662499905797, 0.18295276800199645, 0.1808972399958293, 0.1915725970029598, 0.1858781179980724, 0.1852233840036206, 0.20095539799513062, 0.19129146700288402, 0.1786896199992043, 0.7116553150044638, 0.1853037749970099, 0.19137218999821926, 0.19161357999837492, 0.19181732800643658, 0.1918253419935354, 0.1994055470058811, 0.19443612999748439, 0.19903594499919564, 0.20117089099949226, 0.19078623699897435, 0.18678786600503372, 0.19876089799799956, 0.1929201649982133, 0.19363456300197868, 0.23106843999994453, 0.19516133399883984, 0.19755597799667157, 0.20800041100301314, 0.19561149700166425, 0.19599395399563946, 0.20434402100363513, 0.202627519996895, 0.20515785300085554, 0.2525854679988697, 0.1996346580053796, 0.19644370199966943, 0.2225993269967148, 0.2199247579992516, 0.2088151639982243, 0.20726764500432182, 0.2000554190017283, 0.20280286799970781, 0.2012766339976224, 0.19584984100220026, 0.2015640959943994, 0.2487095260003116, 0.2506050000010873, 0.26770039600523887, 0.24512968799535884, 0.25132811399817, 0.24794660000043223, 0.28747535400179913, 0.2582275890017627, 0.2510093649980263, 0.21712412800116, 0.22512393099896144, 0.18518044899974484, 0.18146257699845592, 0.17708195100567536, 0.19128140199609334, 0.1802811980014667, 0.18567388800147455, 0.18703791499865474, 0.18435879600292537, 0.18167498799448367, 0.2131364820015733, 0.18758401700324612, 0.18719548099761596, 0.245084672998928, 0.23622671399789397, 0.18794110100134276, 0.24779792100162013, 0.22847182599798543, 0.1491241550029372, 0.15575863899721298, 0.19330913099838654, 0.18532008900365327, 0.19347192699933657, 0.22327208000206156, 0.19294623199675698, 0.19572449200495612, 0.23731986199709354, 0.2480888279969804, 0.23089563500252552, 0.24207494599977508, 0.2405792819990893, 0.24039864400401711, 0.24964387499494478, 0.23968221500399522, 0.24539290499524213, 0.1941591110007721, 0.2694555850030156, 0.24907259800238535, 0.19635162699705688, 0.24528923899924848, 0.2488605649996316, 0.2050545790043543, 0.16106490999663947, 0.15623195700027281, 0.1997414299985394, 0.16705681900202762, 0.16589929399924586, 0.15677390500059118, 0.161608296999475, 0.15737764700315893, 0.16865039699769113, 0.16588885899545858, 0.18527933400037, 0.16242456900363322, 0.17290586700255517, 0.1636270599992713, 0.16509769600088475, 0.16650844699324807, 0.16383085300185485, 0.1787673930011806, 0.13990826399822254, 0.13958265900146216, 0.13748896199831506, 0.1436135740004829, 0.17847143600374693, 0.17967708699870855, 0.183649519996834, 0.20799276400066447, 0.18270220400154358, 0.19150813599844696, 0.18419980400358327, 0.22705945299821906, 0.24003584100137232, 0.22713008199934848, 0.18804673699924024, 0.1862659030011855, 0.18259938399569364, 0.19887075400038157, 0.20203954799944768, 0.18832352600293234, 0.21121396299713524, 0.19000425100239227, 0.18459904900373658, 0.19280110099498415, 0.18814027800544864, 0.18749747199763078, 0.20156263499666238, 0.19072872700053267, 0.19252470300125424, 0.19897911600128282, 0.1943305530003272, 0.15721180899708997, 0.1908869760009111, 0.1600112000014633, 0.15587970599881373, 0.15472021300229244, 0.1816565740009537, 0.16157448200101499, 0.1551920700003393, 0.16664876099821413, 0.15548539799783612, 0.14418872500391444, 0.16434792899963213, 0.15399100299691781, 0.16600011300033657, 0.16178752500127302, 0.1610780889968737, 0.15592786900378997, 0.1521920989980572, 0.1631581140027265, 0.15797293099603849, 0.17091086000436917, 0.1838814199945773, 0.15940790100285085, 0.1638869269954739, 0.16773109700443456, 0.1627130019987817, 0.16883682899788255, 0.20643552700494183, 0.20630431699828478, 0.13324512299732305, 0.132819530001143, 0.13687352499982808, 0.1857098699983908, 0.14180840300105046, 0.13579577099881135, 0.13204334799956996, 0.1822121870063711, 0.20213226600026246, 0.13786629599781008, 0.1380974019994028, 0.14300854099565186, 0.13464625300548505, 0.13707606599928113, 0.13426298800186487, 0.14486536699405406, 0.14192977600032464, 0.1430519630011986, 0.1407388980005635, 0.14190984699962428, 0.15378477099875454, 0.14905052900576266, 0.15072113199857995, 0.14507951799896546, 0.17659190599806607, 0.15364077599951997, 0.14990406799915945, 0.15371682900149608, 0.14960318300290965, 0.16190904499671888, 0.15246219500113511, 0.1581188310010475, 0.1593789089965867, 0.1635278340036166, 0.14934250000078464, 0.15427377799642272, 0.15652822999982163, 0.15216776900342666, 0.2272754099976737, 0.20866574499814305, 0.19941311800357653, 0.20681279699783772, 0.20251654300227528, 0.24908060600137105, 0.24147335299494443, 0.23763237900129752, 0.25438618800399126, 0.24331018399971072, 0.24886262899963185, 0.2503235019976273, 0.2552516560026561, 0.24772617999406066, 0.22758872900158167, 0.26552484500280116, 0.2103209679989959, 0.26380962599796476, 0.2142075500014471, 0.2154005460033659, 0.22291418099484872, 0.2172286070053815, 0.17956858999968972, 0.193194464998669, 0.23105660900182556, 0.23488469799485756, 0.18795073000364937, 0.24782255000172881, 0.23401748499600217, 0.23245450400281698, 0.2435218509999686, 0.23636807200091425, 0.18540088599547744, 0.23817348499869695, 0.26958218900108477, 0.23995547300000908, 0.23378803000377957, 0.24275585099530872, 0.1976081860047998, 0.2311380690007354, 0.15713901399431052, 0.15342935000080615, 0.10852395000256365, 0.11067393700068351, 0.1074194679968059, 0.10996002799947746, 0.10627696600568015, 0.11915729399333941, 0.10921224800404161, 0.11109642399969744, 0.11407529700227315, 0.11744437200104585, 0.1403926139973919, 0.11772500799997943, 0.11256228300044313, 0.11120706299698213, 0.1269923630025005, 0.11794810999708716, 0.11685152900463436, 0.11845619599625934, 0.11422630800370825, 0.16615725299925543, 0.16187939099472715, 0.18440860700502526, 0.1628145799986669, 0.16694257999915862, 0.19138896799995564, 0.17433364799944684, 0.16814510800031712, 0.16844171800039476, 0.17454446299962, 0.16640442900097696, 0.18120666899631033, 0.17153664800571278, 0.16953101599938236, 0.17452404699724866, 0.18397714300226653, 0.1703507880010875, 0.17303154999535764, 0.19475864300329704, 0.1762153660019976, 0.1864005739989807, 0.13795864900021115, 0.14131844700023066, 0.13731623399507953, 0.1499300499999663, 0.1430093110029702, 0.14060012299887603, 0.14414996199775487, 0.13964113300608005, 0.15010128999711014, 0.10140240399778122, 0.1030201070025214, 0.17440025199903175, 0.1452072309984942, 0.1550848129991209, 0.1483949610046693, 0.10382165799819632, 0.14939673300250433, 0.14957948200026294, 0.1571211529953871, 0.06274729600409046, 0.10914861999481218, 0.0662678990047425, 0.10766653799510095, 0.11147117600194179, 0.06695504100207472, 0.09605106699746102, 0.11255168999923626, 0.15928015600366052, 0.1613629330022377, 0.17137287099467358, 0.1609503100044094, 0.15613908099476248, 0.164328162005404, 0.1753040260009584, 0.20957782799814595, 0.2523341359992628, 0.2611725579990889, 0.28315103700151667, 0.21705369799747132, 0.30092853200039826, 0.2639672959994641, 0.2981423450037255, 0.3037818429947947, 0.29632520399900386, 0.31231356900389073, 0.3425799999968149, 0.3593330510047963, 0.27150700299534947, 0.30561129100533435, 0.34540484399622073, 0.31025915699865436, 0.30690646300354274, 0.30097423899860587, 0.31481288999930257, 0.3088833349975175, 0.3211042170005385, 0.3245157900018967, 0.306636665998667, 0.36425213199981954, 0.3540571949997684, 0.27420823000284145, 0.34196642699680524, 0.27623723900615005, 0.3271654039999703, 0.2681937960005598, 0.2815934779937379, 0.27805223500035936, 0.2828974220028613, 0.2777214419984375, 0.2776574450035696, 0.2758907659954275, 0.28091165800287854, 0.26716864199988777, 0.2378071569983149, 0.23925639200024307, 0.23222250200342387, 0.23889762399630854, 0.2550635880033951, 0.23168822799925692, 0.24813327899755677, 0.23625207300210604, 0.2517218220018549, 0.2369691679996322, 0.2459334779996425, 0.27633430199784925, 0.25567659799708053, 0.24223591700138059, 0.23663242700422416, 0.24863711199577665, 0.24413354900025297, 0.2640422900003614, 0.24881031100085238, 0.24661423100042157, 0.24589288699644385, 0.253034387002117, 0.28840911899897037, 0.24842120700486703, 0.2796957449972979, 0.24375323000276694, 0.25077646899444517, 0.2447015569996438, 0.2622142540058121, 0.24828841499402188, 0.2505477200029418, 0.2505099379995954, 0.25579962600022554, 0.2496849739982281, 0.24988131900317967, 0.2683423349953955, 0.28608780500508146, 0.25829502399574267, 0.2521037940023234, 0.259273909003241, 0.3069395309939864, 0.304000869000447, 0.3109480880011688, 0.3132370820021606, 0.31043742899782956, 0.3032255790021736, 0.31958126399695175, 0.28609594300360186, 0.27328347499860683, 0.27033487200242234, 0.27868056899751537, 0.2804918469992117, 0.2744636610004818, 0.27247366400115425, 0.28514963499765145, 0.28537285700440407, 0.27102227800060064, 0.2829952029933338, 0.2812666950048879, 0.2802138949991786, 0.3083506569964811, 0.28960267700313125, 0.2773756870010402, 0.2891929569959757, 0.2863654230022803, 0.28214491400285624, 0.29510537999885855, 0.29221706799580716, 0.30123289100447437, 0.2831126789969858, 0.29083400100353174, 0.28133990700007416, 0.29490932999760844, 0.316975594003452, 0.287975519997417, 0.2853819169977214, 0.2949211520026438, 0.2897563849983271, 0.28940609699930064, 0.2895607000027667, 0.30175615399639355, 0.2929906089993892, 0.24515491500642383, 0.7599147719956818, 0.2563435530028073, 0.2415284359958605, 0.31169702500483254, 0.2498116559945629, 0.25471011100307805, 0.2506536099972436, 0.2462786070027505, 0.2562964960015961, 0.24870355099847075, 0.25042632300028345, 0.26251932000013767, 0.2526054199988721, 0.2591770560029545, 0.25298636999650626, 0.2621519480017014, 0.21557111100264592, 0.23120723299507517, 0.2138636800009408, 0.25047746599739185, 0.2593104030020186, 0.2606176780027454, 0.2558095089989365, 0.2226843620010186, 0.17380511599913007, 0.1866633629979333, 0.22756868300348287, 0.22206474199629156, 0.23426702800497878, 0.18408365499635693, 0.1826927509973757, 0.19782921000296483, 0.21437199100182625, 0.185802366999269, 0.23916223800188163, 0.22995970599731663, 0.23822338200261584, 0.2412210159964161, 0.24500787100259913, 0.23191357999894535, 0.2424350500004948, 0.23411397799645783, 0.23177617300098063, 0.24224717199831503, 0.2360756480047712, 0.2369651150002028, 0.2679956479987595, 0.25279037300060736, 0.24386739100009436, 0.24280155800079228, 0.24008611599856522, 0.23402112600160763, 0.26497001700045075, 0.2820899639991694, 0.2945383810001658, 0.2868823649987462, 0.28592228799971053, 0.2904450080022798, 0.27977593099785736, 0.24839460200018948, 0.19668385200202465, 0.24284058699413436, 0.25703175400121836, 0.1928790530000697, 0.25395310800377047, 0.2046310609948705, 0.19880851700145286, 0.215767579000385, 0.20806238300428959, 0.20481263299734564, 0.20979780599736841, 0.23456881500169402, 0.24672017199918628, 0.21004754600289743, 0.25330588199722115, 0.2050315739979851, 0.2530428220052272, 0.2564181919951807, 0.25631553499988513, 0.25725542200234486, 0.2542148089996772, 0.25973684299970046, 0.2591153920002398, 0.30011679499875754, 0.29670245599845657, 0.23263749400211964, 0.22113247599918395, 0.2732566039994708, 0.2741625330017996, 0.26138023500243435, 0.27055735400062986, 0.2649474149948219, 0.27407072899950435, 0.2692962950022775, 0.2789113070029998, 0.27797463599563343, 0.2296730820016819, 0.22943722199852346, 0.25871578700025566, 0.23039914200489875, 0.23749015799694462, 0.23107036700093886, 0.23473958599788602, 0.2340117569983704, 0.237502198004222, 0.24170688199956203, 0.23466258499684045, 0.2465491920011118, 0.24079355799767654, 0.2437222130029113, 0.23960188800265314, 0.2739569909972488, 0.23765949300286593, 0.24223142599657876, 0.2454594349983381, 0.2382891630040831, 0.24672220600041328, 0.2463038459973177, 0.24202272199909203, 0.24073279600270325, 0.2520395519968588, 0.23605291500280146, 0.25933941500261426, 0.24654452899994794, 0.2395008749954286, 0.27290142999845557, 0.25101025600451976, 0.24484295599540928, 0.25009128100646194, 0.2551140759969712, 0.24928666999767302, 0.24355638300039573, 0.24621240999840666, 0.2499073010039865, 0.2698599959985586, 0.2527838820024044, 0.25926190499740187, 0.2554763610023656, 0.2847836479995749, 0.25569611199898645, 0.2599149919988122, 0.2558677609995357, 0.24569941200024914, 0.2608078100020066, 0.22671660399646498, 0.23801173900574213, 0.2326155189948622, 0.231122281998978, 0.22325364100106526, 0.23327406900352798, 0.2298535730005824, 0.2321397519990569, 0.26184962799743516, 0.2236187400048948, 0.24320006699417718, 0.2293021330042393, 0.23943209299613954, 0.23595120100071654, 0.24081272900366457, 0.23134337800001958, 0.22372817900031805, 0.2377444169978844, 0.24065812800108688, 0.2420263019957929, 0.2356478750007227, 0.24278721200244036, 0.25842225099768257, 0.24476406300527742, 0.23652953699638601, 0.2307370010021259, 0.26362434399925405, 0.243528637001873, 0.24152565399708692, 0.2412626299992553, 0.24171240300347563, 0.23658870999497594, 0.24857788300141692, 0.25004612100019585, 0.2395023979988764, 0.2639609260004363, 0.24897233200317714, 0.253202848994988, 0.24694373400416225, 0.24108816400257638, 0.2392058539990103, 0.24762367899529636, 0.239415406002081, 0.24380989999917801, 0.25416388000303414]\n",
            "[0.027073753997683525, 0.024799216000246815, 0.02399388399499003, 0.02532593899377389, 0.023921232001157477, 0.023170123000454623, 0.02344266699947184, 0.023597537001478486, 0.023058384002069943, 0.024854031995346304, 0.02369506399554666, 0.02388887500273995, 0.023494927998399362, 0.027367167996999342, 0.023927695998281706, 0.02422808399569476, 0.024173251003958285, 0.023746371000015642, 0.02430796599946916, 0.024387992998526897, 0.02461481199861737, 0.02451602299697697, 0.023854970997490454, 0.023247619996254798, 0.02449954300391255, 0.024129131001245696, 0.024174093996407464, 0.023996176998480223, 0.02417524300108198, 0.026600108001730405, 0.02402955999423284, 0.02324618100101361, 0.024653762004163582, 0.025228509002772626, 0.025271553997299634, 0.023665251006605104, 0.02868590599973686, 0.023390489994199015, 0.024372063999180682, 0.028281744998821523, 0.024090831000648905, 0.024709033998078667, 0.024428085002000444, 0.02663328000198817, 0.023885326998424716, 0.03221734100225149, 0.023518009002145845, 0.02479282500280533, 0.02577037700393703, 0.023430779001500923, 0.024781833999441005, 0.02483421099896077, 0.023903228000563104, 0.023720559001958463, 0.023794209999323357, 0.04531823899742449, 0.024551379996410105, 0.026702009003201965, 0.028238045000762213, 0.024137218002579175, 0.026217054000881035, 0.024641218995384406, 0.024362422998819966, 0.023805054996046238, 0.022809415000665467, 0.023240311995323282, 0.025181348995829467, 0.024695641004655045, 0.024420833004114684, 0.042397145996801555, 0.023290297998755705, 0.023716504998446908, 0.02420602599886479, 0.024975685999379493, 0.023852869002439547, 0.023568800002976786, 0.024340294003195595, 0.02353981100168312, 0.024422804002824705, 0.023717008996754885, 0.02260454599309014, 0.02325336999638239, 0.023484606994315982, 0.02472951399977319, 0.04259880700556096, 0.023835738997149747, 0.02531114100565901, 0.023222982003062498, 0.02369626099971356, 0.02679185000306461, 0.024718192995351274, 0.02507437000167556, 0.0243349699958344, 0.023793783999281004, 0.023168371997599024, 0.024728003001655452, 0.02595870100049069, 0.02576412700000219, 0.044650076000834815, 0.02344075400469592, 0.02346837600634899, 0.029870157995901536, 0.024514632001228165, 0.024072272004559636, 0.02434548699966399, 0.024262613005703315, 0.024556298994866665, 0.024804610999126453, 0.024733509999350645, 0.024129420999088325, 0.024083645999780856, 0.02470074400480371, 0.024772311000560876, 0.02351751999958651, 0.024294412003655452, 0.024404426003457047, 0.02464423699711915, 0.026820235994819086, 0.024317982999491505, 0.0239687130015227, 0.024368569000216667, 0.027361194996046834, 0.02613914100220427, 0.028940958000021055, 0.024392755003646016, 0.026820680999662727, 0.025603682996006683, 0.027828303995192982, 0.023655106000660453, 0.02435185200010892, 0.02390884199849097, 0.028260095001314767, 0.024157722997188102, 0.024385391996474937, 0.02364168799977051, 0.023578878994158003, 0.024093555002764333, 0.024724240000068676, 0.023866707997513004, 0.023615756996150594, 0.025671547002275474, 0.024750042000960093, 0.024288382002850994, 0.02811336499871686, 0.024070081999525428, 0.02437944900157163, 0.024169803997210693, 0.023955974997079466, 0.023815981003281195, 0.024825223001244012, 0.024518211001122836, 0.02454164899972966, 0.024597165996965487, 0.02513465500669554, 0.02919603000191273, 0.024485235000611283, 0.024577664000389632, 0.024601511999208014, 0.027599518005445134, 0.025281102003646083, 0.023848149001423735, 0.02363296100520529, 0.02421437700104434, 0.025151433001155965, 0.024734482998610474, 0.02363129599689273, 0.02863370799605036, 0.02515406400198117, 0.02614837400324177, 0.025119319005170837, 0.023215266002807766, 0.02388539599633077, 0.03330911100056255, 0.023684101994149387, 0.023367455003608484, 0.02434019099746365, 0.025876204002997838, 0.02383045399619732, 0.033491388996480964, 0.02436872300313553, 0.024144766997778788, 0.02546265199634945, 0.025091409996093716, 0.024823762003507, 0.024413803999777883, 0.024620629999844823, 0.023768095998093486, 0.024642655000207014, 0.028065926002454944, 0.02419331100099953, 0.02459240700409282, 0.023784697994415183, 0.023955214004672598, 0.028576338998391293, 0.024259265999717172, 0.02411273699544836, 0.02458851799747208, 0.024154799997631926, 0.028088430997740943, 0.028089667001040652, 0.030747169003007002, 0.02398543899471406, 0.025130362999334466, 0.044087524998758454, 0.02364325300004566, 0.02445478900335729, 0.02363496500038309, 0.0254557099979138, 0.02513053300208412, 0.023830995000025723, 0.027626483999483753, 0.025616283994168043, 0.024439086999336723, 0.024970291000499856, 0.024472215001878794, 0.0236167960028979, 0.03217054899869254, 0.026478776999283582, 0.031706234003650025, 0.025180329001159407, 0.02517386899853591, 0.024506226000085007, 0.024347584003407974, 0.02419454800110543, 0.032051619004050735, 0.024551394002628513, 0.02379191700310912, 0.02454174000013154, 0.024490193005476613, 0.023410720998072065, 0.024134803999913856, 0.024390153004787862, 0.026431361002323683, 0.023956878001627047, 0.023723598002106883, 0.024333590001333505, 0.024801778999972157, 0.02508355399913853, 0.02792918900377117, 0.023809936006728094, 0.02416960800474044, 0.02446496399352327, 0.025010201999975834, 0.02416792400617851, 0.02411303000553744, 0.02347001599991927, 0.02483842000219738, 0.02399352799693588, 0.02394628200272564, 0.024640684001496993, 0.023592352998093702, 0.024229652997746598, 0.024107052995532285, 0.02423280299990438, 0.03079530499962857, 0.04325457399681909, 0.02344168799754698, 0.02368931499950122, 0.023537547000159975, 0.023492869004257955, 0.022529913003381807, 0.024675973996636458, 0.023278847002075054, 0.02430086999811465, 0.024053845001617447, 0.02443226199829951, 0.026060337004309986, 0.026489819996641017, 0.023043029999826103, 0.023863338996306993, 0.025237098001525737, 0.02436903699708637, 0.027558777001104318, 0.024167056995793246, 0.02464850300020771, 0.02436048399977153, 0.02578174800146371, 0.024316850998729933, 0.026892643996689003, 0.02448213200113969, 0.023301000997889787, 0.024083591997623444, 0.023634123004740104, 0.02353581300121732, 0.025520164002955426, 0.024423893002676778, 0.023692504000791814, 0.023803017000318505, 0.02555200100323418, 0.024430942001345102, 0.02469047599879559, 0.023685363004915416, 0.023555580999527592, 0.02391570199688431, 0.024457557003188413, 0.02293620799900964, 0.024756698003329802, 0.04717032000189647, 0.025299585999164265, 0.023896958999102935, 0.02361381900118431, 0.02304248500149697, 0.02273193100700155, 0.02308599899697583, 0.023191064996353816, 0.024282073994982056, 0.024126958996930625, 0.02831776699895272, 0.023580614004458766, 0.02358022500266088, 0.028836457997385878, 0.0245938189982553, 0.023453018999134656, 0.025162640005873982, 0.025415338001039345, 0.023823674004233908, 0.02412204199936241, 0.023705007995886263, 0.025378936996276025, 0.02401372000167612, 0.024389252001128625, 0.026664415003324393, 0.024216297002567444, 0.024747684998146724, 0.024019692005822435, 0.025995900999987498, 0.02430671100592008, 0.024927684004069306, 0.025508419996185694, 0.02592492399708135, 0.024527123001462314, 0.024432951999187935, 0.024252306000562385, 0.024650036000821274, 0.024237061996245757, 0.025203282995789777, 0.024185514004784636, 0.025637621998612303, 0.023774022993166, 0.0243061770015629, 0.024516969999240246, 0.02427817400166532, 0.023918346996651962, 0.02431977899686899, 0.022968145996856038, 0.024150443001417443, 0.024989527999423444, 0.024088252997898962, 0.02373451300081797, 0.024512331001460552, 0.02430289500625804, 0.047499227002845146, 0.026101322000613436, 0.023623981003765948, 0.024240243998065125, 0.024448482996376697, 0.024520427999959793, 0.024300423996464815, 0.024224494001828134, 0.025689697002235334, 0.02484611899853917, 0.02756996599782724, 0.02438345699920319, 0.024344978999579325, 0.02466523600014625, 0.02392825100105256, 0.025980559999879915, 0.02447323400701862, 0.02422993200161727, 0.02439434100233484, 0.02453515499655623, 0.02376445199479349, 0.024095704000501428, 0.02505567600019276, 0.023740217999147717, 0.02434612599608954, 0.024346630001673475, 0.024988420998852234, 0.023999386001378298, 0.023773864995746408, 0.02466192899737507, 0.023515363005572, 0.024115627995342948, 0.02499420200183522, 0.030045865001739003, 0.025121165002929047, 0.02489802599302493, 0.02423335699859308, 0.024387131998082623, 0.024370718994759955, 0.025451305002206936, 0.023525304000941105, 0.023340803003520705, 0.02371452200168278, 0.02374959999724524, 0.02502716200251598, 0.025490118001471274, 0.02576056699763285, 0.024193436998757534, 0.025119744997937232, 0.024936840003647376, 0.02474335499573499, 0.02402657399943564, 0.02428124200378079, 0.025274671999795828, 0.025293924001744017, 0.024948415004473645, 0.025122652994468808, 0.024112692000926472, 0.025574327999493107, 0.024297590003698133, 0.025794036999286618, 0.024435742001514882, 0.025273667997680604, 0.025888885000313167, 0.02441602999897441, 0.023592088000441436, 0.024620841999421827, 0.02395062399591552, 0.02456797700142488, 0.024720480003452394, 0.023933841999678407, 0.023978734003321733, 0.02455630199983716, 0.025882383997668512, 0.024054403002082836, 0.025328281000838615, 0.028031902002112474, 0.024920210002164822, 0.025930843003152404, 0.026002755999797955, 0.02530791900062468, 0.024830900998495053, 0.025004720002471004, 0.025342634005937725, 0.02341896900179563, 0.023435158997017425, 0.024625318001199048, 0.02500273699843092, 0.02465542600111803, 0.02410651700483868, 0.02501768599904608, 0.025456080998992547, 0.04885776800074382, 0.02512862500589108, 0.024229482994996943, 0.024887380001018755, 0.02275101099803578, 0.02463908000208903, 0.02360152300389018, 0.024886614999559242, 0.024730144999921322, 0.023866439994890243, 0.024904489997425117, 0.025259326997911558, 0.027182423997146543, 0.024379870999837294, 0.024984073999803513, 0.024615359005110804, 0.04181853800400859, 0.025408194996998645, 0.0248470080041443, 0.025977937002608087, 0.0257046149999951, 0.025819916001637466, 0.024993029997858685, 0.026988186000380665, 0.02509234700119123, 0.025578350003343076, 0.024238113001047168, 0.024541417995351367, 0.024763524997979403, 0.024265668995212764, 0.043584058999840636, 0.02644436500122538, 0.025104025000473484, 0.025551923994498793, 0.02606442700198386, 0.025814303000515793, 0.024667671001225244, 0.02374819199758349, 0.030339039003592916, 0.025889648000884335, 0.024584331004007254, 0.02478968100331258, 0.024749714000790846, 0.025319000997114927, 0.044570989994099364, 0.025217895999958273, 0.025229304999811575, 0.02542100899881916, 0.025631415002862923, 0.025442694000958, 0.025842232003924437, 0.02572907599824248, 0.025011013000039384, 0.025966829001845326, 0.026974209999025334, 0.025016334002430085, 0.02385328499804018, 0.02491640700463904, 0.024218555001425557, 0.026407595993077848, 0.025569988996721804, 0.0256259170055273, 0.0244910780020291, 0.026677740999730304, 0.025638314000389073, 0.02707308399840258, 0.0255301569995936, 0.024408123004832305, 0.026436724001541734, 0.02522636500361841, 0.025627039998653345, 0.027538353999261744, 0.025181110002449714, 0.025339341002108995, 0.02400579499953892, 0.0266712109951186, 0.023542157003248576, 0.023734362002869602, 0.02603740199992899, 0.024992758000735193, 0.02470055800222326, 0.025156358002277557, 0.027305939998768736, 0.02517049099697033, 0.02572558500105515, 0.02405917100259103, 0.024448424999718554, 0.025286269999924116, 0.02440454599855002, 0.025066713002161123, 0.023276838997844607, 0.024847049004165456, 0.025528133002808318, 0.027222548000281677, 0.026104265001777094, 0.025566568998328876, 0.027423644001828507, 0.026103280993993394, 0.02553463199728867, 0.026620912001817487, 0.02487828300218098, 0.023465896003472153, 0.024637173002702184, 0.024278160999529064, 0.025619182997616008, 0.027729648994863965, 0.028169735996925738, 0.025634459998400416, 0.02434379699843703, 0.043021278994274326, 0.025522990996250883, 0.024584395003330428, 0.025660832994617522, 0.02449728099600179, 0.02530131099774735, 0.025525015000312123, 0.025856584004941396, 0.0243383020060719, 0.025887329000397585, 0.02639844300574623, 0.024323865996848326, 0.024869439002941363, 0.025277852997533046, 0.03208698899834417, 0.025278574001276866, 0.02529731699905824, 0.026688499994634185, 0.02376295899739489, 0.024713082995731384, 0.02479966700047953, 0.024399106994678732, 0.024216359001002274, 0.024547854998672847, 0.025451445995713584, 0.02408210300200153, 0.02433751500211656, 0.02399703900300665, 0.02867695500026457, 0.024076200999843422, 0.02459970099880593, 0.028796573998988606, 0.02648751300148433, 0.02531739600090077, 0.024057607995928265, 0.023931855997943785, 0.02521508799691219, 0.02451442900201073, 0.02467692399659427, 0.02396998200129019, 0.05202801900304621, 0.023941048995766323, 0.02582371499738656, 0.02520191999792587, 0.024795641998935025, 0.02449247099866625, 0.026235196004563477, 0.02573040800052695, 0.02458959999785293, 0.02443360799952643, 0.025204149998899084, 0.02528549000271596, 0.02917876200081082, 0.024695025000255555, 0.023973873001523316, 0.025830297003267333, 0.025971613002184313, 0.02522595299524255, 0.02561209799750941, 0.025081807005335577, 0.023583321999467444, 0.02567333399929339, 0.024991036996652838, 0.025385622997418977, 0.025137194999842905, 0.02555123600177467, 0.028374452995194588, 0.024313330999575555, 0.02478046699980041, 0.025924786998075433, 0.025530022001476027, 0.024325954997038934, 0.025206309001077898, 0.023894063000625465, 0.02516781000304036, 0.028885647996503394, 0.025293359001807403, 0.024541850994864944, 0.024989593999634963, 0.024516364006558433, 0.025089561997447163, 0.025279369998315815, 0.027983191997918766, 0.02518177100137109, 0.025952805997803807, 0.024612232999061234, 0.024801548002869822, 0.057260610003140755, 0.024679980000655632, 0.025984897001762874, 0.02637371300079394, 0.024194730998715386, 0.024033742003666703, 0.025969408998207655, 0.024878286007151473, 0.025316670995380264, 0.024616255002911203, 0.02473800800362369, 0.024734758997510653, 0.02446149600291392, 0.02592996299790684, 0.02464068800327368, 0.02439505200163694, 0.024061347998213023, 0.025041384003998246, 0.024149855998985004, 0.025047891998838168, 0.023291843994229566, 0.024022357996727806, 0.023857489999500103, 0.024310908003826626, 0.023790309998730663, 0.02624124300200492, 0.024884824000764638, 0.02489273600076558, 0.02449839699693257, 0.0257382760028122, 0.025529293998260982, 0.02470920800260501, 0.02462951800407609, 0.026150470002903603, 0.025983179999457207, 0.02621948999876622, 0.026669048995245248, 0.024434040002233814, 0.025406535001820885, 0.029024103001574986, 0.025257371002226137, 0.026082517004397232, 0.026515202000155114, 0.024774823999905493, 0.02564323200203944, 0.025183677003951743, 0.025545541000610683, 0.02503241899830755, 0.025937523001630325, 0.02572368500113953, 0.025373584998305887, 0.025872247999359388, 0.025195882997650187, 0.026067320999572985, 0.025881254994601477, 0.025685747001261916, 0.025624799003708176, 0.02648466800019378, 0.025842205999651924, 0.024979208996228408, 0.024509464994480368, 0.024852360002114438, 0.024070334999123588, 0.02438562300085323, 0.02469387699966319, 0.026515571000345517, 0.024589796994405333, 0.025438589000259526, 0.024106723998556845, 0.026092289001098834, 0.023524550997535698, 0.02602411899715662, 0.02442572599829873, 0.024035408998315688, 0.025066324000363238, 0.04823666599986609, 0.02385111899639014, 0.0317944290000014, 0.024383258998568635, 0.02342088599834824, 0.024144538001564797, 0.02448110099794576, 0.025250910002796445, 0.024859224999090657, 0.02405214300233638, 0.024705846997676417, 0.024135727995599154, 0.02514140999846859, 0.02389956800470827, 0.043980635004118085, 0.023917711005196907, 0.02399042600154644, 0.024595776994829066, 0.023926030000438914, 0.023767229002260137, 0.02389279100316344, 0.023687228000198957, 0.024145176997990347, 0.023718346004898194, 0.02435813599731773, 0.025208671002474148, 0.02333182499569375, 0.024544369996874593, 0.025310359000286553, 0.024225763998401817, 0.0241014589992119, 0.024615656002424657, 0.024391486003878526, 0.025712473994644824, 0.025063071996555664, 0.024264883999421727, 0.02465103199938312, 0.02414248699642485, 0.02430515499872854, 0.02468103299906943, 0.02507521000370616, 0.024338299997907598, 0.025005980998685118, 0.032184780000534374, 0.02495965400157729, 0.024169003998395056, 0.025638334001996554, 0.025019264001457486, 0.024643929995363578, 0.03882043300109217, 0.02373998099938035, 0.023730489003355615, 0.02387350099888863, 0.02552054800617043, 0.025357143000292126, 0.024645088997203857, 0.028438289002224337, 0.02456713699939428, 0.02645931799634127, 0.02441984900360694, 0.02383479200216243, 0.027143839994096197, 0.02418417000444606, 0.02406389899988426, 0.025227291996998247, 0.022782517997256946, 0.023873121004726272, 0.0244398210052168, 0.025600203000067268, 0.024823525003739633, 0.02573487899644533, 0.025039483000000473, 0.02420665699901292, 0.024265905005449895, 0.024295032999361865, 0.024525152002752293, 0.024477808001392987, 0.02356563300418202, 0.02475343300466193, 0.02504960599617334, 0.026775815997098107, 0.026615818998834584, 0.025132275994110387, 0.024541023005440366, 0.024446036004519556, 0.024841758000547998, 0.024676142005773727, 0.02363361899915617, 0.023638307997316588, 0.024057449998508673, 0.025928381997800898, 0.02485439900192432, 0.02648385700013023, 0.02543788000184577, 0.025474023997958284, 0.0279797690018313, 0.025405874999705702, 0.024549153000407387, 0.030859838996548206, 0.023732663998089265, 0.025187512001139112, 0.025186437997035682, 0.031601457005308475, 0.024578271993959788, 0.025137830001767725, 0.025542888994095847, 0.025591732999600936, 0.026150240999413654, 0.030969247003667988, 0.02548099800333148, 0.02497674400365213, 0.030092741995758843, 0.02579991200036602, 0.024851542002579663, 0.024273981995065697, 0.024392218998400494, 0.02444463799474761, 0.025312863996077795, 0.02463123000052292, 0.025847832999716047, 0.025665888999355957, 0.02472221400239505, 0.047825480993196834, 0.025925974994606804, 0.024917820002883673, 0.027971650000836235, 0.02502600799925858, 0.0279393370001344, 0.024806666995573323, 0.024589467997429892, 0.028834276003181003, 0.02676528300071368, 0.025858657994831447, 0.025142462000076193, 0.02415736799594015, 0.024936636997153983, 0.025110686001426075, 0.025773909997951705, 0.02522663999843644, 0.025212982996890787, 0.02590839599724859, 0.024917510003433563, 0.026319526994484477, 0.02493034199869726, 0.02472670000133803, 0.024706498996238224, 0.02573072800441878, 0.02516132999880938, 0.02488103100040462, 0.02687993199651828, 0.024835963995428756, 0.031134064993239008, 0.02545048200408928, 0.025747653999133036, 0.025959604005038273, 0.02599571099563036, 0.025658005994046107, 0.024377408997679595, 0.026361584998085164, 0.024960670998552814, 0.024970267004391644, 0.02464180300012231, 0.024584656006481964, 0.024141694004356395, 0.02678017799917143, 0.025666866000392474, 0.02512093699624529, 0.03642739900533343, 0.02576179899915587, 0.024638169998070225, 0.025545073993271217, 0.02507062900258461, 0.02544090899755247, 0.024639905997901224, 0.02637332499580225, 0.02475871000206098, 0.0463410290030879, 0.0296804869940388, 0.023831235004763585, 0.025317061998066492, 0.024380618997383863, 0.02516650699544698, 0.02453440200042678, 0.025160012999549508, 0.02597890199831454, 0.024971846003609244]\n",
            "[0.048295711996615864, 0.043136239000887144, 0.04311901600158308, 0.049654916998406406, 0.04462790700199548, 0.04320208099670708, 0.045398349000606686, 0.04463778300123522, 0.044187585001054686, 0.0498370880013681, 0.043669803999364376, 0.04164868599764304, 0.05620911400183104, 0.04211800100165419, 0.043218955004704185, 0.04500854499929119, 0.0431428540032357, 0.046173105998605024, 0.04483201800030656, 0.04385886799718719, 0.04269181499694241, 0.052267273000325076, 0.04090379800618393, 0.04125997400115011, 0.045439928995619994, 0.042629064999346156, 0.04311642100219615, 0.04447388900007354, 0.04060203299741261, 0.040249953999591526, 0.053304701003071386, 0.042884749003860634, 0.04475242699845694, 0.04606194300140487, 0.043790917996375356, 0.042935400997521356, 0.04335058799915714, 0.04193258700252045, 0.04622061999543803, 0.05786247400101274, 0.041847535998385865, 0.04351167599816108, 0.048401372994703706, 0.0460630300003686, 0.04535826099890983, 0.04211210100038443, 0.04494907700427575, 0.0407327049979358, 0.04901419899397297, 0.04356548500072677, 0.04537263000383973, 0.04282816899649333, 0.043178008003451396, 0.04611937000299804, 0.04293425400101114, 0.04405817899532849, 0.04271877800056245, 0.05139512599998852, 0.04163417599920649, 0.04355898000358138, 0.04220797300513368, 0.04456285500054946, 0.04318876400066074, 0.042901120003079996, 0.041453712998190895, 0.042791947002115194, 0.047938529001839925, 0.04182800499984296, 0.0437945759986178, 0.04639313100051368, 0.04430023200256983, 0.044449594999605324, 0.04327512400050182, 0.043981380003970116, 0.04231790300400462, 0.049917185002414044, 0.04179171400028281, 0.04621866999514168, 0.045466992996807676, 0.0414093959989259, 0.04774164800619474, 0.04376075099571608, 0.04250270400370937, 0.041569546003302094, 0.049400067000533454, 0.04347393600619398, 0.043267797998851165, 0.04413011200085748, 0.04339082300430164, 0.045571470000140835, 0.042054460995132104, 0.04116638900450198, 0.041156881998176686, 0.05087102800462162, 0.04050286400160985, 0.040764780998870265, 0.04314019999583252, 0.047055928996996954, 0.04471463300433243, 0.04234744999848772, 0.041750214993953705, 0.04384078499424504, 0.04769007700087968, 0.04084122899803333, 0.04192033300205367, 0.04505375000007916, 0.04215122699679341, 0.04144821099907858, 0.042809542996110395, 0.04460351099987747, 0.041741924003872555, 0.05823952899663709, 0.04246475699619623, 0.04669943600310944, 0.04268291100015631, 0.048421719999169, 0.04356456999812508, 0.04571679099899484, 0.04164812700037146, 0.0425482459977502, 0.05090357099834364, 0.04529092599841533, 0.04231351500493474, 0.04649163699650671, 0.04297138000401901, 0.04535758100246312, 0.042916223996144254, 0.043133594997925684, 0.04132139000284951, 0.05533463900064817, 0.0423930079996353, 0.04245368899864843, 0.04210465800133534, 0.04716620199906174, 0.044725590996677056, 0.04225849399517756, 0.041996280997409485, 0.04398288100492209, 0.05121441499795765, 0.043770812000730075, 0.04658864899829496, 0.04696444799628807, 0.042609892996551935, 0.042923467997752596, 0.04211405399837531, 0.04508635200181743, 0.041771782001887914, 0.05348301200137939, 0.04528366000158712, 0.04978383499837946, 0.043809875001898035, 0.04383368200069526, 0.04208026199921733, 0.04526268500194419, 0.042022961999464314, 0.042779855997650884, 0.05096550899907015, 0.0464928229994257, 0.04297774899896467, 0.043249294998531695, 0.04242321000492666, 0.044744781000190414, 0.04144668200024171, 0.043659640999976546, 0.040873981997719966, 0.0529921440029284, 0.043929079998633824, 0.04218488500191597, 0.04319765800028108, 0.0465327280035126, 0.04605233800248243, 0.04421155500313034, 0.041594527996494435, 0.04627646699373145, 0.05091821799578611, 0.04340385399700608, 0.04166327600250952, 0.04625171700172359, 0.041528092995577026, 0.0433884140002192, 0.04235095400508726, 0.04453107799781719, 0.04318346599757206, 0.053817455998796504, 0.04213109800184611, 0.05023853600141592, 0.04180171599728055, 0.04281742699822644, 0.046234294000896625, 0.046550009996281005, 0.04552159299782943, 0.04298157500306843, 0.051948049003840424, 0.04378325000288896, 0.0454402850009501, 0.04168135500367498, 0.04352553400531178, 0.04444107600284042, 0.04368566199991619, 0.045958533002703916, 0.04269183299766155, 0.05318904500018107, 0.04045112199673895, 0.044428209999750834, 0.04547077899769647, 0.04772434000187786, 0.042364567998447455, 0.044357585997204296, 0.04479259800427826, 0.04083164499752456, 0.05288802200084319, 0.04683133700018516, 0.04365263899671845, 0.042274033003195655, 0.04556409200449707, 0.043402524999692105, 0.04137459200137528, 0.046589448997110594, 0.04255897099938011, 0.051095347000227775, 0.047087132996239234, 0.04405808100273134, 0.04255537100107176, 0.045990728998731356, 0.04257158900145441, 0.04249306499696104, 0.045955622001201846, 0.041211232004570775, 0.05361036099930061, 0.0424877409968758, 0.046721594997507054, 0.04253964899544371, 0.04590178999933414, 0.043776398997579236, 0.04602582799998345, 0.044096762998378836, 0.044867621996672824, 0.050918007000291254, 0.041823563995421864, 0.045114671003830153, 0.046945971000241116, 0.04402732600283343, 0.04619130100036273, 0.0432885879999958, 0.043771416996605694, 0.04492951899737818, 0.055526195996208116, 0.04380954399675829, 0.046483947000524495, 0.04650878699612804, 0.04555189700477058, 0.04182839599525323, 0.05029905300034443, 0.04494845099543454, 0.04352873400057433, 0.048966770998958964, 0.04425341100431979, 0.04457966300105909, 0.045735831998172216, 0.04874710400326876, 0.05107938100263709, 0.04320818599808263, 0.04687400099646766, 0.040139040000212844, 0.060234538999793585, 0.04332026100018993, 0.045969459002662916, 0.043181773005926516, 0.049468983997940086, 0.045336053997743875, 0.045292082002561074, 0.04093222699884791, 0.04330092299642274, 0.052175378004903905, 0.04454844399879221, 0.04487960199912777, 0.044426560001738835, 0.04315795000002254, 0.04576622099557426, 0.042172583998763, 0.046443567000096664, 0.04138309699919773, 0.05458771200210322, 0.0460655630013207, 0.04598824399727164, 0.04312427000695607, 0.04642241900000954, 0.042621797998435795, 0.04414087300392566, 0.04265422399475938, 0.045928505998745095, 0.049656470997433644, 0.04387548100203276, 0.0444771020047483, 0.046714019001228735, 0.042942733998643234, 0.045119499998691026, 0.04320684900449123, 0.04561468199972296, 0.04313665600056993, 0.052549641994119156, 0.04436406299646478, 0.047284141001000535, 0.04544009399978677, 0.045951986001455225, 0.041232822004531045, 0.04786201000388246, 0.0499412970020785, 0.04644720399664948, 0.05363008700078353, 0.04692755999712972, 0.04775675500422949, 0.04546716200275114, 0.04521201099851169, 0.04576092999923276, 0.045487857998523396, 0.0442925160023151, 0.047884096995403524, 0.05297805499867536, 0.042907327006105334, 0.0474885700023151, 0.04227023300336441, 0.04492760499852011, 0.04972825299773831, 0.04282186099590035, 0.04096688300342066, 0.0433712710000691, 0.052808318003371824, 0.048426525994727854, 0.04308297500392655, 0.04578171500179451, 0.04173983700457029, 0.04227874600474024, 0.046500361000653356, 0.041632155000115745, 0.04175248500541784, 0.054790171001513954, 0.042491738000535406, 0.044918233004864305, 0.04516337499808287, 0.04442797000228893, 0.04351311299978988, 0.04731713599903742, 0.04539489899616456, 0.043354369001463056, 0.053502417998970486, 0.04229135499917902, 0.044032131001586094, 0.0516497700009495, 0.044005437004670966, 0.04256138599885162, 0.05029082800319884, 0.04511402900243411, 0.04349486299906857, 0.05634595900482964, 0.04499109499738552, 0.04390899199643172, 0.04706119599723024, 0.04296606199932285, 0.04207857399887871, 0.04482985100185033, 0.04377691700210562, 0.04106735600362299, 0.053889923001406714, 0.04187194300175179, 0.04561162400204921, 0.04662467899470357, 0.041822676997981034, 0.041284321996499784, 0.04525736699724803, 0.04275723699538503, 0.04200472500087926, 0.05104720099916449, 0.04749928700039163, 0.04188406699540792, 0.041087896002864, 0.04240023899910739, 0.04909098400094081, 0.04330374800338177, 0.04410137200466124, 0.040388369001448154, 0.05393132300378056, 0.042664530003094114, 0.04443936800089432, 0.04427243699319661, 0.0445349180008634, 0.04169394599739462, 0.04336571799649391, 0.04390130899992073, 0.04459494999900926, 0.05104563200438861, 0.04030922099627787, 0.04571986100199865, 0.04553180399670964, 0.04438439699879382, 0.050754404001054354, 0.04273909799667308, 0.04403591900336323, 0.04578915300226072, 0.05077575199538842, 0.04204432599362917, 0.04577887000050396, 0.04278486499970313, 0.04688634999911301, 0.043452882004203275, 0.04702757199993357, 0.04173037099826615, 0.04092690599645721, 0.04872416900616372, 0.04421766500308877, 0.045508030998462345, 0.04267021900159307, 0.04220278100547148, 0.044861068003228866, 0.04100643999845488, 0.041655670000182, 0.04117173700069543, 0.05365568900015205, 0.04238546999840764, 0.04800500000419561, 0.04399452600046061, 0.045074435001879465, 0.04529225599981146, 0.04542753299756441, 0.04199832199810771, 0.04472704300133046, 0.05064774200582178, 0.044108285997936036, 0.04194299800292356, 0.04686784200021066, 0.045060015996568836, 0.042684460000600666, 0.04087412299850257, 0.044025369999872055, 0.04145335300563602, 0.05035295400011819, 0.04365751599834766, 0.04550906100485008, 0.042833352999878116, 0.041135535000648815, 0.044214232002559584, 0.043639716001052875, 0.04103055399900768, 0.04047856899705948, 0.054420852997282054, 0.0413756950001698, 0.0420547319954494, 0.04810079700109782, 0.04739072400116129, 0.04253397999855224, 0.04228699100349331, 0.04903754900442436, 0.04278782799519831, 0.0500781669979915, 0.041988789998868015, 0.04445640100311721, 0.044387159003235865, 0.046846905002894346, 0.04383682899788255, 0.04575817799923243, 0.04354243099805899, 0.044697502999042626, 0.05055494800035376, 0.04385916100000031, 0.047622717000194825, 0.04312985699652927, 0.04341038800339447, 0.042972269002348185, 0.049475204999907874, 0.04213698099920293, 0.04079456200270215, 0.049090695996710565, 0.04601756400370505, 0.04361738299485296, 0.04191817199898651, 0.04287193500204012, 0.044888069001899567, 0.04543399300018791, 0.042025697002827656, 0.04131571199832251, 0.0557429000036791, 0.0426207170021371, 0.04357728799368488, 0.049793470003351104, 0.04567700299958233, 0.04271732800407335, 0.04283337500237394, 0.048518690004129894, 0.04253966400574427, 0.05654905799747212, 0.04501471399998991, 0.04298233400186291, 0.04320816000108607, 0.04578231099731056, 0.042882452995399944, 0.0435199680068763, 0.047858165999059565, 0.04578321099688765, 0.05050874099833891, 0.04397162199893501, 0.04284470099810278, 0.0440669550007442, 0.04516183400119189, 0.04204658299568109, 0.04492085899983067, 0.04855620399757754, 0.040857862994016614, 0.04896724099671701, 0.04504064700449817, 0.041870248001941945, 0.047831923999183346, 0.04420245900109876, 0.04114865000155987, 0.042921407002722844, 0.04215289700368885, 0.045272253002622165, 0.058695075000287034, 0.04247545799444197, 0.04249406199960504, 0.047878511002636515, 0.04334266200021375, 0.04305465400102548, 0.04354352399968775, 0.04203166400111513, 0.04043749699485488, 0.04982702399865957, 0.042037756997160614, 0.047950318999937735, 0.043623789002595004, 0.04322207099903608, 0.046593277002102695, 0.046253580003394745, 0.041583646001527086, 0.044025474999216385, 0.04946219699922949, 0.04234422399895266, 0.04479584599903319, 0.042056558006152045, 0.041813918003754225, 0.04560928399587283, 0.04290177900111303, 0.043007218999264296, 0.05024620199401397, 0.05318240899941884, 0.04403196899511386, 0.04574760399555089, 0.04254025800037198, 0.043274767995171715, 0.044786320002458524, 0.04672708499856526, 0.04261477099498734, 0.04521812899474753, 0.05092140800115885, 0.0433905620011501, 0.046285402997455094, 0.04400262599665439, 0.04749095100123668, 0.04534138600138249, 0.04224600800080225, 0.041390721999050584, 0.04563555100321537, 0.04992288300127257, 0.04415906500071287, 0.047944169004040305, 0.04358800200134283, 0.04412586100079352, 0.04692323700146517, 0.043868146000022534, 0.0416318570059957, 0.043916132002777886, 0.05086967600072967, 0.042928844995913096, 0.04542595399834681, 0.04944568700011587, 0.042282443995645735, 0.04587011400144547, 0.04156348899414297, 0.040691526999580674, 0.04425287199410377, 0.049388801999157295, 0.04187829299917212, 0.04760338599589886, 0.04311148400302045, 0.043693840998457745, 0.044334664999041706, 0.04244878399913432, 0.04378223700041417, 0.045505637004680466, 0.05072125000151573, 0.041408864002733026, 0.04598674999579089, 0.04315634399972623, 0.04490236900164746, 0.044742099998984486, 0.04409249900345458, 0.043927181999606546, 0.04756655200617388, 0.051085111997963395, 0.043485718000738416, 0.045379631003015675, 0.041846191001241095, 0.047888978995615616, 0.04428261300199665, 0.04295844699663576, 0.04361171599884983, 0.04379910599527648, 0.04966138499730732, 0.04284269400523044, 0.0480497149983421, 0.0430183720018249, 0.043070024003100116, 0.04618454500450753, 0.04306637499394128, 0.0433718280037283, 0.042943916996591724, 0.049998370006505866, 0.042557462002150714, 0.047076936993107665, 0.041945591001422144, 0.044799060000514146, 0.04527150399371749, 0.04341568700328935, 0.04197795299842255, 0.048676439000701066, 0.0500484919975861, 0.04429997999977786, 0.04874354800267611, 0.044198171999596525, 0.043735502993513364, 0.04769967599713709, 0.04281222799909301, 0.04176246599672595, 0.04671622499881778, 0.05664365799748339, 0.045898805001343135, 0.04499252899404382, 0.04170202300156234, 0.04438896600186126, 0.043990488004055806, 0.04429513800278073, 0.04130217100464506, 0.04543783900589915, 0.050869181999587454, 0.044114707998232916, 0.0529224769998109, 0.044376685000315774, 0.04470194200257538, 0.048550293999142013, 0.041280024001025595, 0.04378442499728408, 0.043973328996798955, 0.04884336399845779, 0.0432084069980192, 0.04617803300061496, 0.043380270995839965, 0.04728015000000596, 0.04586052100057714, 0.042076209996594116, 0.04124221299571218, 0.044397214995115064, 0.05313137600023765, 0.04316686499805655, 0.04670464599621482, 0.04326941700128373, 0.046336542000062764, 0.04612223200092558, 0.042301401001168415, 0.041531260998453945, 0.04622580899740569, 0.04995597900415305, 0.048460001999046654, 0.04519260799861513, 0.0423953689969494, 0.043100441005663015, 0.046913569000025745, 0.04525122199993348, 0.040992267000547145, 0.04536874499899568, 0.05017093199421652, 0.04706425900076283, 0.044984733998717275, 0.04441927299922099, 0.04367231200012611, 0.04310531300143339, 0.0450124529961613, 0.04529273699881742, 0.04114765100530349, 0.04986316999566043, 0.047701315001177136, 0.044185848004417494, 0.043500202998984605, 0.04356114099937258, 0.04393849299958674, 0.042226137993566226, 0.0435428269993281, 0.04233699299948057, 0.05642898299993249, 0.04151191800337983, 0.04129836300126044, 0.04137635799997952, 0.0479370269968058, 0.04329969199898187, 0.0415472629974829, 0.04266318200097885, 0.043098588997963816, 0.049411517997214105, 0.04183895700407447, 0.04232581399992341, 0.04497983100009151, 0.04055892700125696, 0.043424569004855584, 0.04495895400032168, 0.04460651300178142, 0.04711852299806196, 0.0517366359999869, 0.04105199699552031, 0.04745498499687528, 0.046584457995777484, 0.04679000499891117, 0.04446303500299109, 0.04478976599784801, 0.0416319400028442, 0.04239562300062971, 0.04941773299651686, 0.0452856289994088, 0.04672087799553992, 0.04321969699958572, 0.043998191998980474, 0.045446802003425546, 0.04298171800473938, 0.04191927899955772, 0.04129371100134449, 0.05287072899955092, 0.04360547800024506, 0.043057015995145775, 0.04184605999762425, 0.04542063199914992, 0.0470091120005236, 0.043499377999978606, 0.041961164999520406, 0.04627284700109158, 0.0495066229996155, 0.04493938599625835, 0.04475638199801324, 0.04449019800085807, 0.04354778899869416, 0.047649597996496595, 0.042343660003098194, 0.04563478799536824, 0.04111001999990549, 0.05002195700217271, 0.04535791000671452, 0.04693528899952071, 0.04706683500262443, 0.044419829006073996, 0.04254305000358727, 0.05212125300022308, 0.04465836700546788, 0.04169049300253391, 0.05036585299967555, 0.0450980560053722, 0.04848383399803424, 0.04596835900156293, 0.046000119000382256, 0.048202105004747864, 0.04422269399947254, 0.04241475799790351, 0.042522167997958604, 0.05410529300570488, 0.04437028400570853, 0.043624981997709256, 0.04588736499863444, 0.04398234100517584, 0.04180066999833798, 0.04563387699454324, 0.04440230900218012, 0.040975816002173815, 0.05654903200047556, 0.04317704000277445, 0.04311624600086361, 0.04607891799969366, 0.04231554199941456, 0.04141890999744646, 0.04587885799992364, 0.04885462299716892, 0.040546813994296826, 0.0527842960000271, 0.041514839002047665, 0.04223406699748011, 0.045087623002473265, 0.043562312006542925, 0.04751565300102811, 0.0444637119944673, 0.04102140299801249, 0.04163879899715539, 0.05437939699913841, 0.04192513399902964, 0.04403323199949227, 0.04517248299816856, 0.04129525199823547, 0.043501220003236085, 0.045589203997224104, 0.04502624700398883, 0.04116971299663419, 0.051871784999093506, 0.04111206199740991, 0.048890097001276445, 0.04387503900215961, 0.04487016799976118, 0.04355928899894934, 0.046149531997798476, 0.041339184004755225, 0.044977604993619025, 0.05337489399971673, 0.045056647999444976, 0.0422838500016951, 0.04671509000036167, 0.0434882980043767, 0.04313463600556133, 0.04564921899873298, 0.04424988599930657, 0.04164061100163963, 0.0556687080024858, 0.04284834699501516, 0.04317974200239405, 0.04653281799983233, 0.043683711999619845, 0.04625913000199944, 0.04583961299795192, 0.04547773399826838, 0.04337305799708702, 0.053359126002760604, 0.04274500799510861, 0.04486361199815292, 0.04674700400209986, 0.04388336499687284, 0.0469227450012113, 0.04402134900010424, 0.041173463003360666, 0.042306219998863526, 0.051011686999117956, 0.565969335999398, 0.04395508800371317, 0.04533423400425818, 0.04312669100181665, 0.04172121900046477, 0.04526418499881402, 0.04845912400196539, 0.04459850899729645, 0.04499192799994489, 0.046907342002668884, 0.04076581000117585, 0.04555426400474971, 0.04647383499832358, 0.04409639300138224, 0.04579771000135224, 0.04478012299659895, 0.045150803998694755, 0.047094323999772314, 0.051909349000197835, 0.043943928998487536, 0.04470420799771091, 0.049005433000274934, 0.04502502499963157, 0.04490951399930054, 0.04755948499951046, 0.04438971699710237, 0.04338479600119172, 0.052091053999902215, 0.04634813799930271, 0.046197759998904075, 0.045018327997240704, 0.04293578000215348, 0.04593294899677858, 0.04296596500353189, 0.04294705600477755, 0.04760844400152564, 0.0511516609985847, 0.04490046100545442, 0.04526737799460534, 0.04306495300261304, 0.0463212519971421, 0.04782247100229142, 0.04314425600023242, 0.04559622800297802, 0.04413317000580719, 0.0508733079986996, 0.04750620399863692, 0.04533900899696164, 0.04682683399732923, 0.04356283300148789, 0.04669110499526141, 0.04349934700439917, 0.04784205499890959, 0.044669312999758404, 0.05824844999733614, 0.04470932800177252, 0.044766198996512685, 0.043269006004265975, 0.04548107000300661, 0.04587898799945833, 0.04457748700224329, 0.043536234996281564, 0.04379685599997174, 0.05351704000349855, 0.04400416900170967, 0.04597055799968075, 0.04695483899558894, 0.04616209799860371, 0.047881980004603975, 0.04721333000634331, 0.045785854003042914, 0.04500217399618123, 0.053959650998876896, 0.0446857760034618, 0.04672865499742329, 0.04839089199958835, 0.04424789800395956, 0.043390753999119624, 0.04448349399899598, 0.044253672000195365, 0.04734315400128253, 0.054003276003641076, 0.050564997996843886, 0.051824640002450906, 0.04478497999662068, 0.046056932005740236, 0.04491061600128887, 0.0479025989989168, 0.045935014000860974, 0.04393420099950163, 0.054852161003509536, 0.04239631899690721, 0.04335576899757143, 0.04575107799610123, 0.04526823900232557, 0.04440646899456624, 0.043670418999681715, 0.046668301001773216, 0.04507282399572432, 0.05165653899894096, 0.04246512099780375, 0.041125514995655976, 0.045249287999467924, 0.042296003994124476, 0.042035004997160286, 0.04483776100096293, 0.0438717710057972, 0.04660108299867716, 0.05216891100280918, 0.04169438099779654, 0.041807538000284694, 0.04279458699602401, 0.046693106000020634, 0.04192273400258273, 0.044926749003934674, 0.04785891799838282, 0.04745728999841958, 0.05322836300183553, 0.04533656000421615, 0.043460501001391094, 0.047109954997722525, 0.047539525003230665, 0.041776032994675916, 0.0412968800010276, 0.04555223000352271, 0.0420848530047806, 0.05170056499628117, 0.042412791000970174, 0.04587299600098049, 0.04246082100144122, 0.041254173003835604, 0.04312570999900345, 0.05088984999747481, 0.04348228999879211, 0.04384135699365288, 0.04960806400049478, 0.05012752799666487, 0.04644601199834142, 0.04423978999693645, 0.04435411599843064, 0.04515198100125417, 0.04346283499762649, 0.04158137299964437, 0.04450465699483175, 0.05628709999291459, 0.04255560300225625, 0.04277117099991301, 0.046268885998870246, 0.04384137099987129, 0.04323815599491354, 0.04290043000219157, 0.04693764299736358, 0.040855774997908156, 0.05090833899885183, 0.04527103299915325, 0.04434101700462634, 0.04527672399854055, 0.04371107799670426, 0.043139945999428164, 0.041794974997173995, 0.04868237700429745, 0.04427663899696199, 0.05122443599975668, 0.045530214003520086, 0.04380457300430862, 0.04365399500238709, 0.04696826500003226, 0.043204253997828346, 0.04444528400199488, 0.04476124400389381, 0.04189911700086668, 0.051296894998813514, 0.045842676001484506, 0.04579591000219807, 0.041847025000606664, 0.04499270000087563, 0.042772079999849666, 0.04495990600116784, 0.0450388440003735, 0.041619192001235206, 0.049191952995897736, 0.04694905700307572, 0.04260170699853916, 0.04483285999594955, 0.046468649998132605, 0.0449131000059424, 0.041776605998165905, 0.048126403999049217, 0.04373063600360183, 0.052300126997579355, 0.044262249997700565, 0.04815938799583819, 0.04298938800639007, 0.04546543000469683, 0.04282996300025843, 0.046806810998532455, 0.04231908499787096, 0.0432178099945304, 0.054306828998960555, 0.04397973100276431, 0.04695414200250525, 0.04517109799780883, 0.04638385400176048, 0.04524702100025024, 0.04193313900032081, 0.04096426099567907, 0.043716738997318316, 0.05047204100264935, 0.048033731000032276, 0.05004768700018758, 0.05243702300504083, 0.04191851199720986, 0.04097873000137042, 0.04500631099654129, 0.04170034400158329, 0.04193739499896765, 0.05314156699751038, 0.04083135699329432, 0.04175305399985518, 0.04888139800459612, 0.042119529003684875, 0.042225894998409785, 0.04404787199746352, 0.0431133879974368, 0.046928248993935995, 0.05218853100086562, 0.04147719499451341, 0.043207375005295034, 0.049417153000831604, 0.04407785699731903, 0.04276578999997582, 0.04294918800587766, 0.04718168800172862, 0.04358615999808535, 0.0530980300027295, 0.04398693999974057, 0.04526883600192377, 0.04302152099990053, 0.04171241500444012, 0.041400604000955354, 0.04841241199756041, 0.04136362000281224, 0.04251315299916314, 0.053409731001011096, 0.04501092699501896, 0.047130639002716634, 0.04372650400182465, 0.042730401997687295, 0.04546857000241289, 0.04536565000307746, 0.04597422499500681, 0.0436737960044411, 0.05731801399815595, 0.042341630993178114, 0.043991919003019575, 0.0433505730034085, 0.04787684899929445, 0.04584372400131542, 0.04232238599797711, 0.04384805299923755, 0.045497945000533946, 0.05123935300071025, 0.04285708899988094, 0.04295355000067502, 0.04517103800026234, 0.04422674999659648, 0.04211926899733953, 0.04709423800522927, 0.04456760800530901, 0.04174571299517993, 0.0515051170004881, 0.04520455400052015, 0.043951195999397896, 0.046297446999233216, 0.043189710995648056, 0.04549762900569476, 0.04370495600596769, 0.04277924900088692, 0.04319161400053417, 0.051426981000986416, 0.04390286499983631, 0.042742818004626315, 0.04962893500487553, 0.04410430299321888, 0.045263880005222745, 0.042890802993497346, 0.043862277998414356, 0.041896395006915554, 0.05318653499853099, 0.04645735400117701, 0.043753320001997054, 0.04631558500113897, 0.044187014005729, 0.04607409999880474, 0.043036905000917614, 0.0424639129996649, 0.04560773699631682, 0.05304531599540496, 0.047289732996432576, 0.04521739399933722, 0.04647213399584871, 0.04240906699851621, 0.046012475999305025, 0.041957978006394114, 0.04385621700203046, 0.04235693599912338, 0.05398677000630414, 0.04214420299831545, 0.046296205997350626, 0.04418156599422218, 0.04694383899914101, 0.04416433099686401, 0.04563894899911247, 0.043076134999864735, 0.04772486299771117, 0.05192035000072792, 0.04643456100166077, 0.044106965004175436, 0.0476047570045921, 0.0447095889976481, 0.046526320998964366, 0.04567519199918024, 0.04620179400080815, 0.04316365599515848, 0.05647362900344888, 0.04393814000650309, 0.047171553000225686, 0.04563325600611279, 0.0489972490031505, 0.04420625299826497, 0.045149507001042366, 0.043019123004341964, 0.04285532799985958, 0.052752349998627324, 0.044632589000684675, 0.04200058299466036, 0.0438978029997088, 0.04578432499693008, 0.04164501999912318, 0.04188443200109759, 0.04541589099972043, 0.04036478800117038, 0.04990174400154501, 0.04530973899818491, 0.043055428002844565, 0.042932341995765455, 0.04442689300049096, 0.04254892400058452, 0.044608370000787545, 0.04492107800615486, 0.04149544500251068, 0.04902883799513802, 0.04602932499983581, 0.04314262900152244, 0.04370701599691529, 0.04778752499987604, 0.04490330599946901, 0.04214235599647509, 0.04141284299839754, 0.04358574399520876, 0.049722363000910264, 0.043456407001940534, 0.04543140299938386, 0.04614674900221871, 0.04568122800264973, 0.041602681994845625, 0.041137544001685455, 0.044604925002204254, 0.04129710599954706, 0.04955342400353402, 0.04547344300226541, 0.04383393299940508, 0.04206643599900417, 0.045310765999602154, 0.041516936995321885, 0.043129181998665445, 0.044593298000108916, 0.04209222899953602, 0.052636207998148166, 0.045984104996023234, 0.04662546300096437, 0.043099428999994416, 0.0461165209999308, 0.043414858999312855, 0.043030102002376225, 0.045924498001113534, 0.04103073599981144, 0.050054537001415156, 0.04635695199976908, 0.04431496599863749, 0.04113537399825873, 0.0440261389958323, 0.04198792899842374, 0.04258943299646489, 0.04466732100263471, 0.040972259994305205, 0.049052946000301745, 0.04422965100093279, 0.04176265799469547, 0.043697458000679035, 0.0449242510003387, 0.042625128000508994, 0.042504596000071615, 0.04563570699974662, 0.044103997002821416, 0.05194717700214824, 0.04709306799486512, 0.04378976999578299, 0.04196656699787127, 0.044470449996879324, 0.043827389999933075, 0.04412454300472746, 0.045720524998614565, 0.041858070995658636, 0.05190543999924557, 0.04629311600001529, 0.042531087005045265, 0.04272254199895542, 0.04620537099981448, 0.044474421003542375, 0.045249150003655814, 0.04103356700215954, 0.042010858000139706, 0.05563420100224903, 0.04631473899644334, 0.0473530489980476, 0.04417732699948829, 0.04761213100573514, 0.04545738399610855, 0.04767219600034878, 0.04334915399522288, 0.044956132005609106, 0.054130192002048716, 0.045642381002835464, 0.04544434800482122, 0.04835628699947847, 0.046493574998748954, 0.046158247998391744, 0.04600879899953725, 0.04269709399522981, 0.041675191998365335, 0.054450494004413486, 0.04493798699695617, 0.04364023800007999, 0.044968716996663716, 0.04435070199542679, 0.04446099499909906, 0.047840664999966975, 0.04274904300109483, 0.043830248003359884, 0.049690923995513, 0.0444234519964084, 0.044727066000632476, 0.04346841399819823, 0.04048284300370142, 0.04546241500065662, 0.045220617997983936, 0.046537837995856535, 0.04057917099999031, 0.05312585499632405, 0.0467152229975909, 0.04665655999997398, 0.04318657399562653, 0.045640091004315764, 0.04328149100183509, 0.04539946199656697, 0.0425254700021469, 0.045172313002694864, 0.05071673799830023, 0.04636094599845819, 0.04253985700051999, 0.04518884199933382, 0.045603420003317297, 0.044992469003773294, 0.04351796900300542, 0.04173753099894384, 0.041902721000951715, 0.04831000700505683, 0.04230731799907517, 0.044370030002028216, 0.04522454400284914, 0.04545140099799028, 0.04344029600179056, 0.04698981199908303, 0.04313823599659372, 0.044030954006302636, 0.05128510199574521, 0.042665121000027284, 0.047162319999188185, 0.042515158995229285, 0.04604166899662232, 0.04246604500076501, 0.042960461003531236, 0.04161912299605319, 0.042484746998525225, 0.04958166900178185, 0.04073039699869696, 0.04501673400227446, 0.04291537400422385, 0.04096564099745592, 0.044178467003803235, 0.04040870300377719, 0.0454042479977943, 0.04258047100302065, 0.051772267994238064, 0.040984657003718894, 0.04466673100250773, 0.041778475999308284, 0.043693683997844346, 0.0420788480041665, 0.043028555999626406, 0.04042922199732857, 0.04269769600068685, 0.05095772899949225, 0.04623228400305379, 0.043341679993318394, 0.04491437999968184, 0.04394927999965148, 0.04605521600024076, 0.041497274993162137, 0.04419436800526455, 0.04148703299870249, 0.053774972999235615, 0.041033718996914104, 0.04507422200549627, 0.04347216799942544, 0.04606574100034777, 0.04730577699956484, 0.045503597997594625, 0.041968124001869, 0.04894701099692611, 0.05288055199343944, 0.046600837995356414, 0.04240639100316912, 0.044688622998364735, 0.04141948499454884, 0.04863852899870835, 0.04392113899666583, 0.046311229998536874, 0.041531242000928614, 0.053741133997391444, 0.042817797999305185, 0.04596714099898236, 0.043590547000349034, 0.04857669199554948, 0.04437172899633879, 0.04394403500191402, 0.04412627599958796, 0.04441960099939024, 0.05336685500515159, 0.0442443860010826, 0.04097054600424599, 0.04649609699845314, 0.042873307000263594, 0.04302965599345043, 0.04973741000139853, 0.04567592999956105, 0.04249912600062089, 0.05553311800031224, 0.04362381499959156, 0.043430924000858795, 0.0459029690027819, 0.04508317400177475, 0.044076406993553974, 0.04594841400103178, 0.043745720002334565, 0.04231961500045145, 0.05369957200309727, 0.04151611099950969, 0.04099939700245159, 0.044059603998903185, 0.04628961900016293, 0.04316204299539095, 0.04477989400038496, 0.04186057799961418, 0.04120192799746292, 0.05004929900314892, 0.04714450099709211, 0.042605946997355204, 0.04282479399989825, 0.041396165994228795, 0.04472213899862254, 0.04071681699861074, 0.04137330000230577, 0.04201974900206551, 0.053112465000594966, 0.04178653300186852, 0.04215506900072796, 0.04309111299517099, 0.04403043400088791, 0.044539829999848735, 0.042111952003324404, 0.041147865005768836, 0.04314129499834962, 0.0484652239974821, 0.04266692400415195, 0.042006726005638484, 0.04650181000033626, 0.04543581399775576, 0.04324027900293004, 0.04254492100153584, 0.046325306000653654, 0.04241042600187939, 0.0510329280004953, 0.04360888699739007, 0.04612050899595488, 0.04341476500121644, 0.04080190600507194, 0.04297574100201018, 0.04760822100070072, 0.04339341299782973, 0.04150911499891663, 0.05407330999878468, 0.04260917300416622, 0.04377985700557474, 0.042150717999902554, 0.046252997002738994, 0.04410584399738582, 0.040582688998256344, 0.04411944300227333, 0.04156097600207431, 0.04938723400118761, 0.048207414998614695, 0.047574184005497955, 0.04519242099922849, 0.04471270700014429, 0.04738379699847428, 0.047609325003577396, 0.045374061999609694, 0.04594839000492357, 0.051021132996538654, 0.043087587000627536, 0.04455879000306595, 0.04580729900044389, 0.04506606500217458, 0.045749936005449854, 0.04314932100533042, 0.0431500650011003, 0.04481816000043182, 0.05299610400106758, 0.043864444000064395, 0.045580238998809364, 0.04402098299760837, 0.04479083100159187, 0.04587527799594682, 0.04515964299935149, 0.04154575300344732, 0.04546172399568604, 0.052734683005837724, 0.04801309100002982, 0.04632243200467201, 0.042970652000803966, 0.04502566000155639, 0.042080279999936465, 0.04180497799825389, 0.041956050001317635, 0.04593002199544571, 0.051465695003571454, 0.04076135899958899, 0.04154776300129015, 0.047863471998425666, 0.04287540999939665, 0.042183125995507, 0.04160522399615729, 0.04564097100228537, 0.041868414999044035, 0.05463741700077662, 0.0459633450009278, 0.04583402400021441, 0.040828114993928466, 0.04569440000341274, 0.04506293400481809, 0.04641928899945924, 0.0432332190030138, 0.04208492600446334, 0.051315030999830924, 0.04352384099911433, 0.04208404800010612, 0.04217121299734572, 0.04577669699938269, 0.044671775998722296, 0.043543324994971044, 0.04097249200276565, 0.03992172799917171, 0.05149246800283436, 0.041778165999858174, 0.04291289099637652, 0.04564387600112241, 0.04560848700202769, 0.044698379999317694, 0.04204841099999612, 0.041372368003067095, 0.0458014780015219, 0.05193963000056101, 0.04313639500469435, 0.04332707900175592, 0.04851825800142251, 0.04427364200091688, 0.04429810399597045, 0.047552379000990186, 0.053710494001279585, 0.04293374300323194, 0.05247424000117462, 0.04270050299965078, 0.04717770999559434, 0.043177879000722896, 0.04671663499902934, 0.047489300995948724, 0.04606311499810545, 0.04396157700102776, 0.04491137900186004, 0.05396013799327193, 0.04597627899784129, 0.046356070997717325, 0.04564085700258147, 0.046503300000040326, 0.04606440800125711, 0.04576887500297744, 0.04394998300267616, 0.043121762995724566, 0.05200611199688865, 0.04543713300517993, 0.046460176999971736, 0.0463130740026827, 0.04697254900383996, 0.0470804440046777, 0.04088612299528904, 0.04512826900463551, 0.040403607999905944, 0.057595314996433444, 0.04311456100549549, 0.04535122000379488, 0.0425870129984105, 0.045539034996181726, 0.041899499999999534, 0.047702994001156185, 0.04201637199730612, 0.04405656299786642, 0.0505748690047767, 0.04476780600089114, 0.044493590998172294, 0.04491249399870867, 0.04152218699891819, 0.04875827399519039, 0.04388626099535031, 0.0451365650005755, 0.04189138400397496, 0.0591584390058415, 0.04214690499793505, 0.04605287299636984, 0.04356154400011292, 0.046130547001666855, 0.041015070004505105, 0.044904246999067254, 0.043300199999066535, 0.04617607800173573, 0.05144310600007884, 0.04443403299956117, 0.043131104997883085, 0.046024957999179605, 0.044416882999939844, 0.04649668100319104, 0.04575664200092433, 0.04622977500548586, 0.043595294999249745, 0.05361168699892005, 0.04348045199731132, 0.04423654600395821, 0.04416170599870384, 0.0482359509987873, 0.04245287799858488, 0.04455431200040039, 0.042432841000845656, 0.04507122899667593, 0.05306140799802961, 0.04412738000246463, 0.04680286200164119, 0.045673594999243505, 0.04310099300346337, 0.04425392299890518, 0.042014917999040335, 0.044053140001778957, 0.041778295002586674, 0.052915325002686586, 0.04399601399927633, 0.04534298799990211, 0.04460358300275402, 0.046482187004585285, 0.04328885200084187, 0.04527557199617149, 0.04356619000463979, 0.04566922599769896, 0.051467999001033604, 0.04575679400295485, 0.0415296369974385, 0.044680596998659894, 0.047888433997286484, 0.04117181200126652, 0.0452883190009743, 0.04343281999899773, 0.04417491499771131, 0.05238942500000121, 0.046763473001192324, 0.04478034299972933, 0.04457486799947219, 0.04590728000039235, 0.04702717200416373, 0.04292400899430504, 0.044832800995209254, 0.042415004994836636, 0.053810532997886185, 0.04243246899568476, 0.04552954900282202, 0.043153371996595524, 0.04399504700268153, 0.041471084994554985, 0.04634874099428998, 0.04305072699935408, 0.04838966600073036, 0.05113801499828696, 0.04712282899708953, 0.04230858500523027, 0.04129871400073171, 0.0446216850032215, 0.04502924800181063, 0.042550657999527175, 0.042130344998440705, 0.04325582800083794, 0.049668248000671156, 0.0476367079972988, 0.04212236500461586, 0.04613101499853656, 0.042670055001508445, 0.04193941999983508, 0.043541893996007275, 0.04426878000231227, 0.041342507996887434, 0.05506917599996086, 0.04434183299599681, 0.04551665900362423, 0.043893082001886796, 0.04267854399950011, 0.045684684002480935, 0.044261848997848574, 0.04342674800136592, 0.04308360900176922, 0.054791326001577545, 0.046812842003419064, 0.04352583599393256, 0.04328314900340047, 0.04141534499649424, 0.042168029001913965, 0.04361411300487816, 0.04018963599810377, 0.040315905993338674, 0.049463879004179034, 0.04826401900209021, 0.04613446300209034, 0.04198188900045352, 0.041084272001171485, 0.04170506900118198, 0.04139748200395843, 0.044938807004655246, 0.03978025900141802, 0.04979229399759788, 0.04140668900072342, 0.04624773300020024, 0.04464608900161693, 0.044473586000094656, 0.043805346998851746, 0.04457666099915514, 0.041062532996875234, 0.04122652300429763, 0.05368806499609491, 0.042613361998519395, 0.04390869399503572, 0.047384184996190015, 0.04242256599536631, 0.04200881099677645, 0.04567675800353754, 0.04155770999932429, 0.04314455800340511, 0.05286503599927528, 0.044694367999909446, 0.042628460003470536, 0.04405976400448708, 0.042255080996255856, 0.04184240600443445, 0.04497699099738384, 0.04142786000011256, 0.04501122100191424, 0.051163762000214774, 0.04277372800424928, 0.04637942599947564, 0.043145167001057416, 0.044880815999931656, 0.04233710200060159, 0.04240994100109674, 0.04446756499964977, 0.04273706099775154, 0.04879798299953109, 0.0434695610019844, 0.041759420004382264, 0.045166502000938635, 0.04284250399359735, 0.04243175400188193, 0.04123894900112646, 0.04049729899998056, 0.045273358999111224, 0.04792448400257854, 0.043684422998921946, 0.04427519699675031, 0.04526418400200782, 0.04364759499731008, 0.047422791001736186, 0.042041690001497045, 0.04224926700408105, 0.04424130099505419, 0.05545948899816722, 0.04274005100160139, 0.045196289000159595, 0.041921705997083336, 0.043398847003118135, 0.04809016799845267, 0.0442346700001508, 0.042377954996482003, 0.04287079200003063, 0.050504368999099825, 0.04531053499522386, 0.041459954001766164, 0.040584707996458746, 0.04242346400133101, 0.041740234999451786, 0.045174000995757524, 0.04357434099802049, 0.0426056639989838, 0.052754999000171665, 0.04197202399518574, 0.0431373760002316, 0.044982487001107074, 0.046294250001665205, 0.04239799800416222, 0.04318663499725517, 0.04253198899823474, 0.0407837059974554, 0.053310002003854606, 0.04172332800226286, 0.04576250199897913, 0.04226530400046613, 0.04240571000264026, 0.04564029299945105, 0.04388015199947404, 0.039996062994759995, 0.040846174997568596, 0.04970504499942763, 0.042027609000797383, 0.0452929290040629, 0.04524646499339724, 0.043155396000656765, 0.04319022800336825, 0.045652132997929584, 0.044961743005842436, 0.04182913000113331, 0.049168195997481234, 0.04447210900252685, 0.043854167000972666, 0.04555735499889124, 0.045510258998547215, 0.04391358500288334, 0.044627736999245826, 0.04209376600192627, 0.040323034998436924, 0.05369368099491112, 0.04390191000129562, 0.04281461799837416, 0.0444349350000266, 0.04292461099976208, 0.045138216999475844, 0.04338759000529535, 0.04236490299808793, 0.040972319002321456, 0.050288023994653486, 0.045966796002176125, 0.04889450799964834, 0.04240863099403214, 0.04365788200084353, 0.045624797996424604, 0.045268670997757, 0.04367815399746178, 0.04090864799945848, 0.048391677999461535, 0.042574019003950525, 0.04719359900627751, 0.045167407995904796, 0.041620771997259, 0.045295399002498016, 0.04268754299846478, 0.04480898600013461, 0.041779193001275416, 0.047960046998923644, 0.04205297800217522, 0.041999954999482725, 0.04897452300065197, 0.04141925300064031, 0.04267375700146658, 0.04688142600207357, 0.043257659002847504, 0.04497727900161408, 0.05005771700234618, 0.04548052699828986, 0.043642415999784134, 0.045313978000194766, 0.04362914899684256, 0.04460988900245866, 0.04198624499986181, 0.044345959002384916, 0.04171935699559981, 0.04962489200261189, 0.044417837998480536, 0.04561565999756567, 0.0423753819995909, 0.04061697900033323, 0.040993736998643726, 0.04339128200081177, 0.04204589199798647, 0.04073968499869807, 0.05014307999954326, 0.04602276200603228, 0.04849788999854354, 0.05216233999817632, 0.04518993700185092, 0.043876363000890706, 0.041656809000414796, 0.04316749000281561, 0.041130767996946815, 0.05281978700077161, 0.04467519500030903, 0.0429008920036722, 0.04430870299984235, 0.04570331000286387, 0.0441342589983833, 0.0422647009982029, 0.041832264003460295, 0.045135140004276764, 0.052260935997765046, 0.043479776999447495, 0.043982419003441464, 0.044350557000143453, 0.04265495100116823, 0.04324665399326477, 0.04268810400390066, 0.04434073100128444, 0.04443352700036485, 0.05023413500020979, 0.04157039200072177, 0.04594376299792202, 0.04297028199653141, 0.044671189003565814, 0.04275496199988993, 0.04439906599873211, 0.050218912001582794, 0.04218273999867961, 0.052253212001232896, 0.0458444129981217, 0.043186324000998866, 0.04336334500112571, 0.043015956005547196, 0.0446237470023334, 0.04119342299964046, 0.042366310000943486, 0.041486125002847984, 0.05181253999762703, 0.042815704000531696, 0.04416960399976233, 0.04219098199973814, 0.04480867699749069, 0.04476095299469307, 0.04265375500108348, 0.04307618000166258, 0.04698515900236089, 0.05625648899876978, 0.0451188250008272, 0.04181465299916454, 0.04692053300095722, 0.04294947100424906, 0.0431727549948846, 0.04527959299593931, 0.04652896300103748, 0.04129168600047706, 0.04932893899967894, 0.04230858100345358, 0.04657028999645263, 0.0442218669995782, 0.04256737299874658, 0.041178061001119204, 0.04512160399463028, 0.044313894999504555, 0.04085832200507866, 0.05046185499668354, 0.04464711100445129, 0.044211690001247916, 0.04329095600405708, 0.04245339200133458, 0.044852029997855425, 0.045520473999204114, 0.04250948399567278, 0.04165611299686134, 0.04897153899946716, 0.04646305499773007, 0.04319118999410421, 0.04185790799965616, 0.04504161400109297, 0.044906340997840744, 0.041882921002979856, 0.04153375900204992, 0.042840686000999995, 0.05415714800619753, 0.04194534200360067, 0.041212840005755424, 0.0423836210029549, 0.04716071800066857, 0.04181044600409223, 0.04179307900631102, 0.04143495600146707, 0.04586868899787078, 0.050105674999940675, 0.04257671600498725, 0.04989380500046536, 0.04586415200174088, 0.04174693200184265, 0.04196238300210098, 0.04428894300508546, 0.04475742900103796, 0.041858258002321236, 0.047684959004982375, 0.043038675998104736, 0.04476350500044646, 0.04306000200449489, 0.042578294000122696, 0.04487458799849264, 0.045157831998949405, 0.04150223799661035, 0.04063139200297883, 0.05354797499603592, 0.04649835100281052, 0.042000460998679046, 0.04132765900430968, 0.04259496199665591, 0.04580762799741933, 0.0433502009982476, 0.04248251099488698, 0.04325362099916674, 0.05394222500035539, 0.04097882499627303, 0.04491864000010537, 0.0470043599998462, 0.04546584100171458, 0.046517627000866923, 0.043678515998180956, 0.04492161799862515, 0.04454056800022954, 0.051216724001278635, 0.04243704999680631, 0.043622019002214074, 0.04842262199963443, 0.04373452500294661, 0.041374559004907496, 0.041594785994675476, 0.04559697499644244, 0.04275290600344306, 0.05259577700053342, 0.04081878400029382, 0.04522847400221508, 0.04550836400449043, 0.04389964899746701, 0.04329268899891758, 0.04442016500252066, 0.045716676002484746, 0.04045448700344423, 0.04874500600271858, 0.043796858000860084, 0.04404024500399828, 0.049254434001340996, 0.044951314994250424, 0.043828358000610024, 0.04660625099495519, 0.04323616799956653, 0.043857625998498406, 0.050733739997667726, 0.045056527997076046, 0.04222472500259755, 0.045623762998729944, 0.05503367599885678, 0.04373430900159292, 0.042599077001796104, 0.0433041520009283, 0.04235004699876299, 0.0519870339994668, 0.04147218500293093, 0.04152320499997586, 0.040841476002242416, 0.04449246599688195, 0.045624509002664126, 0.04295735599589534, 0.04083854200143833, 0.04297319200122729, 0.05483012000331655, 0.04618353299883893, 0.04243680299987318, 0.04542505599965807, 0.05505717100459151, 0.04661311900417786, 0.045273192998138256, 0.04194414299854543, 0.044137427998066414, 0.04982349700003397, 0.043523686996195465, 0.046195169001293834, 0.0431851910034311, 0.04474038899934385, 0.042791993997525424, 0.04266807800013339, 0.04306313200504519, 0.04243861600116361, 0.05008244499913417, 0.04389589400670957, 0.042856700005359016, 0.045189570999355055, 0.044380669001839124, 0.04656280799827073, 0.044649198003753554, 0.045738449996861164, 0.04387177999888081, 0.050018616995657794, 0.04466801899980055, 0.04116343500209041, 0.04157618099998217, 0.044813481996243354, 0.04303070500463946, 0.044152481998025905, 0.04315138699894305, 0.04259627000283217, 0.05250768300174968, 0.04671067599701928, 0.04428453100263141, 0.04386321000492899, 0.04300863700336777, 0.045892269001342356, 0.04504513800202403, 0.041943754004023504, 0.04315561599651119, 0.0540760349977063, 0.044849896999949124, 0.041723668000486214, 0.042035798003780656, 0.042723569997178856, 0.043771112999820616, 0.045171689002017956, 0.04385200199612882, 0.042540561997157056, 0.051905993997934274, 0.0423667610011762, 0.04553570900316117, 0.04295294699841179, 0.04407189400080824, 0.04711873699852731, 0.04363744999864139, 0.045380900002783164, 0.0408802599995397, 0.049171442005899735, 0.041552561000571586, 0.043310929999279324, 0.04906768799992278, 0.044203152996487916, 0.04362789799779421, 0.0451831839964143, 0.0405653340058052, 0.04500857800303493, 0.05351464400155237, 0.04343420999794034, 0.04202866699779406, 0.04398210100043798, 0.04626429000200005, 0.0436802969998098, 0.04299019699828932, 0.041591499000787735, 0.04179594600282144, 0.0534431910054991, 0.0431330919964239, 0.042495004992815666, 0.04367389999970328, 0.043935821995546576, 0.04472751499997685, 0.04312255400145659, 0.04338503500184743, 0.045267276000231504, 0.04855401500390144, 0.04434076499455841, 0.04337584099994274, 0.041182608001690824, 0.04321698300191201, 0.04371037899545627, 0.0463327899997239, 0.04723671099782223, 0.04114828599995235, 0.05137023399583995, 0.04459682900051121, 0.04728808399522677, 0.04525189300329657, 0.04645004499616334, 0.04512694000004558, 0.046558361005736515, 0.04591339000035077, 0.04532056800235296, 0.054241991994786076, 0.04477385499922093, 0.044650236995948944, 0.04564346800179919, 0.04658863299846416, 0.0435706680000294, 0.04226874800224323, 0.04067464500258211, 0.04306706299394136, 0.05256412999733584, 0.04349566000018967, 0.043319551994500216, 0.04338706799899228, 0.04446384499897249, 0.044026585994288325, 0.04584997300116811, 0.04247396599384956, 0.041618982002546545, 0.054242006001004484, 0.042982245999155566, 0.04258575700077927, 0.04225944600329967, 0.043342454999219626, 0.048046266005258076, 0.044537617002788465, 0.04197015100362478, 0.04114890300115803, 0.050167664005130064, 0.04437056300230324, 0.043795976002002135, 0.042485468002269045, 0.045897611998952925, 0.0452530789989396, 0.045104563992936164, 0.041271227004472166, 0.04145584799698554, 0.04789266700390726, 0.042611781995219644, 0.04525587399984943, 0.04300241999590071, 0.047953902001609094, 0.04163328000140609, 0.04051494099985575, 0.04435825000109617, 0.04016020199924242, 0.05243679400155088, 0.04297399800270796, 0.04509270800190279, 0.04739802100084489, 0.041433573998801876, 0.046297799999592826, 0.04206301600061124, 0.0422502190049272, 0.04380041600234108, 0.05167532299674349, 0.04273833100160118, 0.04493629500211682, 0.04118783599551534, 0.04757157800486311, 0.0478825259997393, 0.04303029600123409, 0.04164430899982108, 0.042002980000688694, 0.05574504799733404, 0.04333409299579216, 0.04301713700260734, 0.04393489100039005, 0.04183892999571981, 0.048921569003141485, 0.0437240280007245, 0.04200935599510558, 0.040151549997972324, 0.05202091100363759, 0.5502912250012741, 0.042279175999283325, 0.04237987699889345, 0.04950265099614626, 0.044398164995072875, 0.044947762005904224, 0.045385228004306555, 0.04435672899853671, 0.047235555000952445, 0.041213260003132746, 0.03995308399316855, 0.04731190699385479, 0.043578634002187755, 0.04258810199826257, 0.04319134900288191, 0.04193077800300671, 0.04419541600509547, 0.049842328000522684, 0.04235813100240193, 0.040809965001244564, 0.04382522500236519, 0.04394183099793736, 0.041034665002371185, 0.04584200499812141, 0.04566626500309212, 0.040851231999113224, 0.04924533799930941, 0.04218263200164074, 0.044916971994098276, 0.04531768200104125, 0.0435134349972941, 0.041181584005244076, 0.04672610299894586, 0.04334412999742199, 0.04250319700076943, 0.0496404769946821, 0.04645208999863826, 0.042988624998542946, 0.04322153799876105, 0.04200528299406869, 0.046592318001785316, 0.04117552200477803, 0.04355493099865271, 0.04090042700408958, 0.05295124699478038, 0.04414482300489908, 0.04321426599926781, 0.04374775099859107, 0.05040840199944796, 0.04119808899849886, 0.04114811999897938, 0.04115499799809186, 0.044082411004637834, 0.050433027994586155, 0.04268962400237797, 0.04332422000152292, 0.04519502699986333, 0.04439033800008474, 0.042417085998749826, 0.042418618002557196, 0.04650148300424917, 0.04209864600125002, 0.054125526003190316, 0.041853587004879955, 0.045548916001280304, 0.04319705699890619, 0.04467001400189474, 0.049264219000178855, 0.041063596996536944, 0.04186061999644153, 0.04611740600375924, 0.049049984001612756, 0.041487766000500415, 0.044237932001124136, 0.043205884998315014, 0.04189657399547286, 0.04099060600128723, 0.04431191199546447, 0.04320946700318018, 0.04129550499783363, 0.04887920599867357, 0.04475692699634237, 0.044700607999402564, 0.046257927002443466, 0.04521312600263627, 0.044091089999710675, 0.04594157099927543, 0.041980040005000774, 0.04199578300176654, 0.05426437400456052, 0.042560911999316886, 0.0424311500028125, 0.04325295199669199, 0.041883174002578016, 0.04268591200525407, 0.05148134599585319, 0.04426230899844086, 0.04297062400291907, 0.05443675300193718, 0.04061594400263857, 0.03992734100029338, 0.045033622001938056, 0.04536005100089824, 0.04603564299759455, 0.04232801799662411, 0.041909195999323856, 0.04926866300229449, 0.05143262199999299, 0.04251621600269573, 0.041426520998356864, 0.04749317100504413, 0.045339020995015744, 0.04236708600365091, 0.044383016997016966, 0.0425008300007903, 0.04644902799918782, 0.055204396005137824, 0.0423725939981523, 0.04334487899905071, 0.046727212000405416, 0.04439204499794869, 0.04273378599464195, 0.0454780829968513, 0.04338259300129721, 0.04502891300217016, 0.05541949900361942, 0.045598889999382664, 0.04380531999777304, 0.041680804002680816, 0.04374699199979659, 0.042234190994349774, 0.04249956399871735, 0.049237074999837205, 0.04552626499935286, 0.05541941100091208, 0.04167048500676174, 0.042410214999108575, 0.04456307699729223, 0.046896760002709925, 0.0495419770013541, 0.04362101299921051, 0.04895284299709601, 0.044231018000573386, 0.051967696999781765, 0.04371214599814266, 0.04464567799732322, 0.0424446069955593, 0.0432920009989175, 0.04386214199621463, 0.04784257899882505, 0.044392059004167095, 0.04166871499910485, 0.049077690004196484, 0.046268199002952315, 0.043718271001125686, 0.04402358500374248, 0.04333855500590289, 0.047415577006177045, 0.04230394999467535, 0.04365749700082233, 0.041024494006705936, 0.05323873799352441, 0.04515848399751121, 0.04360791299404809, 0.04303208900091704, 0.04743627499556169, 0.04711415600468172, 0.042441384997800924, 0.041002287005539984, 0.0430347889996483, 0.04961749199719634, 0.04248049500165507, 0.04282751100254245, 0.04477388299710583, 0.042089354996278416, 0.04357953599537723, 0.04346635699766921, 0.04798309799662093, 0.0455532129999483, 0.051457431000017095, 0.04494651799905114, 0.04922703000192996, 0.04625412099994719, 0.0450426269962918, 0.04306225800246466, 0.04490926000289619, 0.042951689996698406, 0.0422361050004838, 0.05057023499830393, 0.04580791400076123, 0.042983375999028794, 0.04507215200283099, 0.04316930999630131, 0.04581738499837229, 0.043475906997628044, 0.04187566399923526, 0.04110683200269705, 0.05434030300239101, 0.04519660899677547, 0.043729952005378436, 0.04336816800059751, 0.045876894997491036, 0.04615252999792574, 0.04221954599779565, 0.04132167100033257, 0.04213038099987898, 0.054498826000781264, 0.04388831999676768, 0.04232125999988057, 0.04348001099424437, 0.047995913002523594, 0.0448711720018764, 0.041988603996287566, 0.041871130000799894, 0.0448370989979594, 0.051822324996464886, 0.04631062800035579, 0.0416860999976052, 0.04181470400362741, 0.04255401900445577, 0.04227214399725199, 0.04660630899888929, 0.04576616100530373, 0.04050252900196938, 0.051169211001251824, 0.04244473700236995, 0.047035666000738274, 0.04135458800010383, 0.049607466004090384, 0.04672944999765605, 0.044725763000315055, 0.04164816700358642, 0.043106113000249024, 0.0502151690016035, 0.044756132003385574, 0.04247512100118911, 0.042909133997454774, 0.04483708999759983, 0.044720238001900725, 0.0431496099990909, 0.04154102699976647, 0.04476143899955787, 0.05151044599915622, 0.04574163899815176, 0.04106856299767969, 0.0436201459961012, 0.04261973399843555, 0.04193978600233095, 0.045856794000428636, 0.04449257500527892, 0.04250325500470353, 0.049990195999271236, 0.04637568900216138, 0.042681042999902274, 0.042976324999472126, 0.043740906003222335, 0.04127015999983996, 0.04111842800193699, 0.04850569999689469, 0.04264906799653545, 0.05315004299336579, 0.04845601700071711, 0.044017732005158905, 0.04772757700266084, 0.046279397996841, 0.04440328799682902, 0.043226244997640606, 0.04558096700202441, 0.04130003200407373, 0.04947305999667151, 0.04991275099746417, 0.045559956997749396, 0.04462189300102182, 0.0453254919993924, 0.0419867019954836, 0.045073064997268375, 0.04341192400170257, 0.044704808002279606, 0.050427130998286884, 0.04093068600195693, 0.04826983700331766, 0.043157682004675735, 0.04267284899833612, 0.04297038799995789, 0.04445898900303291, 0.04069246700237272, 0.04522606999671552, 0.052780989004531875, 0.041648095997516066, 0.042127512002480216, 0.04216808599448996, 0.048070124998048414, 0.04451755899935961, 0.04390352699556388, 0.04068433600332355, 0.04362072500225622, 0.04934125400177436, 0.041422204005357344, 0.04094720500143012, 0.044979614998737816, 0.043150619996595196, 0.043202315995586105, 0.05097992700029863, 0.04585834300087299, 0.04204905899678124, 0.04943500899389619, 0.04236527199827833, 0.050655124003242236, 0.04374015799839981, 0.045714599000348244, 0.04085333000693936, 0.04886762199748773, 0.043654888999299146, 0.04200476199912373, 0.04940226500184508, 0.046953682001912966, 0.04188773300120374, 0.042294066995964386, 0.047574374002579134, 0.0431920910050394, 0.04717025999707403, 0.0418735610001022, 0.041850246001558844, 0.05162321099487599, 0.04325140199944144, 0.04727778500091517, 0.043870411995158065, 0.050000491995888297, 0.04296840599999996, 0.04365631700056838, 0.041525157997966744, 0.039876319002360106, 0.0477823859982891, 0.045861159000196494, 0.043255894001049455, 0.04638203500508098, 0.04475700300099561, 0.04301919299905421, 0.04521758099872386, 0.04244264699809719, 0.04279771900473861, 0.0523011509940261, 0.04184045599686215, 0.0462508179989527, 0.04324973199982196, 0.04244528600247577, 0.04118151400325587, 0.04049244599445956, 0.04418651699961629, 0.04077809199952753, 0.05335531099990476, 0.04271326999878511, 0.04098264200001722, 0.04452382599993143, 0.04335812899807934, 0.04559725199942477, 0.0425620739988517, 0.040396493997832295, 0.0429236799973296, 0.0510936610007775, 0.044204142002854496, 0.04175635799765587, 0.042519911999988835, 0.04587798900320195, 0.04215254000155255, 0.042079441001988016, 0.04255570900568273, 0.04304411100019934, 0.054135020000103395, 0.04364793800050393, 0.041805206004937645, 0.04324710200307891, 0.044812886997533496, 0.04506937999394722, 0.04116000000067288, 0.04201747300248826, 0.04397968099510763, 0.05218027500086464, 0.044404055006452836, 0.04308383299940033, 0.04191688799619442, 0.04340468199370662, 0.050035872001899406, 0.04418554100266192, 0.041098041998338886, 0.03987099100049818, 0.05504011300217826, 0.041262475002440624, 0.04151318900403567, 0.042037382001581136, 0.04563858999608783, 0.04235505800170358, 0.04268624100222951, 0.0432075239950791, 0.0464071789974696, 0.05460981000214815, 0.04222151599969948, 0.042624528003216255, 0.04442670899879886, 0.04183005600498291, 0.04183487200498348, 0.040886798000428826, 0.0449780710041523, 0.045226748996356037, 0.04947917300160043, 0.040309818999958225, 0.050442303996533155, 0.04208596399985254, 0.04267673700087471, 0.04216871700191405, 0.04395135099912295, 0.04354025499924319, 0.04134893300215481, 0.05119972899410641, 0.046534445995348506, 0.04588712799886707, 0.04148079400329152, 0.04228188900015084, 0.0458324619976338, 0.04184306700335583, 0.04354694500216283, 0.04237731500325026, 0.05487995700241299, 0.04283830999338534, 0.042240919996402226, 0.04675005900207907, 0.045121135997760575, 0.0445483589937794, 0.04362417799711693, 0.042437388001417276, 0.04352978699898813, 0.050575032000779174, 0.043967612997221295, 0.045535776000178885, 0.04391534499882255, 0.04612158900272334, 0.04310423000424635, 0.04526440599875059, 0.04549923099693842, 0.04139964199566748, 0.05019107100088149, 0.04433390300255269, 0.04505445899849292, 0.0437906510051107, 0.044405443004507106, 0.042222573996696156, 0.045683080999879166, 0.04328114199597621, 0.042231891995470505, 0.049589754999033175, 0.045416189001116436, 0.04318217999389162, 0.04432493000058457, 0.0455807129983441, 0.04708410199964419, 0.04329320900433231, 0.042073174998222385, 0.042370952003693674, 0.053615787997841835, 0.04119354599970393, 0.04537967599753756, 0.0437080240008072, 0.04494047399930423, 0.04417812999599846, 0.046585920004872605, 0.04823717800172744, 0.04410495900083333, 0.049547162001545075, 0.04102891300135525, 0.042109164998691995, 0.04435837100027129, 0.04241462899517501, 0.04370685100002447, 0.043712539998523425, 0.04360030200041365, 0.04082610399927944, 0.048405991998151876, 0.049292373994830996, 0.04322259799664607, 0.043678786998498254, 0.043082517993752845, 0.04223082100361353, 0.046352615994692314, 0.04282357199554099, 0.04196147600305267, 0.05318169900419889, 0.04903220999403857, 0.041477515995211434, 0.0419172769979923, 0.04362967899942305, 0.044417535995307844, 0.043699883004592266, 0.042760819000250194, 0.04221512399817584, 0.05356620199745521, 0.04258306200063089, 0.04458437299763318, 0.04228590800630627, 0.04682154400506988, 0.04335767299926374, 0.04290456699527567, 0.04358966300060274, 0.045465638002497144, 0.04892115999973612, 0.04026434300612891, 0.04298319700319553, 0.04613002699625213, 0.04333133999898564, 0.04282805899856612, 0.04308837999997195, 0.0456805669964524, 0.04263632299989695, 0.05288342999847373, 0.04198203099804232, 0.04474481099896366, 0.04089118899719324, 0.04270889899635222, 0.04152776100090705, 0.04376435699668946, 0.04080915200029267, 0.03958008599875029, 0.05052655199688161, 0.04378714199992828, 0.041437087005760986, 0.04838547799590742, 0.042679302001488395, 0.044822010000643786, 0.04090742300468264, 0.041412335995119065, 0.04030275099648861, 0.051862088999769185, 0.041344339995703194, 0.045471345001715235, 0.041610202999436297, 0.04725888100074371, 0.041833098002825864, 0.04304424700239906, 0.043031604000134394, 0.04568330699839862, 0.05198435499914922, 0.0421993600029964, 0.042535505002888385, 0.04543461900175316, 0.04378284599806648, 0.04268683400005102, 0.04410910399747081, 0.044590540004719514, 0.042637944003217854, 0.052944028000638355, 0.04366035100247245, 0.04407223199814325, 0.048866391996853054, 0.04405088900239207, 0.04285244700440671, 0.04472995499963872, 0.042727330001071095, 0.046843985001032706, 0.04943175699736457, 0.04815230999520281, 0.04233554199890932, 0.04326938099984545, 0.04418016400450142, 0.04497484699822962, 0.043119848996866494, 0.041822840001259465, 0.04203858199616661, 0.05422308700508438, 0.04244026300148107, 0.04254617200058419, 0.0420733150021988, 0.043866351996257436, 0.043843752006068826, 0.047920642005919944, 0.04319891900377115, 0.04406506400118815, 0.05205861800641287, 0.042178435003734194, 0.043295843999658246, 0.04953633900004206, 0.044430354006181005, 0.04295842900319258, 0.04276943299919367, 0.04417481100244913, 0.04155343899765285, 0.052800391997152474, 0.041802856998401694, 0.04408796300413087, 0.04534473399689887, 0.04221900400443701, 0.04306578500109026, 0.04477567599678878, 0.04311437399883289, 0.04307703400263563, 0.051648655004100874, 0.04573111100035021, 0.04516789600165794, 0.04267112800152972, 0.04658552999899257, 0.047468566001043655, 0.04962705799698597, 0.042501543000980746, 0.04183405100047821, 0.052940692999982275, 0.042656322999391705, 0.044323394999082666, 0.044251286002690904, 0.04487298400636064, 0.042163300000538584, 0.04162635599641362, 0.043402731003880035, 0.04314301200065529, 0.05186678700556513, 0.044967501002247445, 0.044487514001957607, 0.045557233002909925, 0.043530839000595734, 0.041990074001660105, 0.04381915299745742, 0.04750980200333288, 0.04539941799885128, 0.05084521399840014, 0.04248183000163408, 0.0470421029967838, 0.04170366100152023, 0.05013050400157226, 0.041960812006436754, 0.04672869400383206, 0.044699271995341405, 0.042651786003261805, 0.05134201800683513, 0.04499828699772479, 0.043584507002378814, 0.04264772699389141, 0.04165529199963203, 0.044139698999060784, 0.04309416400064947, 0.040825096002663486, 0.04105549799714936, 0.05337819499982288, 0.04424870900402311, 0.041670106002129614, 0.040823139002895914, 0.045702160998189356, 0.044647360999078956, 0.047765025999979116, 0.04190622300666291, 0.04588350500125671, 0.05052531199908117, 0.0430342930048937, 0.04732904199772747, 0.04493944700516295, 0.04208982200361788, 0.045436079999490175, 0.044335505001072306, 0.045921100994746666, 0.04409074399882229, 0.0508975359989563, 0.04248884400294628, 0.04764066799543798, 0.04389836700283922, 0.04333070799475536, 0.041512787000101525, 0.044516146001114976, 0.04354068999964511, 0.04092784399836091, 0.05425447000015993, 0.04544109600101365, 0.04278853900177637, 0.04370756199932657, 0.043141583002579864, 0.04319282899814425, 0.041480684994894546, 0.04209971000091173, 0.0409819670021534, 0.054895784000109416, 0.04301356599899009, 0.0433853549984633, 0.04590108800039161, 0.046825890996842645, 0.04827421899972251, 0.04259117400215473, 0.04251343500072835, 0.04446130200085463, 0.04802975300117396, 0.04071249900152907, 0.04502944499836303, 0.046285131997137796, 0.04332459199940786, 0.04294840299553471, 0.041694801999256015, 0.04472265799995512, 0.04049008600122761, 0.04976444999920204, 0.04553438600123627, 0.04871899700083304, 0.04162929100129986, 0.0423482799960766, 0.04091792699910002, 0.045504572000936605, 0.04357581700605806, 0.04123382599937031, 0.05128279500058852, 0.04707114699704107, 0.04527488999883644, 0.04281816800357774, 0.045192277000751346, 0.051376852999965195, 0.04235802499897545, 0.04320201300288318, 0.04187871799513232, 0.05274272099632071, 0.04066888100351207, 0.0420614629983902, 0.04271714499918744, 0.04704693300300278, 0.0416985339979874, 0.042762293000123464, 0.045932135995826684, 0.04562316100054886, 0.05399987499549752, 0.04390882600273471, 0.04315167900494998, 0.0459150589958881, 0.042253735999111086, 0.046107520000077784, 0.04233569400093984, 0.04271757400420029, 0.039689974997600075, 0.0525860929992632, 0.04368588200304657, 0.04531208000116749, 0.04335694300243631, 0.04307384699495742, 0.04211288400256308, 0.04565826800535433, 0.042072563999681734, 0.04056748899893137, 0.05187891900277464, 0.04454894200171111, 0.041131812002277, 0.04219443500187481, 0.0441900580044603, 0.04384377700625919, 0.042310456999985036, 0.04518289399857167, 0.04103611900063697, 0.05304772999807028, 0.04153342499921564, 0.041245529006118886, 0.04082276400004048]\n",
            "The average Q1 time is  0.03947421001317303\n",
            "The average Q2 time is  0.17549427427331263\n",
            "The average Throughput time is  0.2150416910567315\n",
            "The average YOLO time is  0.025502690808854013\n",
            "The average Classifier time is  0.0452621233969479\n",
            "The average cropping time is  2.7246792681338207e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S_hq7MOl8L_"
      },
      "source": [
        "#THIS IS THE FUNCTIONS FOR THE PART WITH OPTIMIZATION WITH A FEW MINOR ALTERATIONS\n",
        "#NO NEED TO COMMENT AGAIN AS THERE ARE TINY DIFFEENCES \n",
        "\n",
        "def getFrame2(sec,cval, vid):\n",
        "    done = False\n",
        "    vid.set(cv.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames,image = vid.read()\n",
        "    if hasFrames and cval <= 900:\n",
        "      cv.imwrite(\"/content/gdrive/MyDrive/images/image\"+str(cval)+\".jpg\", image)     # save frame as JPG file\n",
        "      path = \"/content/gdrive/MyDrive/images/image\"+str(cval)+\".jpg\"\n",
        "    else:\n",
        "      path = \"\"\n",
        "      hasFrames = False\n",
        "\n",
        "    if (cval == 900):\n",
        "      done = True\n",
        "    return hasFrames, path, done\n",
        "\n",
        "\n",
        "def YOLO2(path, cval, net, class_names, colors):\n",
        "  weights = \"/content/yolov3-tiny.weights\"\n",
        "  data = \"/content/darknet/cfg/coco.data\"\n",
        "\n",
        "  \n",
        "  nms_thresh = 0.2\n",
        "  thresh = 0.2\n",
        "  hier_thresh = 0.5\n",
        "  \n",
        "  det = dn.detect_image(net, class_names, path.encode('utf-8'), thresh=thresh, hier_thresh=hier_thresh, nms=nms_thresh)\n",
        "  newimage, bblist = dn.draw_boxes(det, path, colors)\n",
        "  return newimage, det\n",
        "\n",
        "def cropRegions(image, left,right,top,bottom, val):\n",
        "  if left < 0:\n",
        "    left = 0\n",
        "  if right < 0:\n",
        "    right = 0\n",
        "  if top < 0:\n",
        "    top = 0\n",
        "  if bottom < 0:\n",
        "    bottom = 0\n",
        "  image = image[top:bottom, left:right]\n",
        "\n",
        "  return image \n",
        "\n",
        "def classify(img, i):\n",
        "\n",
        "  dim = (416, 416)\n",
        "  resized = cv.resize(img, dim, interpolation = cv.INTER_AREA)\n",
        "\n",
        "  img = np.expand_dims(resized, axis=0)\n",
        "  predictions_single = probability_model.predict(img)\n",
        "  class_res = np.argmax(predictions_single)\n",
        "  sedan = 0\n",
        "  suv = 0\n",
        "\n",
        "  if class_res == 0:\n",
        "    sedan = 1\n",
        "\n",
        "  elif class_res == 1:\n",
        "    suv = 1\n",
        "\n",
        "  return sedan, suv\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw6Xy7Iz9Fdf"
      },
      "source": [
        "This is the example with the optimization of multithreading included. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFd9oRhmalw-",
        "outputId": "0738f5c7-3e5d-462a-cd3f-1eb27ef648b3"
      },
      "source": [
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from threading import Thread, Lock\n",
        "import time\n",
        "import random\n",
        "import queue\n",
        "import threading\n",
        "from threading import Condition\n",
        "import sys\n",
        "import timeit\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "#HERE WE USE A QUEUE WHICH MAKES IT THREAD SAFE AND ALSO SENDS NOTIFICATIONS \n",
        "#WHEN DATA HAS BEEN ADDED OR REMOVED \n",
        "q = queue.Queue()\n",
        "#LIST FOR THE THREADS \n",
        "threads = list()\n",
        "\n",
        "#ALL THE SAME AS IN LAST EXAMPLE\n",
        "count = 1\n",
        "bb = []\n",
        "\n",
        "frameRate = 1/30 \n",
        "count=1\n",
        "model = \"/content/darknet/cfg/yolov3-tiny.cfg\"\n",
        "weights = \"/content/yolov3-tiny.weights\"\n",
        "data = \"/content/darknet/cfg/coco.data\"\n",
        "net, class_names, colors = dn.load_network(model, data, weights, 0)\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"/content/gdrive/MyDrive/Groundtruth.xlsx\", sheet_name = 'Sheet2')\n",
        "frames = pd.DataFrame(df['Frame#'])\n",
        "sedans = pd.DataFrame(df['Sedan'])\n",
        "suvs = pd.DataFrame(df['SUV'])\n",
        "total = pd.DataFrame(df['Total'])\n",
        "\n",
        "\n",
        "total = total.to_numpy()\n",
        "sedans = sedans.to_numpy()\n",
        "suvs = suvs.to_numpy()\n",
        "frames = frames.to_numpy()\n",
        "\n",
        "result = []\n",
        "\n",
        "throughput_times = []\n",
        "\n",
        "\n",
        "#THIS IS THE PRODUCER THREAD WHICH IS THE VIDEO READER\n",
        "#IT TAKES THE VIDEO AND PUTS THE OUTPUT WHICH IS A FRAME INTO THE QUEUE\n",
        "class ProducerThread(Thread):\n",
        "    Thread._is_running = True\n",
        "    def run(self):\n",
        "        start = timeit.default_timer()\n",
        "        frameRate = 1/30 \n",
        "        global queue\n",
        "        sec = 0\n",
        "        cval = 1\n",
        "        hasFrames = True\n",
        "        vid = cv.VideoCapture('/content/assignment-clip.mp4')\n",
        "\n",
        "        #LOOP HERE WHILE THE THREAD IS ALIVE \n",
        "        while (self._is_running):\n",
        "\n",
        "            hasFrame, path, done = getFrame2(sec,cval, vid)\n",
        "            #IF WE ARE DONE END THREAD\n",
        "            if (done):\n",
        "              stop = timeit.default_timer()\n",
        "              print('Throughput (FPS) with optimization of part 1 is: ', (900 / (stop - start))) \n",
        "              self.stop()\n",
        "\n",
        "            sec = sec + frameRate\n",
        "            cval = cval + 1\n",
        "            q.put(path)\n",
        "\n",
        "    def stop(self):\n",
        "      self._is_running = False\n",
        "\n",
        "\n",
        "#THIS THREAD TAKES THE FRAMES PATH OFF THE QUEUE AND RUNS YOLO, CROPS THE IMAGES\n",
        "#AND RUNS THE CLASSIFIER ON EACH PROPOSED VEHICLE\n",
        "class ConsumerThread(Thread):\n",
        "    Thread._is_running = True\n",
        "    net, class_names, colors = dn.load_network(model, data, weights, 0)\n",
        "\n",
        "    def run(self):\n",
        "        main_start = timeit.default_timer()\n",
        "\n",
        "        start = timeit.default_timer()\n",
        "        stop = \"\"\n",
        "        global queue\n",
        "        counter = 0\n",
        "        output = []\n",
        "        img_array = []\n",
        "\n",
        "        while (self._is_running):\n",
        "            main_start = timeit.default_timer()\n",
        "\n",
        "            #TAKE PATH FROM QUEUE \n",
        "            path = q.get()\n",
        "            if (path != \"\"):\n",
        "              counter = counter + 1\n",
        "\n",
        "            #RUN YOLO ON IMAGE \n",
        "            x, bb = YOLO2(path, counter, net, class_names, colors)\n",
        "            image_list = []\n",
        "            val = 0\n",
        "            total_vehicles = 0\n",
        "            sedan = 0\n",
        "            suv = 0\n",
        "            i = 0\n",
        "\n",
        "            #we need to look up the image here so we can add the boxes and the labels\n",
        "            frame = cv.imread(\"/content/gdrive/MyDrive/images/image\"+str(counter)+\".jpg\")     # save frame as JPG file\n",
        "\n",
        "            #LOOP THROUGH BOUNDING BOXES \n",
        "            for b in bb:\n",
        "              if (b[0] == \"car\"):\n",
        "                left, top, right, bottom = dn.bbox2points(b[2])\n",
        "                image = cropRegions(x, left,right,top,bottom, val)\n",
        "                val = val + 1\n",
        "                total_vehicles = total_vehicles + 1\n",
        "                sedanres, suvres = classify(image, i)\n",
        "                label = \"\"\n",
        "                col = (0,255,0)\n",
        "\n",
        "                #HERE WE ANNOTATE EACH FRAME WITH THE BOUNDING BOX AND THE LABEL OF THE PREDICTION\n",
        "                if (sedanres == 1):\n",
        "                  label = \"Sedan\"\n",
        "                  cv.rectangle(frame, (left, top), (right, bottom), (0,255,0), 1)\n",
        "                  cv.putText(frame, \"{}\".format(label) , (left, top - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "\n",
        "                else:\n",
        "                  label = \"SUV\"\n",
        "                  cv.rectangle(frame, (left, top), (right, bottom), (0,0,128), 1)\n",
        "                  cv.putText(frame, \"{}\".format(label) , (left, top - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,128), 2)\n",
        "                \n",
        "\n",
        "                sedan = sedan + sedanres\n",
        "                suv = suv + suvres\n",
        "                i = i + 1\n",
        "\n",
        "            #GET THE RESULTS OF THE FRAME AND PUT THEM INTO A TUPLE \n",
        "            x = (sedan, suv, total_vehicles)\n",
        "            output.append(x)\n",
        "            img_array.append(frame)\n",
        "            main_end = timeit.default_timer()\n",
        "\n",
        "            throughput_times.append(main_end - main_start)\n",
        "\n",
        "\n",
        "            #IF WE ARE DONE \n",
        "            if (counter == 900):\n",
        "              stop = timeit.default_timer()\n",
        "              print('Throughput (FPS) with optimization of part 2 is: ', (900 / (stop - start))) \n",
        "              throughput_times.append()\n",
        "              height, width, layers = frame.shape\n",
        "              size = (width, height)\n",
        "\n",
        "              #HERE IS WHERE WE CREATE THE VIDEO \n",
        "              out = cv.VideoWriter('projectoutputvideo.avi', cv.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
        "              for i in range(len(img_array)):\n",
        "                  out.write(img_array[i])\n",
        "\n",
        "              out.release()\n",
        "              cv.destroyAllWindows()\n",
        "\n",
        "              self.stop()\n",
        "\n",
        "    def stop(self):\n",
        "      self._is_running = False\n",
        " \n",
        "#CREATE AND START THE PRODUCER AND CONSUMER THREADS \n",
        "produce = ProducerThread()\n",
        "consume = ConsumerThread()\n",
        "threads.append(produce)\n",
        "threads.append(consume)\n",
        "produce.start()\n",
        "consume.start()\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Throughput (FPS) with optimization of part 1 is:  19.659058988400083\n",
            "Throughput (FPS) with optimization of part 2 is:  5.169814323080091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-61:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-71-1640c72f5dad>\", line 158, in run\n",
            "    throughput_times.append()\n",
            "TypeError: append() takes exactly one argument (0 given)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv2VNPNqDy44",
        "outputId": "c08a180b-146b-4fa7-ab29-cf81289f233b"
      },
      "source": [
        "print(\"The average Throughput time is \", (sum(throughput_times)/len(throughput_times)))\n",
        "\n",
        "with open('/content/results.csv','w') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['Q1 Times','Q2 Times', 'Throughput'])\n",
        "    for i in range(len(throughput_times)):\n",
        "        resnew = ((i), (throughput_times[i]))\n",
        "        csv_out.writerow(resnew)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average Throughput time is  0.19342741207565672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drj9gdm5kGoS",
        "outputId": "4d19c51e-563e-4326-de64-812753344df1"
      },
      "source": [
        "import csv\n",
        "\n",
        "#HERE WE CALCULATE THE F1 SCORES OF ALL THE OUTPUTS\n",
        "f_pos_sedan = 0\n",
        "t_pos_sedan = 0\n",
        "f_neg_sedan = 0\n",
        "\n",
        "\n",
        "f_pos_suv = 0\n",
        "t_pos_suv = 0\n",
        "f_neg_suv = 0\n",
        "\n",
        "f_pos_total = 0\n",
        "t_pos_total = 0\n",
        "f_neg_total = 0\n",
        "\n",
        "#LOOP THROUGH ALL THE RESULTS OF THE 900 FRAMES \n",
        "for i in range(len(res)):\n",
        "\n",
        "  #CALCULATE THE F1 SCORE FOR SEDANS \n",
        "  fp_val = res[i][0] - sedans[i]\n",
        "  if (fp_val >= 0):\n",
        "    f_pos_sedan = f_pos_sedan + abs(fp_val)\n",
        "\n",
        "  fn_val = sedans[i] - res[i][0]\n",
        "  if (fn_val >= 0):\n",
        "    f_neg_sedan = f_neg_sedan + abs(fn_val)\n",
        "\n",
        "  if (res[i][0] >= sedans[i]):\n",
        "    t_pos_sedan = t_pos_sedan + sedans[i]\n",
        "  else:\n",
        "    t_pos_sedan = t_pos_sedan + res[i][0]\n",
        "\n",
        "\n",
        "  #CALCULATE THE F1 SCORE FOR SUVS\n",
        "  fp_val = res[i][1] - suvs[i]\n",
        "  if (fp_val >= 0):\n",
        "    f_pos_suv = f_pos_suv + abs(fp_val)\n",
        "\n",
        "  fn_val = suvs[i] - res[i][1]\n",
        "  if (fn_val >= 0):\n",
        "    f_neg_suv = f_neg_suv + abs(fn_val)\n",
        "\n",
        "  if (res[i][1] >= suvs[i]):\n",
        "    t_pos_suv = t_pos_suv + sedans[i]\n",
        "  else:\n",
        "    t_pos_suv = t_pos_suv + res[i][1]\n",
        "\n",
        "\n",
        "  #CALCULATE THE F1 SCORE FOR THE TOTAL VEHICLES (PART 1 THE YOLO OUTPUT)\n",
        "  fp_val = res[i][2] - total[i]\n",
        "  if (fp_val >= 0):\n",
        "    f_pos_total = f_pos_total + abs(fp_val)\n",
        "\n",
        "  fn_val = total[i] - res[i][2]\n",
        "  if (fn_val >= 0):\n",
        "    f_neg_total = f_neg_total + abs(fn_val)\n",
        "\n",
        "  if (res[i][2] >= total[i]):\n",
        "    t_pos_total = t_pos_total + sedans[i]\n",
        "  else:\n",
        "    t_pos_total = t_pos_total + res[i][2]\n",
        "\n",
        "\n",
        "#HERE WE GET THE F1 SCORE \n",
        "score_sedan = ((t_pos_sedan)/(t_pos_sedan + .5*(f_pos_sedan + f_neg_sedan)))\n",
        "score_suv = ((t_pos_suv)/(t_pos_suv + .5*(f_pos_suv + f_neg_suv)))\n",
        "score_total = ((t_pos_total)/(t_pos_total + .5*(f_pos_total + f_neg_total)))\n",
        "\n",
        "print(\"The f score for part 1 (the YOLO detection results): \", \"{:.2f}\".format(score_total[0] * 100), \"%\")\n",
        "print(\"The f score for the Sedan results are: \", \"{:.2f}\".format(score_sedan[0] * 100), \"%\")\n",
        "print(\"The f score for the SUV results are: \", \"{:.2f}\".format(score_suv[0] * 100), \"%\")\n",
        "\n",
        "print(res)\n",
        "\n",
        "#WRITE OUR RESULTS TO A CSV FILE \n",
        "with open('/content/results_per_frame.csv','w') as out:\n",
        "    csv_out=csv.writer(out)\n",
        "    csv_out.writerow(['Frame#','Sedan','SUV', 'Total'])\n",
        "    for i in range(len(res)):\n",
        "        print(res[i])\n",
        "        resnew = ((i+1,) + res[i])\n",
        "        csv_out.writerow(resnew)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f score for part 1 (the YOLO detection results):  86.45 %\n",
            "The f score for the Sedan results are:  73.04 %\n",
            "The f score for the SUV results are:  75.08 %\n",
            "[(2, 1, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (0, 4, 4), (1, 3, 4), (0, 3, 3), (0, 4, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (0, 4, 4), (0, 4, 4), (1, 3, 4), (0, 4, 4), (2, 2, 4), (1, 3, 4), (0, 4, 4), (1, 3, 4), (0, 4, 4), (0, 4, 4), (1, 3, 4), (0, 4, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 2, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 3, 3), (0, 1, 1), (0, 2, 2), (0, 3, 3), (0, 1, 1), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (1, 2, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (0, 3, 3), (1, 2, 3), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 2, 2), (0, 3, 3), (1, 3, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 3, 3), (0, 3, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (0, 3, 3), (0, 4, 4), (0, 4, 4), (0, 4, 4), (1, 3, 4), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (0, 3, 3), (2, 1, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (3, 0, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (1, 2, 3), (2, 2, 4), (2, 2, 4), (0, 4, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (0, 4, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (1, 2, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (3, 0, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (2, 1, 3), (1, 2, 3), (0, 3, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 3, 4), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (2, 1, 3), (2, 2, 4), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (1, 2, 3), (0, 3, 3), (0, 3, 3), (0, 4, 4), (1, 3, 4), (0, 3, 3), (0, 4, 4), (0, 4, 4), (0, 2, 2), (0, 2, 2), (0, 3, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 3, 4), (1, 3, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 4, 4), (0, 3, 3), (0, 4, 4), (1, 3, 4), (0, 3, 3), (0, 4, 4), (0, 4, 4), (1, 2, 3), (0, 2, 2), (0, 2, 2), (0, 3, 3), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 4, 4), (0, 4, 4), (0, 4, 4), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (1, 2, 3), (1, 2, 3), (1, 2, 3), (1, 1, 2), (2, 1, 3), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 2, 3), (1, 2, 3), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 2, 3), (1, 1, 2), (1, 1, 2), (1, 1, 2), (0, 3, 3), (0, 3, 3), (0, 2, 2), (1, 1, 2), (1, 1, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (0, 2, 2), (0, 3, 3), (0, 3, 3), (1, 2, 3), (1, 2, 3), (0, 3, 3), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (0, 4, 4), (1, 2, 3), (1, 3, 4), (0, 3, 3), (0, 4, 4), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (0, 3, 3), (2, 2, 4), (0, 4, 4), (2, 1, 3), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (0, 4, 4), (1, 2, 3), (0, 4, 4), (3, 2, 5), (1, 3, 4), (2, 2, 4), (2, 2, 4), (0, 3, 3), (0, 3, 3), (0, 2, 2), (0, 2, 2), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 1, 1), (0, 2, 2), (0, 2, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (2, 0, 2), (1, 1, 2), (2, 0, 2), (2, 0, 2), (1, 1, 2), (1, 1, 2), (2, 0, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (2, 0, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (1, 1, 2), (0, 1, 1), (0, 1, 1), (1, 1, 2), (0, 2, 2), (1, 1, 2), (1, 1, 2), (0, 1, 1), (0, 2, 2), (0, 2, 2), (1, 1, 2), (0, 0, 0), (0, 1, 1), (0, 0, 0), (0, 1, 1), (0, 1, 1), (0, 0, 0), (0, 0, 0), (0, 1, 1), (0, 2, 2), (0, 2, 2), (0, 2, 2), (1, 1, 2), (0, 2, 2), (1, 1, 2), (1, 1, 2), (1, 2, 3), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 2, 3), (2, 3, 5), (3, 1, 4), (2, 3, 5), (1, 4, 5), (1, 4, 5), (3, 2, 5), (1, 5, 6), (2, 4, 6), (2, 2, 4), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (2, 4, 6), (3, 3, 6), (4, 1, 5), (1, 5, 6), (2, 3, 5), (2, 4, 6), (3, 2, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (3, 2, 5), (4, 1, 5), (4, 1, 5), (4, 1, 5), (1, 3, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 4, 5), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 4, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (2, 3, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (3, 2, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (3, 2, 5), (3, 2, 5), (2, 3, 5), (2, 3, 5), (2, 3, 5), (1, 3, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (3, 2, 5), (3, 1, 4), (3, 1, 4), (3, 1, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 2, 4), (3, 1, 4), (3, 1, 4), (3, 0, 3), (2, 1, 3), (3, 0, 3), (1, 2, 3), (2, 1, 3), (3, 1, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (3, 1, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (3, 1, 4), (3, 1, 4), (3, 1, 4), (2, 2, 4), (2, 3, 5), (2, 3, 5), (2, 3, 5), (1, 4, 5), (3, 2, 5), (3, 1, 4), (3, 1, 4), (2, 1, 3), (3, 1, 4), (3, 1, 4), (2, 1, 3), (3, 1, 4), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 1, 3), (2, 2, 4), (2, 1, 3), (2, 2, 4), (2, 1, 3), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 3, 5), (1, 4, 5), (1, 3, 4), (1, 3, 4), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (1, 4, 5), (2, 3, 5), (2, 3, 5), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (3, 1, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (1, 3, 4), (2, 2, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (1, 3, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4), (2, 2, 4)]\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(0, 3, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(3, 0, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(3, 0, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 3, 4)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 2, 3)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(0, 4, 4)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 1, 2)\n",
            "(2, 1, 3)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 2, 3)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(1, 2, 3)\n",
            "(1, 2, 3)\n",
            "(0, 3, 3)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(1, 2, 3)\n",
            "(1, 3, 4)\n",
            "(0, 3, 3)\n",
            "(0, 4, 4)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(2, 2, 4)\n",
            "(0, 4, 4)\n",
            "(2, 1, 3)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(0, 4, 4)\n",
            "(1, 2, 3)\n",
            "(0, 4, 4)\n",
            "(3, 2, 5)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(0, 3, 3)\n",
            "(0, 3, 3)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(2, 0, 2)\n",
            "(1, 1, 2)\n",
            "(2, 0, 2)\n",
            "(2, 0, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(2, 0, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(2, 0, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(0, 0, 0)\n",
            "(0, 1, 1)\n",
            "(0, 0, 0)\n",
            "(0, 1, 1)\n",
            "(0, 1, 1)\n",
            "(0, 0, 0)\n",
            "(0, 0, 0)\n",
            "(0, 1, 1)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(0, 2, 2)\n",
            "(1, 1, 2)\n",
            "(1, 1, 2)\n",
            "(1, 2, 3)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 2, 3)\n",
            "(2, 3, 5)\n",
            "(3, 1, 4)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(3, 2, 5)\n",
            "(1, 5, 6)\n",
            "(2, 4, 6)\n",
            "(2, 2, 4)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 4, 6)\n",
            "(3, 3, 6)\n",
            "(4, 1, 5)\n",
            "(1, 5, 6)\n",
            "(2, 3, 5)\n",
            "(2, 4, 6)\n",
            "(3, 2, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(3, 2, 5)\n",
            "(4, 1, 5)\n",
            "(4, 1, 5)\n",
            "(4, 1, 5)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 4, 5)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(3, 2, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(3, 2, 5)\n",
            "(3, 2, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(3, 2, 5)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(3, 0, 3)\n",
            "(2, 1, 3)\n",
            "(3, 0, 3)\n",
            "(1, 2, 3)\n",
            "(2, 1, 3)\n",
            "(3, 1, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(3, 2, 5)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(2, 1, 3)\n",
            "(3, 1, 4)\n",
            "(3, 1, 4)\n",
            "(2, 1, 3)\n",
            "(3, 1, 4)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(2, 1, 3)\n",
            "(2, 2, 4)\n",
            "(2, 1, 3)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 3, 5)\n",
            "(1, 4, 5)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(1, 4, 5)\n",
            "(2, 3, 5)\n",
            "(2, 3, 5)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(3, 1, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(1, 3, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n",
            "(2, 2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}